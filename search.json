[{"title":"Beatiful_soup_note","url":"/2019/05/29/Beatiful_soup_note/","content":"\n# Beatifu_Soup 漂亮的汤\n\nBeautiful Soup提供一些简单的、Python式的函数来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。\n\nBeautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为UTF-8编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时你仅仅需要说明一下原始编码方式就可以了。\n\nBeautiful Soup已成为和lxml、html6lib一样出色的Python解释器，为用户灵活地提供不同的解析策略或强劲的速度。\n\n## 解析器\nBeautiful Soup在解析时实际上依赖解析器，它除了支持Python标准库中的HTML解析器外，还支持一些第三方解析器（比如lxml）\n\n|解析器|使用方法|优势|劣势|\n|----|----|-----|-----|\n|Python标准库|BeautifulSoup(markup, \"html.parser\")|Python的内置标准库、</n>执行速度适中、文档容错能力强|Python 2.7.3及</r>Python 3.2.2之前的版本文档容错能力差|\n|lxml HTML解析器|BeautifulSoup(markup, \"lxml\")|速度快、文档容错能力强|需要安装C语言库|\n|lxml XML解析器|BeautifulSoup(markup, \"xml\")|速度快、唯一支持XML的解析器|需要安装C语言库|\n|html5lib|BeautifulSoup(markup, \"html5lib\")|最好的容错性、以浏览器的方式解析文档、生成HTML5格式的文档|速度慢、不依赖外部扩展|\n\n\n```python\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup('<p>Hello</p>', 'lxml')\nprint(soup.p.string)\n```\n\n    Hello\n\n\n```\nHello\n```\n\n\n```python\nhtml = \"\"\"\n<html><head><title>The Dormouse's story</title></head>\n<body>\n<p class=\"title\" name=\"dromouse\"><b>The Dormouse's story</b></p>\n<p class=\"story\">Once upon a time there were three little sisters; and their names were\n<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"><!-- Elsie --></a>,\n<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\nand they lived at the bottom of a well.</p>\n<p class=\"story\">...</p>\n\"\"\"\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint(soup.prettify())\nprint(soup.title.string)\n```\n\n```python\n<html>\n <head>\n  <title>\n   The Dormouse's story\n  </title>\n </head>\n <body>\n  <p class=\"title\" name=\"dromouse\">\n   <b>\n    The Dormouse's story\n   </b>\n  </p>\n  <p class=\"story\">\n   Once upon a time there were three little sisters; and their names were\n   <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n    <!-- Elsie -->\n   </a>\n   ,\n   <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n    Lacie\n   </a>\n   and\n   <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\n    Tillie\n   </a>\n   ;\nand they lived at the bottom of a well.\n  </p>\n  <p class=\"story\">\n   ...\n  </p>\n </body>\n</html>\nThe Dormouse's story\n```\n\n\n这里首先声明变量html，它是一个HTML字符串。但是需要注意的是，它并不是一个完整的HTML字符串，因为body和html节点都没有闭合。接着，我们将它当作第一个参数传给BeautifulSoup对象，该对象的第二个参数为解析器的类型（这里使用lxml），此时就完成了BeaufulSoup对象的初始化。然后，将这个对象赋值给soup变量。\n\n接下来，就可以调用soup的各个方法和属性解析这串HTML代码了。\n\n首先，调用prettify()方法。这个方法可以把要解析的字符串以标准的缩进格式输出。这里需要注意的是，输出结果里面包含body和html节点，也就是说对于不标准的HTML字符串BeautifulSoup，可以自动更正格式。这一步不是由prettify()方法做的，而是在初始化BeautifulSoup时就完成了。\n\n然后调用soup.title.string，这实际上是输出HTML中title节点的文本内容。所以，soup.title可以选出HTML中的title节点，再调用string属性就可以得到里面的文本了，所以我们可以通过简单调用几个属性完成文本提取，这是不是非常方便？\n\n## 节点选择器\n直接调用节点的名称就可以选择节点元素，再调用string属性就可以得到节点内的文本了，这种选择方式速度非常快。如果单个节点结构层次非常清晰，可以选用这种方式来解析。\n\n### 选择元素\n\n打印输出title节点的选择结果，输出结果正是title节点加里面的文字内容\n\n\n```python\nsoup.title\n```\n\n\n\n\n```python\n<title>The Dormouse's story</title>\n```\n\n\n\n它的类型，是bs4.element.Tag类型，这是Beautiful Soup中一个重要的数据结构。经过选择器选择后，选择结果都是这种Tag类型。Tag具有一些属性，比如string属性，调用该属性，可以得到节点的文本内容，所以接下来的输出结果正是节点的文本内容。\n\n\n```python\ntype(soup.title)\n```\n\n\n\n\n```python\nbs4.element.Tag\n```\n\n\n\n\n```python\nsoup.title.string\n```\n\n\n\n\n```python\n\"The Dormouse's story\"\n```\n\n\n\n选择了head节点，结果也是节点加其内部的所有内容。\n\n\n```python\nsoup.head\n```\n\n\n\n\n```python\n<head><title>The Dormouse's story</title></head>\n```\n\n\n\n### 提取信息\n\n### 获取名称\n\n可以利用name属性获取节点的名称。这里还是以上面的文本为例，选取title节点，然后调用name属性就可以得到节点名称：\n\n\n```python\nsoup.title.name\n```\n\n\n\n\n```python\n'title'\n```\n\n\n\n### 获取属性\n每个节点可能有多个属性，比如id和class等，选择这个节点元素后，可以调用attrs获取所有属性：\n\n\n```python\nsoup.p.attrs\n```\n\n\n\n\n```python\n{'class': ['title'], 'name': 'dromouse'}\n```\n\n\n\n可以看到，attrs的返回结果是字典形式，它把选择的节点的所有属性和属性值组合成一个字典。接下来，如果要获取name属性，就相当于从字典中获取某个键值，只需要用中括号加属性名就可以了。比如，要获取name属性，就可以通过attrs['name']来得到。\n\n\n```python\nsoup.p.attrs['name']\n```\n\n\n\n\n```python\n'dromouse'\n```\n\n\n\n其实这样有点烦琐，还有一种更简单的获取方式：可以不用写attrs，直接在节点元素后面加中括号，传入属性名就可以获取属性值了。样例如下：\n\n\n```python\nprint(soup.p['name'])\nprint(soup.p['class'])\n```\n\n```python\ndromouse\n['title']\n```\n\n\n这里需要注意的是，有的返回结果是字符串，有的返回结果是字符串组成的列表。比如，name属性的值是唯一的，返回的结果就是单个字符串。而对于class，一个节点元素可能有多个class，所以返回的是列表。在实际处理过程中，我们要注意判断类型。\n\n### 获取内容\n\n可以利用string属性获取节点元素包含的文本内容，比如要获取第一个p节点的文本：\n\n\n```python\nsoup.p.string\n```\n\n\n\n\n```python\n\"The Dormouse's story\"\n```\n\n\n\n在上面的例子中，我们知道每一个返回结果都是bs4.element.Tag类型，它同样可以继续调用节点进行下一步的选择。比如，我们获取了head节点元素，我们可以继续调用head来选取其内部的head节点元素：\n\n\n```python\nhtml = \"\"\"\n<html><head><title>The Dormouse's story</title></head>\n<body>\n\"\"\"\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint(soup.head.title)\nprint(type(soup.head.title))\nprint(soup.head.title.string)\n```\n\n```python\n<title>The Dormouse's story</title>\n<class 'bs4.element.Tag'>\nThe Dormouse's story\n```\n\n\n第一行结果是调用head之后再次调用title而选择的title节点元素。然后打印输出了它的类型，可以看到，它仍然是bs4.element.Tag类型。也就是说，我们在Tag类型的基础上再次选择得到的依然还是Tag类型，每次返回的结果都相同，所以这样就可以做嵌套选择了。\n\n最后，输出它的string属性，也就是节点里的文本内容。\n\n### 关联选择\n在做选择的时候，有时候不能做到一步就选到想要的节点元素，需要先选中某一个节点元素，然后以它为基准再选择它的子节点、父节点、兄弟节点等，这里就来介绍如何选择这些节点元素。\n\n#### 子节点和子孙节点\n\n选取节点元素之后，如果想要获取它的直接子节点，可以调用contents属性，示例如下：\n\n\n```python\nfrom bs4 import BeautifulSoup\nhtml = \"\"\"\n<html>\n    <head>\n        <title>The Dormouse's story</title>\n    </head>\n    <body>\n        <p class=\"story\">\n            Once upon a time there were three little sisters; and their names were\n            <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">\n                <span>Elsie</span>\n            </a>\n            <a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> \n            and\n            <a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>\n            and they lived at the bottom of a well.\n        </p>\n        <p class=\"story\">...</p>\n\"\"\"\nsoup = BeautifulSoup(html, 'lxml')\nprint(soup.p.children)\nfor i, child in enumerate(soup.p.children):\n    print(i, child)\n```\n\n```python\n<list_iterator object at 0x0000019337C7F3C8>\n0 \n            Once upon a time there were three little sisters; and their names were\n            \n1 <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n<span>Elsie</span>\n</a>\n2 \n\n3 <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\n4  \n            and\n            \n5 <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n6 \n            and they lived at the bottom of a well.\n```\n\n\n​    \n\n还是同样的HTML文本，这里调用了children属性来选择，返回结果是生成器类型。接下来，我们用for循环输出相应的内容。\n\n如果要得到所有的子孙节点的话，可以调用descendants属性：\n\n\n```python\nfor i, child in enumerate(soup.p.descendants):\n    print(i, child)\n```\n\n```python\n0 \n            Once upon a time there were three little sisters; and their names were\n            \n1 <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n<span>Elsie</span>\n</a>\n2 \n\n3 <span>Elsie</span>\n4 Elsie\n5 \n\n6 \n\n7 <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\n8 Lacie\n9  \n            and\n            \n10 <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n11 Tillie\n12 \n            and they lived at the bottom of a well.\n```\n\n\n​    \n\n此时返回结果还是生成器。遍历输出一下可以看到，这次的输出结果就包含了span节点。descendants会递归查询所有子节点，得到所有的子孙节点。\n\n### 父节点和祖先节点\n\n\n\n```python\nhtml = \"\"\"\n<html>\n    <head>\n        <title>The Dormouse's story</title>\n    </head>\n    <body>\n        <p class=\"story\">\n            Once upon a time there were three little sisters; and their names were\n            <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">\n                <span>Elsie</span>\n            </a>\n        </p>\n        <p class=\"story\">...</p>\n\"\"\"\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint(soup.a.parent)\n```\n\n```python\n<p class=\"story\">\n            Once upon a time there were three little sisters; and their names were\n            <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n<span>Elsie</span>\n</a>\n</p>\n```\n\n\n这里我们选择的是第一个a节点的父节点元素。很明显，它的父节点是p节点，输出结果便是p节点及其内部的内容。\n\n需要注意的是，这里输出的仅仅是a节点的直接父节点，而没有再向外寻找父节点的祖先节点。如果想获取所有的祖先节点，可以调用parents属性：\n\n\n```python\nhtml = \"\"\"\n<html>\n    <body>\n        <p class=\"story\">\n            <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">\n                <span>Elsie</span>\n            </a>\n        </p>\n\"\"\"\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint(type(soup.a.parents))\nprint(list(enumerate(soup.a.parents)))\n```\n\n```python\n<class 'generator'>\n[(0, <p class=\"story\">\n<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n<span>Elsie</span>\n</a>\n</p>), (1, <body>\n<p class=\"story\">\n<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n<span>Elsie</span>\n</a>\n</p>\n</body>), (2, <html>\n<body>\n<p class=\"story\">\n<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n<span>Elsie</span>\n</a>\n</p>\n</body></html>), (3, <html>\n<body>\n<p class=\"story\">\n<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n<span>Elsie</span>\n</a>\n</p>\n</body></html>)]\n```\n\n\n可以发现，返回结果是生成器类型。这里用列表输出了它的索引和内容，而列表中的元素就是a节点的祖先节点。\n\n### 兄弟节点\n\n\n```python\nhtml = \"\"\"\n<html>\n    <body>\n        <p class=\"story\">\n            Once upon a time there were three little sisters; and their names were\n            <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">\n                <span>Elsie</span>\n            </a>\n            Hello\n            <a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> \n            and\n            <a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>\n            and they lived at the bottom of a well.\n        </p>\n\"\"\"\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint('Next Sibling', soup.a.next_sibling)\nprint('Prev Sibling', soup.a.previous_sibling)\nprint('Next Siblings', list(enumerate(soup.a.next_siblings)))\nprint('Prev Siblings', list(enumerate(soup.a.previous_siblings)))\n```\n\n```python\nNext Sibling \n            Hello\n            \nPrev Sibling \n            Once upon a time there were three little sisters; and their names were\n            \nNext Siblings [(0, '\\n            Hello\\n            '), (1, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>), (2, ' \\n            and\\n            '), (3, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>), (4, '\\n            and they lived at the bottom of a well.\\n        ')]\nPrev Siblings [(0, '\\n            Once upon a time there were three little sisters; and their names were\\n            ')]\n```\n\n\n### 提取信息\n前面讲解了关联元素节点的选择方法，如果想要获取它们的一些信息，比如文本、属性等，也用同样的方法，示例如下：\n\n\n```python\nhtml = \"\"\"\n<html>\n    <body>\n        <p class=\"story\">\n            Once upon a time there were three little sisters; and their names were\n            <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Bob</a><a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> \n        </p>\n\"\"\"\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint('Next Sibling:')\nprint(type(soup.a.next_sibling))\nprint(soup.a.next_sibling)\nprint(soup.a.next_sibling.string)\nprint('Parent:')\nprint(type(soup.a.parents))\nprint(list(soup.a.parents)[0])\nprint(list(soup.a.parents)[0].attrs['class'])\n```\n\n```python\nNext Sibling:\n<class 'bs4.element.Tag'>\n<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\nLacie\nParent:\n<class 'generator'>\n<p class=\"story\">\n            Once upon a time there were three little sisters; and their names were\n            <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Bob</a><a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\n</p>\n['story']\n```\n\n\n如果返回结果是单个节点，那么可以直接调用string、attrs等属性获得其文本和属性；如果返回结果是多个节点的生成器，则可以转为列表后取出某个元素，然后再调用string、attrs等属性获取其对应节点的文本和属性。\n\n## 方法选择器\n\n前面所讲的选择方法都是通过属性来选择的，这种方法非常快，但是如果进行比较复杂的选择的话，它就比较烦琐，不够灵活了。幸好，Beautiful Soup还为我们提供了一些查询方法，比如find_all()和find()等，调用它们，然后传入相应的参数，就可以灵活查询了。\n\n`find_all()` \n\n`find_all`，顾名思义，就是查询所有符合条件的元素。给它传入一些属性或文本，就可以得到符合条件的元素，它的功能十分强大。\n\n它的API如下：\n```python\nfind_all(name , attrs , recursive , text , **kwargs)\n```\n(1) name\n我们可以根据节点名来查询元素，示例如下：\n\n\n```python\nhtml='''\n<div class=\"panel\">\n    <div class=\"panel-heading\">\n        <h4>Hello</h4>\n    </div>\n    <div class=\"panel-body\">\n        <ul class=\"list\" id=\"list-1\">\n            <li class=\"element\">Foo</li>\n            <li class=\"element\">Bar</li>\n            <li class=\"element\">Jay</li>\n        </ul>\n        <ul class=\"list list-small\" id=\"list-2\">\n            <li class=\"element\">Foo</li>\n            <li class=\"element\">Bar</li>\n        </ul>\n    </div>\n</div>\n'''\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint(soup.find_all(name='ul'))\nprint(type(soup.find_all(name='ul')[0]))\n```\n\n```python\n[<ul class=\"list\" id=\"list-1\">\n<li class=\"element\">Foo</li>\n<li class=\"element\">Bar</li>\n<li class=\"element\">Jay</li>\n</ul>, <ul class=\"list list-small\" id=\"list-2\">\n<li class=\"element\">Foo</li>\n<li class=\"element\">Bar</li>\n</ul>]\n<class 'bs4.element.Tag'>\n```\n\n\n这里我们调用了`find_all()`方法，传入`name`参数，其参数值为`ul`。也就是说，我们想要查询所有`ul`节点，返回结果是列表类型，长度为2，每个元素依然都是`bs4.element.Tag`类型。\n\n因为都是`Tag`类型，所以依然可以进行嵌套查询。还是同样的文本，这里查询出所有`ul`节点后，再继续查询其内部的`li`节点：\n\n\n```python\nfor ul in soup.find_all(name='ul'):\n    print(ul.find_all(name='li'))\n    for li in ul.find_all(name='li'):\n        print(li.string)\n```\n\n```python\n[<li class=\"element\">Foo</li>, <li class=\"element\">Bar</li>, <li class=\"element\">Jay</li>]\nFoo\nBar\nJay\n[<li class=\"element\">Foo</li>, <li class=\"element\">Bar</li>]\nFoo\nBar\n```\n\n(2)attrs\n除了根据节点名查询，我们也可以传入一些属性来查询，示例如下：\n\n\n```python\nhtml='''\n<div class=\"panel\">\n    <div class=\"panel-heading\">\n        <h4>Hello</h4>\n    </div>\n    <div class=\"panel-body\">\n        <ul class=\"list\" id=\"list-1\" name=\"elements\">\n            <li class=\"element\">Foo</li>\n            <li class=\"element\">Bar</li>\n            <li class=\"element\">Jay</li>\n        </ul>\n        <ul class=\"list list-small\" id=\"list-2\">\n            <li class=\"element\">Foo</li>\n            <li class=\"element\">Bar</li>\n        </ul>\n    </div>\n</div>\n'''\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint(soup.find_all(attrs={'id': 'list-1'}))\nprint(soup.find_all(attrs={'name': 'elements'}))\n```\n\n```python\n[<ul class=\"list\" id=\"list-1\" name=\"elements\">\n<li class=\"element\">Foo</li>\n<li class=\"element\">Bar</li>\n<li class=\"element\">Jay</li>\n</ul>]\n[<ul class=\"list\" id=\"list-1\" name=\"elements\">\n<li class=\"element\">Foo</li>\n<li class=\"element\">Bar</li>\n<li class=\"element\">Jay</li>\n</ul>]\n```\n\n\n这里查询的时候传入的是attrs参数，参数的类型是字典类型。比如，要查询id为list-1的节点，可以传入attrs={'id': 'list-1'}的查询条件，得到的结果是列表形式，包含的内容就是符合id为list-1的所有节点。在上面的例子中，符合条件的元素个数是1，所以结果是长度为1的列表。\n\n对于一些常用的属性，比如id和class等，我们可以不用attrs来传递。比如，要查询id为list-1的节点，可以直接传入id这个参数。还是上面的文本，我们换一种方式来查询：\n\n\n```python\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint(soup.find_all(id='list-1'))\nprint(soup.find_all(class_='element'))\n```\n\n```python\n[<ul class=\"list\" id=\"list-1\" name=\"elements\">\n<li class=\"element\">Foo</li>\n<li class=\"element\">Bar</li>\n<li class=\"element\">Jay</li>\n</ul>]\n[<li class=\"element\">Foo</li>, <li class=\"element\">Bar</li>, <li class=\"element\">Jay</li>, <li class=\"element\">Foo</li>, <li class=\"element\">Bar</li>]\n```\n\n\n这里直接传入id='list-1'，就可以查询id为list-1的节点元素了。而对于class来说，由于class在Python里是一个关键字，所以后面需要加一个下划线，即class_='element'，返回的结果依然还是Tag组成的列表。\n\n(3) text\ntext参数可用来匹配节点的文本，传入的形式可以是字符串，可以是正则表达式对象，示例如下：\n\n\n```python\nimport re\nhtml='''\n<div class=\"panel\">\n    <div class=\"panel-body\">\n        <a>Hello, this is a link</a>\n        <a>Hello, this is a link, too</a>\n    </div>\n</div>\n'''\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint(soup.find_all(text=re.compile('link')))\n```\n\n```python\n['Hello, this is a link', 'Hello, this is a link, too']\n```\n\n\n这里有两个a节点，其内部包含文本信息。这里在find_all()方法中传入text参数，该参数为正则表达式对象，结果返回所有匹配正则表达式的节点文本组成的列表。\n\nfind()\n\n除了find_all()方法，还有find()方法，只不过后者返回的是单个元素，也就是第一个匹配的元素，而前者返回的是所有匹配的元素组成的列表。示例如下：\n\n\n```python\nhtml='''\n<div class=\"panel\">\n    <div class=\"panel-heading\">\n        <h4>Hello</h4>\n    </div>\n    <div class=\"panel-body\">\n        <ul class=\"list\" id=\"list-1\">\n            <li class=\"element\">Foo</li>\n            <li class=\"element\">Bar</li>\n            <li class=\"element\">Jay</li>\n        </ul>\n        <ul class=\"list list-small\" id=\"list-2\">\n            <li class=\"element\">Foo</li>\n            <li class=\"element\">Bar</li>\n        </ul>\n    </div>\n</div>\n'''\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint(soup.find(name='ul'))\n\n```\n\n```python\n<ul class=\"list\" id=\"list-1\">\n<li class=\"element\">Foo</li>\n<li class=\"element\">Bar</li>\n<li class=\"element\">Jay</li>\n</ul>\n<class 'bs4.element.Tag'>\n<ul class=\"list\" id=\"list-1\">\n<li class=\"element\">Foo</li>\n<li class=\"element\">Bar</li>\n<li class=\"element\">Jay</li>\n</ul>\n```\n\n\n\n```python\nprint(type(soup.find(name='ul')))\n```\n\n```python\n<class 'bs4.element.Tag'>\n```\n\n\n\n```python\nprint(soup.find(class_='list'))\n```\n\n```python\n<ul class=\"list\" id=\"list-1\">\n<li class=\"element\">Foo</li>\n<li class=\"element\">Bar</li>\n<li class=\"element\">Jay</li>\n</ul>\n```\n\n\n这里的返回结果不再是列表形式，而是第一个匹配的节点元素，类型依然是Tag类型。\n\n另外，还有许多查询方法，其用法与前面介绍的find_all()、find()方法完全相同，只不过查询范围不同，这里简单说明一下。\n\n`find_parents()`和`find_parent()`：前者返回所有祖先节点，后者返回直接父节点。\n`find_next_siblings()`和`find_next_sibling()`：前者返回后面所有的兄弟节点，后者返回后面第一个兄弟节点。\n`find_previous_siblings()`和`find_previous_sibling()`：前者返回前面所有的兄弟节点，后者返回前面第一个兄弟节点。\n`find_all_next()`和`find_next()`：前者返回节点后所有符合条件的节点，后者返回第一个符合条件的节点。\n`find_all_previous()`和`find_previous()`：前者返回节点后所有符合条件的节点，后者返回第一个符合条件的节点。\n\n## CSS选择器\n\nBeautiful Soup还提供了另外一种选择器，那就是CSS选择器。如果对Web开发熟悉的话，那么对CSS选择器肯定也不陌生。如果不熟悉的话，可以参考http://www.w3school.com.cn/cssref/css_selectors.asp了解。\n\n使用CSS选择器时，只需要调用select()方法，传入相应的CSS选择器即可，示例如下：\n\n\n```python\nhtml='''\n<div class=\"panel\">\n    <div class=\"panel-heading\">\n        <h4>Hello</h4>\n    </div>\n    <div class=\"panel-body\">\n        <ul class=\"list\" id=\"list-1\">\n            <li class=\"element\">Foo</li>\n            <li class=\"element\">Bar</li>\n            <li class=\"element\">Jay</li>\n        </ul>\n        <ul class=\"list list-small\" id=\"list-2\">\n            <li class=\"element\">Foo</li>\n            <li class=\"element\">Bar</li>\n        </ul>\n    </div>\n</div>\n'''\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint(soup.select('.panel .panel-heading'))\nprint(soup.select('#list-2 .element'))\n```\n\n```python\n[<div class=\"panel-heading\">\n<h4>Hello</h4>\n</div>]\n[<li class=\"element\">Foo</li>, <li class=\"element\">Bar</li>]\n<class 'bs4.element.Tag'>\n```\n\n\nselect('ul li')则是选择所有ul节点下面的所有li节点，结果便是所有的li节点组成的列表。\n\n\n```python\nprint(soup.select('ul li'))\n```\n\n```python\n[<li class=\"element\">Foo</li>, <li class=\"element\">Bar</li>, <li class=\"element\">Jay</li>, <li class=\"element\">Foo</li>, <li class=\"element\">Bar</li>]\n```\n\n\n最后一句打印输出了列表中元素的类型。可以看到，类型依然是Tag类型\n\n\n```python\nprint(type(soup.select('ul')[0]))\n```\n\n```python\n<class 'bs4.element.Tag'>\n```\n\n\n### 获取属性\n\n我们知道节点类型是Tag类型，所以获取属性还可以用原来的方法。仍然是上面的HTML文本，这里尝试获取每个ul节点的id属性：\n\n\n```python\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nfor ul in soup.select('ul'):\n    print(ul['id'])\n    print(ul.attrs['id'])\n```\n\n```python\nlist-1\nlist-1\nlist-2\nlist-2\n```\n\n\n可以看到，直接传入中括号和属性名，以及通过attrs属性获取属性值，都可以成功。\n\n### 获取文本\n要获取文本，当然也可以用前面所讲的string属性。此外，还有一个方法，那就是get_text()，示例如下：\n\n\n```python\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nfor li in soup.select('li'):\n    print('Get Text:', li.get_text())\n    print('String:', li.string)\n```\n\n```python\nGet Text: Foo\nString: Foo\nGet Text: Bar\nString: Bar\nGet Text: Jay\nString: Jay\nGet Text: Foo\nString: Foo\nGet Text: Bar\nString: Bar\n```\n\n\n可以看到，二者的效果完全一致。\n\n到此，Beautiful Soup的用法基本就介绍完了，最后做一下简单的总结。\n\n推荐使用lxml解析库，必要时使用html.parser。\n节点选择筛选功能弱但是速度快。\n建议使用`find()`或者`find_all()`查询匹配单个结果或者多个结果。\n如果对CSS选择器熟悉的话，可以使用`select()`方法选择。 \n","tags":["爬虫"]},{"title":"xpath_note","url":"/2019/05/23/xpath-note/","content":"\n# Xpath 学习笔记\n\n\n## XPath概览\nXPath，全称XML Path Language，即XML路径语言，它是一门在XML文档中查找信息的语言。它最初是用来搜寻XML文档的，但是它同样适用于HTML文档的搜索。\n\n所以在做爬虫时，我们完全可以使用XPath来做相应的信息抽取。本节中，我们就来介绍XPath的基本用法。 \n\nXPath的选择功能十分强大，它提供了非常简洁明了的路径选择表达式。另外，它还提供了超过100个内建函数，用于字符串、数值、时间的匹配以及节点、序列的处理等。几乎所有我们想要定位的节点，都可以用XPath来选择。\n\n它被设计为供XSLT、XPointer以及其他XML解析软件使用，更多的文档可以访问其官方网站：https://www.w3.org/TR/xpath/。\n\n## XPath常用规则\n|表达式|描述|\n|:-----:|:-----:|\n|nodename|选取此节点的所有子节点|\n|/|从当前节点选取直接子节点|\n|//|从当前节点选取子孙节点|\n|.|选取当前节点|\n|..|选取当前节点的父节点|\n|@|选取属性|\n\n\n\n\n\n\nXPath的常用匹配规则，示例如下：\n```python\nstr_xpath=\"//title[@class='python']\"\nhtml.xpath()\n```\n\n这就是一个XPath规则，它代表选择所有名称为title，同时属性class的值为python的节点。\n后面会通过Python的lxml库，利用XPath进行HTML的解析。\n\n## 安装\n```\npip install  lxml\n```\n使用以上命令，安装即可。\n\n## 示例讲解\n\n\n```python\nfrom lxml import etree\n```\n\n对基本包导入，我们需要用到etree这个包来解析网页。\n\n\n```python\ntext = '''\n<div>\n    <ul>\n         <li class=\"item-0\"><a href=\"link1.html\">first item</a></li>\n         <li class=\"item-1\"><a href=\"link2.html\">second item</a></li>\n         <li class=\"item-inactive\"><a href=\"link3.html\">third item</a></li>\n         <li class=\"item-1\"><a href=\"link4.html\">fourth item</a></li>\n         <li class=\"item-0\"><a href=\"link5.html\">fifth item</a>\n     </ul>\n </div>\n'''\n```\n\n\n我们自己声明一个变量，该变量是字符创，html格式的文本，\n\n\n```python\nhtml = etree.HTML(text)\n```\n\n调用HTML类进行初始化，这样就成功构造了一个XPath解析对象。这里需要注意的是，HTML文本中的最后一个li节点是没有闭合的，但是etree模块可以自动修正HTML文本。\n\n\n```python\nprint(etree.tostring(html).decode('utf-8'))\n```\n\n```\n    <html><body><div>\n        <ul>\n             <li class=\"item-0\"><a href=\"link1.html\">first item</a></li>\n             <li class=\"item-1\"><a href=\"link2.html\">second item</a></li>\n             <li class=\"item-inactive\"><a href=\"link3.html\">third item</a></li>\n             <li class=\"item-1\"><a href=\"link4.html\">fourth item</a></li>\n             <li class=\"item-0\"><a href=\"link5.html\">fifth item</a>\n         </li></ul>\n     </div>\n    </body></html>\n    \n```\n\n调用tostring()方法即可输出修正后的HTML代码，但是结果是bytes类型。这里利用decode()方法将其转成str类型。\n我们可以看到，经过处理之后，li节点标签被补全，并且还自动添加了body、html节点。\n\n### 选取所有节点\n一般会用//开头的XPath规则来选取所有符合要求的节点。这里以前面的HTML文本为例，如果要选取所有节点，可以这样实现：\n\n\n```python\nhtml.xpath('//*')\n```\n```\n[<Element html at 0x199551eb408>,\n<Element body at 0x19955205108>,\n<Element div at 0x19955243288>,\n     <Element ul at 0x19955243508>,\n     <Element li at 0x19955243388>,\n     <Element a at 0x199552433c8>,\n     <Element li at 0x19955243408>,\n     <Element a at 0x199552434c8>,\n     <Element li at 0x19955243608>,\n     <Element a at 0x19955243488>,\n     <Element li at 0x19955243648>,\n     <Element a at 0x19955243688>,\n     <Element li at 0x199552436c8>,\n     <Element a at 0x19955243708>]\n```\n\n这里使用*代表匹配所有节点，也就是整个HTML文本中的所有节点都会被获取。\n可以看到，返回形式是一个列表，每个元素是Element类型，其后跟了节点的名称，如html、body、div、ul、li、a等，所有节点都包含在列表中了。\n\n\n### 选取某特定节点\n\n\n```python\nhtml.xpath('//li')\n```\n\n```\n\n    [<Element li at 0x19955243388>,\n     <Element li at 0x19955243408>,\n     <Element li at 0x19955243608>,\n     <Element li at 0x19955243648>,\n     <Element li at 0x199552436c8>]\n\n```\n\n如上所示，此处匹配也可以指定节点名称。获取所有li节点，可以用//，然后加上节点名称既可以，调用时依然需要使用xpath()方法。\n返回的结果是个列表，里面的元素是可以操作的`Element`对象。\n\n### 选取节点下的子节点\n\n\n\n```python\nhtml.xpath('//li/a')\n```\n\n\n\n```\n    [<Element a at 0x199552433c8>,\n     <Element a at 0x199552434c8>,\n     <Element a at 0x19955243488>,\n     <Element a at 0x19955243688>,\n     <Element a at 0x19955243708>]\n\n```\n\n我们通过`/`或`//`即可查找元素的子节点或子孙节点。如上我们选择li节点的所有直接a子节点。\n以上的`/`用于选取直接的子节点，如果想要获取所有的子孙节点，可以使用`//`.\n例如，我们获取'ul'节点下的所有子孙节点`a`,代码如下：\n\n\n```python\nhtml.xpath('//ul//a')\n```\n\n\n\n```\n    [<Element a at 0x199552433c8>,\n     <Element a at 0x199552434c8>,\n     <Element a at 0x19955243488>,\n     <Element a at 0x19955243688>,\n     <Element a at 0x19955243708>]\n```\n\n\n### 父节点\n我们知道通过连续的/或//可以查找子节点或子孙节点，那么假如我们知道了子节点，怎样来查找父节点呢？这可以用..来实现。\n\n比如，现在首先选中href属性为link4.html的a节点，然后再获取其父节点，然后再获取其class属性，相关代码如下：\n\n\n```python\nhtml.xpath('//a[@href=\"link4.html\"]/../@class')\n```\n\n```\n    ['item-1']\n```\n\n检查一下结果发现，这正是我们获取的目标li节点的class。  \n同时，我们也可以通过parent::来获取父节点，代码如下：\n\n\n```python\nhtml.xpath('//a[@href=\"link4.html\"]/parent::*/@class')\n```\n\n```\n    ['item-1']\n\n```\n\n### 属性匹配\n\n在选取的时候，我们还可以用@符号进行属性过滤。\n比如，这里如果要选取class为item-1的li节点，可以这样实现:\n\n\n```python\n html.xpath('//li[@class=\"item-0\"]')\n```\n\n\n\n```\n    [<Element li at 0x19955243388>, <Element li at 0x199552436c8>]\n```\n\n\n这里我们通过加入[@class=\"item-0\"]，限制了节点的class属性为item-0，而HTML文本中符合条件的li节点有两个，所以结果应该返回两个匹配到的元素。\n\n### 文本获取\n我们用XPath中的text()方法获取节点中的文本，接下来尝试获取前面li节点中的文本\n\n\n```python\nhtml.xpath('//li[@class=\"item-0\"]/text()')\n```\n\n\n```\n\n    ['\\n     ']\n```\n\n\n奇怪的是，我们并没有获取到任何文本，只获取到了一个换行符，这是为什么呢？因为XPath中`text()`前面是`/`， \n而此处/的含义是选取直接子节点，很明显li的直接子节点都是a节点，文本都是在a节点内部的，  \n所以这里匹配到的结果就是被修正的li节点内部的换行符，因为自动修正的li节点的尾标签换行了。\n即选中的是这两个节点：\n```\n<li class=\"item-0\"><a href=\"link1.html\">first item</a></li>\n<li class=\"item-0\"><a href=\"link5.html\">fifth item</a>\n</li>\n```\n其中一个节点因为自动修正，li节点的尾标签添加的时候换行了，所以提取文本得到的唯一结果就是li节点的尾标签和a节点的尾标签之间的换行符。\n\n因此，如果想获取`li`节点内部的文本，就有两种方式，一种是先选取a节点再获取文本，另一种就是使用`//`。接下来，我们来看下二者的区别。\n\n首先，选取到a节点再获取文本，代码如下：\n\n\n\n```python\n html.xpath('//li[@class=\"item-0\"]/a/text()')\n```\n\n\n\n````\n    ['first item', 'fifth item']\n````\n\n\n可以看到，这里的返回值是两个，内容都是属性为item-0的li节点的文本，这也印证了前面属性匹配的结果是正确的。\n\n这里我们是逐层选取的，先选取了li节点，又利用/选取了其直接子节点a，然后再选取其文本，得到的结果恰好是符合我们预期的两个结果。\n再来看下用另一种方式（即使用//）选取的结果，代码如下：\n\n\n```python\nhtml.xpath('//li[@class=\"item-0\"]//text()')\n```\n\n\n````\n\n    ['first item', 'fifth item', '\\n     ']\n````\n\n\n不出所料，这里的返回结果是3个。可想而知，这里是选取所有子孙节点的文本，其中前两个就是li的子节点a节点内部的文本，另外一个就是最后一个li节点内部的文本，即换行符。\n\n所以说，如果要想获取子孙节点内部的所有文本，可以直接用//加text()的方式，这样可以保证获取到最全面的文本信息，但是可能会夹杂一些换行符等特殊字符。如果想获取某些特定子孙节点下的所有文本，可以先选取到特定的子孙节点，然后再调用text()方法获取其内部文本，这样可以保证获取的结果是整洁的。\n\n### 属性获取\n我们知道用text()可以获取节点内部文本，那么节点属性该怎样获取呢？其实还是用@符号就可以。例如，我们想获取所有li节点下所有a节点的href属性，代码如下：\n\n\n```python\nhtml.xpath('//li/a/@href')\n```\n\n\n\n````\n    ['link1.html', 'link2.html', 'link3.html', 'link4.html', 'link5.html']\n````\n\n\n我们通过@href即可获取节点的href属性。注意，此处和属性匹配的方法不同，属性匹配是中括号加属性名和值来限定某个属性，如[@href=\"link1.html\"]，而此处的@href指的是获取节点的某个属性，二者需要做好区分。\n\n#### 属性多值匹配\n有时候，某些节点的某个属性可能有多个值，例如：\n\n\n```python\ntext = '''\n<li class=\"li li-first\"><a href=\"link.html\">first item</a></li>\n'''\nhtml = etree.HTML(text)\n\n```\n\n这里HTML文本中li节点的class属性有两个值li和li-first，此时如果还想用之前的属性匹配获取，就无法匹配了，此时的运行结果如下：\n\n\n\n```python\nhtml.xpath('//li[@class=\"li\"]/a/text()')\n```\n\n\n\n```\n    []\n```\n\n\n这时就需要用contains()函数了，代码可以改写如下：\n\n\n```python\nhtml.xpath('//li[contains(@class, \"li\")]/a/text()')\n```\n\n\n\n```\n    ['first item']\n```\n\n\n此种方式在某个节点的某个属性有多个值时经常用到，如某个节点的class属性通常有多个。\n\n#### 多属性匹配\n，我们可能还遇到一种情况，那就是根据多个属性确定一个节点，这时就需要同时匹配多个属性。此时可以使用运算符and来连接，示例如下：\n\n\n```python\n\ntext = '''\n<li class=\"li li-first\" name=\"item\"><a href=\"link.html\">first item</a></li>\n'''\nhtml = etree.HTML(text)\n```\n\n这里的li节点又增加了一个属性name。要确定这个节点，需要同时根据class和name属性来选择，一个条件是class属性里面包含li字符串，另一个条件是name属性为item字符串，二者需要同时满足，需要用and操作符相连，相连之后置于中括号内进行条件筛选。运行结果如下：\n\n\n```python\n html.xpath('//li[contains(@class, \"li\") and @name=\"item\"]/a/text()')\n```\n\n\n\n```\n    ['first item']\n\n```\n\n运算符及其介绍\n\n|运算符|描述|实例|返回值|\n|:-----:|:-----:|:----:|\n|or|或|age=19 or age=20|如果age是19，则返回true。如果age是21，则返回false|\n|and|与|age>19 and age<21|如果age是20，则返回true。如果age是18，则返回false|\n|mod|计算除法的余数|5 mod 2|1|\n|+|加法|6 + 4|10|\n|-|减法|6 - 4|2|\n|* |乘法|6 * 4|24|\n|div|除法|8 div 4|2|\n|=|等于|age=19|如果age是19，则返回true。如果age是20，则返回false|\n|!=|不等于|age!=19|如果age是18，则返回true。如果age是19，则返回false|\n|<|小于|age<19|如果age是18，则返回true。如果age是19，则返回false|\n|<=|小于或等于|age<=19|如果age是19，则返回true。如果age是20，则返回false|\n|>|大于|age>19|如果age是20，则返回true。如果age是19，则返回false|\n|>=|大于或等于|age>=19|如果age是19，则返回true。如果age是18，则返回false|\n\n### 按序选择\n\n有时候，我们在选择的时候某些属性可能同时匹配了多个节点，但是只想要其中的某个节点，如第二个节点或者最后一个节点，这时该怎么办呢？\n这时可以利用中括号传入索引的方法获取特定次序的节点，示例如下：\n\n\n```python\ntext = '''\n<div>\n    <ul>\n         <li class=\"item-0\"><a href=\"link1.html\">first item</a></li>\n         <li class=\"item-1\"><a href=\"link2.html\">second item</a></li>\n         <li class=\"item-inactive\"><a href=\"link3.html\">third item</a></li>\n         <li class=\"item-1\"><a href=\"link4.html\">fourth item</a></li>\n         <li class=\"item-0\"><a href=\"link5.html\">fifth item</a>\n     </ul>\n </div>\n'''\nhtml = etree.HTML(text)\n```\n\n我们选取了第一个li节点，中括号中传入数字1即可。注意，这里和代码中不同，序号是以1开头的，不是以0开头。\n\n\n```python\nhtml.xpath('//li[1]/a/text()')\n```\n\n\n\n```\n    ['first item']\n```\n\n\n我们选取了最后一个li节点，中括号中传入last()即可，返回的便是最后一个li节点。\n\n\n```python\n html.xpath('//li[last()]/a/text()')\n```\n\n\n\n```\n    ['fifth item']\n```\n\n\n我们选取了位置小于3的li节点，也就是位置序号为1和2的节点，得到的结果就是前两个li节点。\n\n\n```python\nhtml.xpath('//li[position()<3]/a/text()')\n```\n\n\n\n```\n    ['first item', 'second item']\n```\n\n\n我们选取了倒数第三个li节点，中括号中传入last()-2即可。因为last()是最后一个，所以last()-2就是倒数第三个。\n\n\n```python\nhtml.xpath('//li[last()-2]/a/text()')\n```\n\n\n\n```\n    ['third item']\n```\n\n\n如果想查询更多XPath的用法，可以查看：http://www.w3school.com.cn/xpath/index.asp。\n\n如果想查询更多Python lxml库的用法，可以查看http://lxml.de/。\n","tags":["xpath"]},{"title":"git 常用命令","url":"/2019/03/17/git常用命令/","content":"\n\n\n\n**git init** 把当前的目录变成可以管理的git仓库，生成隐藏.git文件。\n\n **git add XX** 把xx文件添加到暂存区去。\n\n **git commit –m “XX”** 提交文件 –m 后面的是注释。\n\n **git status** 查看仓库状态\n\n **git diff XX** 查看XX文件修改了那些内容\n\n **git log** 查看历史记录\n\n **git reset –hard HEAD^** 或者 **git reset –hard HEAD~** 回退到上一个版本(如果想回退到100个版本，使用**git reset –hard HEAD~100** )\n\n **cat XX** 查看XX文件内容\n\n **git reflog** 查看历史记录的版本号id\n\n **git checkout — XX** 把XX文件在工作区的修改全部撤销。\n\n **git rm XX** 删除XX文件\n\n **git remote add origin url** 关联一个远程库\n\n **git push –u(第一次要用-u 以后不需要) origin master** 把当前master分支推送到远程库\n\n **git clone url** 从远程库中克隆\n\n **git checkout –b dev** 创建dev分支 并切换到dev分支上\n\n **git branch** 查看当前所有的分支\n\n **git checkout master** 切换回master分支\n\n **git merge dev** 在当前的分支上合并dev分支\n\n **git branch –d dev** 删除dev分支\n\n **git branch name** 创建分支\n\n **git stash** 把当前的工作隐藏起来 等以后恢复现场后继续工作\n\n **git stash list** 查看所有被隐藏的文件列表\n\n **git stash apply** 恢复被隐藏的文件，但是内容不删除\n\n **git stash drop** 删除文件\n\n **git stash pop** 恢复文件的同时 也删除文件\n\n **git remote** 查看远程库的信息\n\n **git remote –v** 查看远程库的详细信息\n\n **git push origin master** Git会把master分支推送到远程库对应的远程分支上\n\n","tags":["git"],"categories":["常用工具"]},{"title":"T检验","url":"/2018/10/31/T检验/","content":"# 均值差异性的检验方法：T检验\n\n  数据分析中有一块很大的版图是属于均值对比的，应用广泛。例如，对比试验前后病人的症状，证明某种药是否有效；对比某个班级两次语文成绩，验证是否有提高；对比某个产品在投放广告前后的销量，看广告是否有效。这些都属于两均值对比的应用。\n\n  均值对比的假设检验方法主要有Z检验和T检验，它们的区别在于Z检验面向总体数据和大样本数据，而T检验适用于小规模抽样样本。下面分别介绍Z检验和T检验。\n\n\n\n  Z检验虽然能够进行均值差异性检验，但是，它要求总体标准差已知或者样本容量足够大，这是很难做到甚至无法达成的。这时候t检验就粉墨登场了，只需从正态总体中抽取小规模的样本数据，并计算均值与标准差，用来代替正态总体的均值和标准差即可，t值计算公式如下：\n\n![img](http://mmbiz.qpic.cn/mmbiz_jpg/YJotEuBMe44mx3ssxoRZOJm8gm3iabbb2Du5IibbRLHljoJhekG7RyW2SHd6lGicZRXMMTtE4iaPhUKPSBeZYO3wyg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)\n\n  样本数据计算得到的所有t值就组成了新的数据序列，这个新的数据形态就是t分布。t分布是曲线族，曲线与自由度密切相关，自由度为n-k-1（这里n是样本容量，k是样本中已知变量个数），自由度越小，曲线越低平，三自由度越大，曲线越接近正态分布。\n\n\n\n  有了t分布和t值计算公式，我们就能够进行T检验了，T检验在数据分析中的用途非常广，它是针对满足正态分布的数据所采取的均值差异显著性的检验方法。\n\n  T检验在使用前有三个应用的注意点：\n\n1、分析的数据对象需要满足正态分布，T检验前需判断样本是否正态分布；\n\n2、分析对比的统计量是均值；\n\n3、对比对象是两个，可以是两个样本；也可以是一个样本和一个常数；\n\n\n\nT检验有四种类别：\n\n1、配对样本的T检验；\n\n2、等方差的独立样本T检验；\n\n3、异方差的独立样本T检验；\n\n4、单样本的T检验。T检验与Z检验不同，需要考虑样本方差是否相同，这是因为自由度决定了T分布曲线，同时，自由度也影响样本方差。下面分别介绍四种T检验的检验公式。\n\n1、配对样本的T检验\n\n  所谓配对样本的T检验，是指参与对比的两列数据都是满足正态分布，而且两列数据之间存在一一对应关系。要想判断这种数据序列之间的差异是否显著，就可以使用配对样本T检验。处于待检验状态的两列配对样本，应该具有相同的数据个数，而且两列数据在语义上有一一对应关系。例如对同一个班级的两次考试成绩，这两次成绩都按照学号顺序存放，具有明确的对应关系。T检验公式如下：\n\n![img](http://mmbiz.qpic.cn/mmbiz_jpg/YJotEuBMe44mx3ssxoRZOJm8gm3iabbb2qUP5qLfervUjMkLNXIPicqWzByRduB463j2CEAzHoQ8gPXkTr0Ie4tw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)\n\n独立样本T检验\n\n   独立样本是两个没有对应关系的独立正态分布数据集合，可以有不同的数据个数，例如，对同一学校的某次考试，如果需要检验男生与女生的成绩之间有无显著性差异在总体成绩满足正态分布的情况下，则都可以使用独立样本的T检验，但是在进行T检验之前，需要明确两个样本的方差是否相同，然后根据方差齐性与否选择相应的计算方法。\n\n2、等方差独立样本T检验\n\n![img](http://mmbiz.qpic.cn/mmbiz_jpg/YJotEuBMe44mx3ssxoRZOJm8gm3iabbb2qARzh5ib2IhtcibRfNoVKKVJxqa1yCrgbLT8lGFtDficI4fpryBRmwgvg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)\n\n3、异方差独立样本T检验\n\n![img](http://mmbiz.qpic.cn/mmbiz_jpg/YJotEuBMe44mx3ssxoRZOJm8gm3iabbb2OJJT3qIUcmZHPhUQibjZINgfOgdyr3PiadUoeSUwfLQoykgQicBEstRCA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)\n\n4、单样本T检验\n\n除了针对两列正态分布数据的均值差异显著性检验，有时还经常需要判断单列正态分布数据是否与某一给定值有显著性差异，或单列正态分布数据是否来自满足某一均值的总体。例如，判断某班语文成绩的均值是否与80分有显著性差别。T检验公式为：\n\n![img](http://mmbiz.qpic.cn/mmbiz_jpg/YJotEuBMe44mx3ssxoRZOJm8gm3iabbb2wU5G5u6VJ31sRqb2et6GIfC5qRXxQZ3yLBlpBDzVGSsAicHxOJY2GPg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)\n","tags":["数据挖掘"]},{"title":"T-test","url":"/2018/10/31/T-test/","content":"\n# 单样本T检验-ttest_1samp \n\n\n```python\nfrom scipy import stats\nimport numpy as np\nnp.random.seed(7654567)\nrvs = stats.norm.rvs(loc=5,scale=10,size=(100,2))  # 均值为5，方差为10\nr\n```\n\n检验两列数的均值与1和2的差异是否显著\n\n\n```python\nstats.ttest_1samp(rvs, [1, 2])   \n# 分别显示两列数的t统计量和p值。\n# 由p值分别为0.042和0.018，\n#  当p值小于0.05时，认为差异显著，即第一列数的均值不等于1，第二列数的均值不等于2。\n```\n\n\n\n\n    Ttest_1sampResult(statistic=array([3.71134343, 2.85084277]), pvalue=array([0.00034072, 0.00530677]))\n\n\n\n不拒绝原假设——均值等于5\n\n\n```python\nstats.ttest_1samp(rvs, 5.0) \n```\n\n\n\n\n    Ttest_1sampResult(statistic=array([-0.01631806, -0.25538594]), pvalue=array([0.98701349, 0.79895486]))\n\n\n\n拒绝原假设——均值不等于5\n\n\n```python\nstats.ttest_1samp(rvs, 0.0) \n```\n\n\n\n\n    Ttest_1sampResult(statistic=array([4.6432588 , 4.92166191]), pvalue=array([1.05510059e-05, 3.42688414e-06]))\n\n\n\n第一行数均值等于5，第二行数均值不等于0\naxis=0按列运算，axis=1按行运算\n\n\n```python\nstats.ttest_1samp(rvs.T,[5.0,0.0],axis=1)\n```\n\n\n\n\n    Ttest_1sampResult(statistic=array([-0.01631806,  4.92166191]), pvalue=array([9.87013492e-01, 3.42688414e-06]))\n\n\n\n#  两独立样本t检验-ttest_ind\n\n生成数据\n\n\n```python\nnp.random.seed(12345678)\n#loc:平均值  scale：方差\nrvs1 = stats.norm.rvs(loc=5,scale=10,size=500)  \nrvs2 = stats.norm.rvs(loc=5,scale=10,size=500)\n```\n\n当两总体方差相等时，即具有“方差齐性”，可以直接检验 \n不拒绝原假设——两总体均值相等\n\n\n```python\nstats.ttest_ind(rvs1,rvs2)\n```\n\n\n\n\n    Ttest_indResult(statistic=0.26833823296238857, pvalue=0.788494433695651)\n\n\n\n当不确定两总体方差是否相等时，应先利用levene检验，检验两总体是否具有方差齐性。\n\n\n```python\nstats.levene(rvs1, rvs2)\n```\n\n\n\n\n    LeveneResult(statistic=0.9775501222315258, pvalue=0.323044034693146)\n\n\n\np值远大于0.05，认为两总体具有方差齐性。\n\n如果两总体不具有方差齐性，需要将equal_val参数设定为“False”。\n\n需注意的情况：\n\n如果两总体具有方差齐性，错将equal_var设为False，p值变大\n\n\n```python\nstats.ttest_ind(rvs1,rvs2, equal_var = False)\n```\n\n\n\n\n    Ttest_indResult(statistic=0.26833823296238857, pvalue=0.7884945274950106)\n\n\n\n两总体方差不等时，若没有将equal_var参数设定为False，则函数会默认equal_var为True，这样会低估p值\n\n\n```python\nrvs3 = stats.norm.rvs(loc=5, scale=20, size=500)\nstats.ttest_ind(rvs1, rvs3, equal_var = False)\n# 正确的p值 \n```\n\n\n\n\n    Ttest_indResult(statistic=-0.46580283298287956, pvalue=0.6414964624656874)\n\n\n\n\n```python\nstats.ttest_ind(rvs1, rvs3)\n#  被低估的p值 \n```\n\n\n\n\n    Ttest_indResult(statistic=-0.46580283298287956, pvalue=0.6414582741343561)\n\n\n\n当两样本数量不等时，equal_val的变化会导致t统计量变化 \nrvs1：来自总体——均值5，方差10，样本数500 \nrvs4：来自总体——均值5，方差20，样本数100 \n两总体不具有方差齐性，应设定equal_var=False\n\n\n\n```python\nrvs4 = stats.norm.rvs(loc=5, scale=20, size=100)\nstats.ttest_ind(rvs1, rvs4)\n# 错误的t统计量 \n```\n\n\n\n\n    Ttest_indResult(statistic=-0.9988253944278285, pvalue=0.3182832709103878)\n\n\n\n\n```python\nstats.ttest_ind(rvs1, rvs4, equal_var = False) # 错误的t统计量 \n```\n\n\n\n\n    Ttest_indResult(statistic=-0.6971257058465435, pvalue=0.4871692772540187)\n\n\n\n不同均值，不同方差，不同样本量的t检验 \n错误的检验：未将equal_var设定为False\n\n\n```python\nrvs5 = stats.norm.rvs(loc=8, scale=20, size=100)\nstats.ttest_ind(rvs1, rvs5)\n```\n\n\n\n\n    Ttest_indResult(statistic=-1.467966985449067, pvalue=0.14263895620529113)\n\n\n\n正确的检验：\n\n\n```python\nstats.ttest_ind(rvs1, rvs5, equal_var = False)\n```\n\n\n\n\n    Ttest_indResult(statistic=-0.9436597361713308, pvalue=0.3474417033479409)\n\n\n\n# 配对样本t检验\n\n\n```python\nnp.random.seed(12345678)\n```\n\n不拒绝原假设，认为rvs1 与 rvs2 所代表的总体均值相等\n\n\n```python\nrvs1 = stats.norm.rvs(loc=5,scale=10,size=500)\nrvs2 = (stats.norm.rvs(loc=5,scale=10,size=500) + stats.norm.rvs(scale=0.2,size=500))\nstats.ttest_rel(rvs1,rvs2)\n```\n\n\n\n\n    Ttest_relResult(statistic=0.24101764965300979, pvalue=0.8096404344581155)\n\n\n\n拒绝原假设，认为rvs1 与 rvs3所代表的总体均值不相等\n\n\n```python\nrvs3 = (stats.norm.rvs(loc=8,scale=10,size=500) + stats.norm.rvs(scale=0.2,size=500))\nstats.ttest_rel(rvs1,rvs3)\n```\n\n\n\n\n    Ttest_relResult(statistic=-3.9995108708727924, pvalue=7.308240219166128e-05)\n\n\n\n","tags":["数据挖掘"]},{"title":"递归","url":"/2018/10/24/递归/","content":"# 递归\n\n递归是一种解决问题的方法，将问题分解为更小的子问题，直到得到一个足够小的问题可以被很简单的解决。通常递归涉及函数调用自身。递归允许我们编写优雅的解决方案，解决可能很难编程的问题。\n\n## 计算整数列表和\n\n我们先不用递归来实现求一个整数列表的和\n\n```python\ndef sum(list):\n    sum = 0\n    for item in list:\n        sum += item\n    return sum\nprint(sum([1,2,3,4,5]))\n        \n```\n\n```\n15\n```\n\n递归实现\n\n```python\ndef sum(list):\n    if len(list)==1:\n        return list[0]\n    return list[0]+sum(list[1:])\nprint(sum([1,2,3,4,5]))\n```\n\n```\n15\n```\n\n所有递归算法必须服从三个重要的定律：\n递归算法必须具有基本情况。\n递归算法必须改变其状态并向基本情况靠近。\n递归算法必须以递归方式调用自身。\n\n递归实现任意进制转化\n\n```python\ndef toStr(n,base):\n   convertString = \"0123456789ABCDEF\"\n   if n < base:\n      return convertString[n]\n   else:\n      return toStr(n//base,base) + convertString[n%base]\n\nprint(toStr(1453,16)[-0:])\n```\n\n```\n5AD\n```\n\n栈帧：实现递归\n\n```python\nclass Stack:\n     def __init__(self):\n         self.items = []\n\n     def isEmpty(self):\n         return self.items == []\n\n     def push(self, item):\n         self.items.append(item)\n\n     def pop(self):\n         return self.items.pop()\n\n     def peek(self):\n         return self.items[len(self.items)-1]\n\n     def size(self):\n         return len(self.items)\n```\n\n```python\nrStack = Stack()\n\ndef toStr(n,base):\n    convertString = \"0123456789ABCDEF\"\n    while n > 0:\n        if n < base:\n            rStack.push(convertString[n])\n        else:\n            rStack.push(convertString[n % base])\n        n = n // base\n    res = \"\"\n    while not rStack.isEmpty():\n        res = res + str(rStack.pop())\n    return res\n\nprint(toStr(1453,16))\n```\n\n```\n5AD\n```\n\n可视化递归\n\n```python\nimport turtle\n\nmyTurtle = turtle.Turtle()\nmyWin = turtle.Screen()\n\ndef drawSpiral(myTurtle, lineLen):\n    if lineLen > 0:\n        myTurtle.forward(lineLen)\n        myTurtle.right(90)\n        drawSpiral(myTurtle,lineLen-5)\n\ndrawSpiral(myTurtle,300)\nmyWin.exitonclick()\n```\n\n谢尔宾斯基三角形\n\n```python\nimport turtle\n\ndef drawTriangle(points,color,myTurtle):\n    myTurtle.fillcolor(color)\n    myTurtle.up()\n    myTurtle.goto(points[0][0],points[0][1])\n    myTurtle.down()\n    myTurtle.begin_fill()\n    myTurtle.goto(points[1][0],points[1][1])\n    myTurtle.goto(points[2][0],points[2][1])\n    myTurtle.goto(points[0][0],points[0][1])\n    myTurtle.end_fill()\n\ndef getMid(p1,p2):\n    return ( (p1[0]+p2[0]) / 2, (p1[1] + p2[1]) / 2)\n\ndef sierpinski(points,degree,myTurtle):\n    colormap = ['blue','red','green','white','yellow',\n                'violet','orange']\n    drawTriangle(points,colormap[degree],myTurtle)\n    if degree > 0:\n        sierpinski([points[0],\n                        getMid(points[0], points[1]),\n                        getMid(points[0], points[2])],\n                   degree-1, myTurtle)\n        sierpinski([points[1],\n                        getMid(points[0], points[1]),\n                        getMid(points[1], points[2])],\n                   degree-1, myTurtle)\n        sierpinski([points[2],\n                        getMid(points[2], points[1]),\n                        getMid(points[0], points[2])],\n                   degree-1, myTurtle)\n\ndef main():\n   myTurtle = turtle.Turtle()\n   myWin = turtle.Screen()\n   myPoints = [[-300,-150],[0,300],[300,-150]]\n   sierpinski(myPoints,5,myTurtle)\n   myWin.exitonclick()\n\nmain()\n```\n","tags":["算法"]},{"title":"链表","url":"/2018/10/24/链表/","content":"# 链表\n\n链表实现的基本构造块是节点。每个节点对象必须至少保存两个信息。\n首先，节点必须包含列表项本身。我们将这个称为节点的数据字段。此外，每个节点必须保存对下一个节点的引用。\n\n## 无序链表\n\n```python\nclass Node():\n    def __init__(self,initdata):\n        self.data = initdata\n        self.next = None\n\n    def getData(self):\n        return self.data\n\n    def getNext(self):\n        return self.next\n\n    def setData(self,newdata):\n        self.data = newdata\n\n    def setNext(self,newnext):\n        self.next = newnext\n```\n\n```python\ntmp=Node(99)\ntmp.getData()\n```\n\n\n\n```\n99\n```\n\n\n\n```python\nclass UnorderedList():\n\n    def __init__(self):\n        self.head = None\n        \n    def isEmpty(self):\n        return self.head == None\n\n    def add(self,item):\n        temp = Node(item)\n        temp.setNext(self.head)\n        self.head = temp\n    \n    def size(self):\n        current = self.head\n        count = 0\n        while current != None:\n            count = count + 1\n            current = current.getNext()\n        return count\n    \n    def search(self,item):\n        current = self.head\n        found = False\n        while current != None and not found:\n            if current.getData() == item:\n                found = True\n            else:\n                current = current.getNext()\n\n        return found\n\n    def remove(self,item):\n        current=self.head\n        previous=None\n        found = False\n        while current != None and not found:\n            if current.getData() == item:\n                found = True\n            else:\n                previous=current\n                current= current.getNext()\n        if previous == None:\n            self.head = current.getNext()\n        else:\n            previous.setNext(current.getNext())\n\n```\n\n```python\nmylist = UnorderedList()\n\n```\n\n```python\nmylist.add(13)\nmylist.add(23)\n```\n\n```python\nmylist.size()\n```\n\n\n\n```\n2\n```\n\n\n\n```python\nmylist.search(13)\n```\n\n\n\n```\nTrue\n```\n\n\n\n```python\nmylist.remove(23)\n```\n\n```python\nmylist.search(23)\n```\n\n\n\n```\nFalse\n```\n\n\n\n## 有序列表\n\n```python\nclass OrderedList:\n    def __init__(self):\n        self.head = None\n               \n    def isEmpty(self):\n        return self.head == None\n\n    def add(self,item):\n        temp = Node(item)\n        temp.setNext(self.head)\n        self.head = temp\n    \n    def size(self):\n        current = self.head\n        count = 0\n        while current != None:\n            count = count + 1\n            current = current.getNext()\n        return count\n        \n        \n    def search(self,item):\n        current = self.head\n        found = False\n        stop = False\n        while current != None and not found and not stop:\n            if current.getData() == item:\n                found = True\n            else:\n                if current.getData() > item:\n                    stop = True\n                else:\n                    current = current.getNext()\n\n        return found\n    \n    def add(self,item):\n        current = self.head\n        previous = None\n        stop = False\n        while current != None and not stop:\n            if current.getData() > item:\n                stop = True\n            else:\n                previous = current\n                current = current.getNext()\n\n        temp = Node(item)\n        if previous == None:\n            temp.setNext(self.head)\n            self.head = temp\n        else:\n            temp.setNext(current)\n            previous.setNext(temp)\n```\n","tags":["算法"]},{"title":"deque","url":"/2018/10/22/deque/","content":"# Deque抽象数据类型\n\ndeque 抽象数据类型由以下结构和操作定义。如上所述，deque 被构造为项的有序集合，其中项从首部或尾部的任一端添加和移除。下面给出了 deque 操作。\n\n1. Deque() 创建一个空的新 deque。它不需要参数，并返回空的 deque。\n2. addFront(item) 将一个新项添加到 deque 的首部。它需要 item 参数 并不返回任何内容。\n3. addRear(item) 将一个新项添加到 deque 的尾部。它需要 item 参数并不返回任何内容。\n4. removeFront() 从 deque 中删除首项。它不需要参数并返回 item。deque 被修改。\n5. removeRear() 从 deque 中删除尾项。它不需要参数并返回 item。deque 被修改。\n6. isEmpty() 测试 deque 是否为空。它不需要参数，并返回布尔值。\n7. size() 返回 deque 中的项数。它不需要参数，并返回一个整数。\n\n## Python实现Deque\n\n```python\nclass Deque:\n    def __init__(self):\n        self.items = []\n\n    def isEmpty(self):\n        return self.items == []\n\n    def addFront(self, item):\n        self.items.append(item)\n\n    def addRear(self, item):\n        self.items.insert(0,item)\n\n    def removeFront(self):\n        return self.items.pop()\n\n    def removeRear(self):\n        return self.items.pop(0)\n\n    def size(self):\n        return len(self.items)\n```\n\n在 removeFront 中，我们使用 pop 方法从列表中删除最后一个元素。 但是，在removeRear中，pop(0)方法必须删除列表的第一个元素。同样，我们需要在 addRear 中使用insert方法（第12行），因为 append 方法在列表的末尾添加一个新元素。\n你可以看到许多与栈和队列中描述的 Python 代码相似之处。你也可能观察到，在这个实现中，从前面添加和删除项是 O(1)，而从后面添加和删除是 O(n)。 考虑到添加和删除项是出现的常见操作，这是可预期的。 同样，重要的是要确定我们知道在实现中前后都分配在哪里。\n\n## 回文检查\n\n使用 deque 数据结构可以容易地解决经典回文问题。回文是一个字符串，读取首尾相同的字符，例如，radar toot madam。 我们想构造一个算法输入一个字符串，并检查它是否是一个回文。\n该问题的解决方案将使用 deque 来存储字符串的字符。我们从左到右处理字符串，并将每个字符添加到 deque 的尾部。在这一点上，deque 像一个普通的队列。然而，我们现在可以利用 deque 的双重功能。 deque 的首部保存字符串的第一个字符，deque 的尾部保存最后一个字符\n![](https://xidianwlc.gitbooks.io/python-data-structrue-and-algrothms/content/3.%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/3.18.%E5%9B%9E%E6%96%87%E6%A3%80%E6%9F%A5/assets/3.18.%E5%9B%9E%E6%96%87%E6%A3%80%E6%9F%A5.figure2.png)\n\n```python\ndef palchecker(aString):\n    chardeque = Deque()\n\n    for ch in aString:\n        chardeque.addRear(ch)\n\n    stillEqual = True\n\n    while chardeque.size() > 1 and stillEqual:\n        first = chardeque.removeFront()\n        last = chardeque.removeRear()\n        if first != last:\n            stillEqual = False\n\n    return stillEqual\n\nprint(palchecker(\"lsdkjfskf\"))\nprint(palchecker(\"radar\"))\n```\n\n```\nFalse\nTrue\n```\n\n\n","tags":["算法"]},{"title":"队列","url":"/2018/10/22/队列/","content":"# 队列\n\n我们为了实现队列抽象数据类型创建一个新类。和前面一样，我们将使用列表集合来作为构建队列的内部表示。\n\n我们需要确定列表的哪一端作为队首，哪一端作为队尾。\n如下所示的实现假定队尾在列表中的位置为 0。\n这允许我们使用列表上的插入函数向队尾添加新元素。弹出操作可用于删除队首的元素（列表的最后一个元素）。回想一下，这也意味着入队为 O(n)，出队为 O(1)。\n\n```python\nclass Queue():\n    def __init__(self):\n        self.items=[]\n    def isEmpty(self):\n        return self.items == []\n    def enqueue(self,item):\n        self.items.insert(0,item)\n    def dequeue(self):\n        return self.items.pop()\n    def size(self):\n        return len(self.items)\n    \n\n```\n\n## 初始化队列以及基础应用\n\n```python\nq = Queue()\nq.isEmpty()\n\n```\n\n\n\n```\nTrue\n```\n\n\n\n```python\nq.enqueue(1)\nq.dequeue()\n```\n\n\n\n```\n1\n```\n\n\n\n```python\nfor i in range(10):\n    q.enqueue(i)\n```\n\n```python\nq.isEmpty()\n```\n\n\n\n```\nFalse\n```\n\n\n\n```python\nq.size()\n```\n\n\n\n```\n10\n```\n\n\n\n```python\nwhile not q.isEmpty():\n    print(q.dequeue())\n```\n\n```\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n```\n\n## 队列的应用\n\n### 烫手山芋\n\n队列的典型应用之一是模拟需要以 FIFO 方式管理数据的真实场景。首先，让我们看看孩子们的游戏烫手山芋，在这个游戏中，孩子们围成一个圈，并尽可能快的将一个山芋递给旁边的孩子。在某一个时间，动作结束，有山芋的孩子从圈中移除。游戏继续开始直到剩下最后一个孩子。\n![](https://xidianwlc.gitbooks.io/python-data-structrue-and-algrothms/content/3.%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/3.13.%E6%A8%A1%E6%8B%9F%EF%BC%9A%E7%83%AB%E6%89%8B%E5%B1%B1%E8%8A%8B/assets/3.13.%E6%A8%A1%E6%8B%9F%EF%BC%9A%E7%83%AB%E6%89%8B%E5%B1%B1%E8%8A%8B.figure2.png)\n\n为了模拟这个圈，我们使用队列。假设拿着山芋的孩子在队列的前面。当拿到山芋的时候，这个孩子将先出列再入队列，把他放在队列的最后。经过 num 次的出队入队后，前面的孩子将被永久移除队列。并且另一个周期开始，继续此过程，直到只剩下一个名字（队列的大小为 1）\n![](https://xidianwlc.gitbooks.io/python-data-structrue-and-algrothms/content/3.%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/3.13.%E6%A8%A1%E6%8B%9F%EF%BC%9A%E7%83%AB%E6%89%8B%E5%B1%B1%E8%8A%8B/assets/3.13.%E6%A8%A1%E6%8B%9F%EF%BC%9A%E7%83%AB%E6%89%8B%E5%B1%B1%E8%8A%8B.figure3.png)\n\n```python\ndef hotPotato(namelist, num):\n    simqueue = Queue()\n    for name in namelist:\n        simqueue.enqueue(name)\n\n    while simqueue.size() > 1:\n        for i in range(num):\n            simqueue.enqueue(simqueue.dequeue())\n\n        simqueue.dequeue()\n\n    return simqueue.dequeue()\n\nprint(hotPotato([\"Bill\",\"David\",\"Susan\",\"Jane\",\"Kent\",\"Brad\"],7))\n```\n\n请注意，在此示例中，计数常数的值大于列表中的名称数。这不是一个问题，因为队列像一个圈，计数会重新回到开始，直到达到计数值。另外，请注意，列表加载到队列中以使列表上的名字位于队列的前面。在这种情况下，Bill 是列表中的第一个项，因此他在队列的前面。\n\n### 打印机\n\n主要模拟步骤\n\n1. 创建打印任务的队列，每个任务都有个时间戳。队列启动的时候为空。\n2. 每秒（currentSecond）：\n   - 是否创建新的打印任务？如果是，将 currentSecond 作为时间戳添加到队列。\n   - 如果打印机不忙并且有任务在等待\n     - 从打印机队列中删除一个任务并将其分配给打印机\n     - 从 currentSecond 中减去时间戳，以计算该任务的等待时间。\n     - 将该任务的等待时间附件到列表中稍后处理。\n     - 根据打印任务的页数，确定需要多少时间。\n   - 打印机需要一秒打印，所以得从该任务的所需的等待时间减去一秒。\n   - 如果任务已经完成，换句话说，所需的时间已经达到零，打印机空闲。\n3. 模拟完成后，从生成的等待时间列表中计算平均等待时间。\n\n为了设计此模拟，我们将为上述三个真实世界对象创建类：Printer, Task, PrintQueue\n\nPrinter 类需要跟踪它当前是否有任务。\n\n如果有，则它处于忙碌状态（13-17 行），并且可以从任务的页数计算所需的时间。\n\n构造函数允许初始化每分钟页面的配置，tick 方法将内部定时器递减直到打印机设置为空闲(11 行)\n\n```python\nclass Printer:\n    def __init__(self, ppm):\n        self.pagerate = ppm\n        self.currentTask = None\n        self.timeRemaining = 0\n\n    def tick(self):\n        if self.currentTask != None:\n            self.timeRemaining = self.timeRemaining - 1\n            if self.timeRemaining <= 0:\n                self.currentTask = None\n\n    def busy(self):\n        if self.currentTask != None:\n            return True\n        else:\n            return False\n\n    def startNext(self,newtask):\n        self.currentTask = newtask\n        self.timeRemaining = newtask.getPages() * 60/self.pagerate\n```\n\nTask 类表示单个打印任务。创建任务时，随机数生成器将提供 1 到 20 页的长度。我们选择使用随机模块中的 randrange 函数\n每个任务还需要保存一个时间戳用于计算等待时间。此时间戳将表示任务被创建并放置到打印机队列中的时间。可以使用 waitTime 方法来检索在打印开始之前队列中花费的时间。\n\n```python\nimport random\n\nclass Task:\n    def __init__(self,time):\n        self.timestamp = time\n        self.pages = random.randrange(1,21)\n\n    def getStamp(self):\n        return self.timestamp\n\n    def getPages(self):\n        return self.pages\n\n    def waitTime(self, currenttime):\n        return currenttime - self.timestamp\n```\n\n以下代码实现了上述算法。PrintQueue 对象是我们现有队列 ADT 的一个实例。\nnewPrintTask 决定是否创建一个新的打印任务。我们再次选择使用随机模块的 randrange 函数返回 1 到 180 之间的随机整数。\n打印任务每 180 秒到达一次。通过从随机整数（32 行）的范围中任意选择，我们可以模拟这个随机事件。\n模拟功能允许我们设置打印机的总时间和每分钟的页数。\n\n```python\ndef simulation(numSeconds, pagesPerMinute):\n\n    labprinter = Printer(pagesPerMinute)\n    printQueue = Queue()\n    waitingtimes = []\n\n    for currentSecond in range(numSeconds):\n\n      if newPrintTask():\n         task = Task(currentSecond)\n         printQueue.enqueue(task)\n\n      if (not labprinter.busy()) and (not printQueue.isEmpty()):\n        nexttask = printQueue.dequeue()\n        waitingtimes.append(nexttask.waitTime(currentSecond))\n        labprinter.startNext(nexttask)\n\n      labprinter.tick()\n\n    averageWait=sum(waitingtimes)/len(waitingtimes)\n    print(\"Average Wait %6.2f secs %3d tasks remaining.\"%(averageWait,printQueue.size()))\n\ndef newPrintTask():\n    num = random.randrange(1,181)\n    if num == 180:\n        return True\n    else:\n        return False\n\n\n```\n当我们运行模拟时，我们不应该担心每次的结果不同。这是由于随机数的概率性质决定的。 因为模拟的参数可以被调整，我们对调整后可能发生的趋势感兴趣。 这里有一些结果。\n首先，我们将使用每分钟五页的页面速率运行模拟 60 分钟（3,600秒）。 此外，我们将进行 10 次独立试验。记住，因为模拟使用随机数，每次运行将返回不同的结果。\n\n```python\nfor i in range(10):\n    simulation(3600,5)\n```\n\n```\nAverage Wait 133.21 secs   0 tasks remaining.\nAverage Wait  69.95 secs   1 tasks remaining.\nAverage Wait  18.21 secs   0 tasks remaining.\nAverage Wait 156.15 secs   1 tasks remaining.\nAverage Wait 124.05 secs   1 tasks remaining.\nAverage Wait 194.70 secs   4 tasks remaining.\nAverage Wait  48.24 secs   1 tasks remaining.\nAverage Wait  80.65 secs   0 tasks remaining.\nAverage Wait  62.31 secs   0 tasks remaining.\nAverage Wait  85.43 secs   2 tasks remaining.\n```\n\n\n","tags":["算法"]},{"title":"数据挖掘基本知识","url":"/2018/10/22/数据挖掘基本知识/","content":"# 数据挖掘的基础知识点\n\n-  数据、信息和知识是广义数据表现的不同形式\n\n-  主要知识模式类型有：广义知识，关联知识，类知识，预测型知识，特异型知识\n\n\n\n-  web挖掘研究的主要流派有：Web结构挖掘、Web使用挖掘、Web内容挖掘\n\n- 一般地说，KDD是一个多步骤的处理过程，一般分为**问题定义**、**数据抽取**、**数据预处理**、.**数据挖掘**以及**模式评估**等基本阶段。\n\n- 数据库中的知识发现处理过程模型有：阶梯处理过程模型，螺旋处理过程模型，以用户为中心的处理结构模型，联机KDD模型，支持多数据源多知识模式的KDD处理模型\n-  粗略地说，知识发现软件或工具的发展经历了独立的知识发现软件、横向的知识发现工具集和纵向的知识发现解决方案三个主要阶段，其中后面两种反映了目前知识发现软件的两个主要发展方向。\n\n-  决策树分类模型的建立通常分为两个步骤：决策树生成，决策树修剪。\n\n\n\n-  从使用的主要技术上看，可以把分类方法归结为四种类型：\n\n  - ​\t基于距离的分类方法\n\n  - ​\t决策树分类方法\n\n  - ​\t贝叶斯分类方法\n\n  - ​\t规则归纳方法\n\n-  关联规则挖掘问题可以划分成两个子问题：\n  1. 发现频繁项目集:通过用户给定Minsupport ，寻找所有频繁项目集或者最大频繁项目集。\n  2. 生成关联规则:通过用户给定Minconfidence ，在频繁项目集中，寻找关联规则。\n\n- 数据挖掘是相关学科充分发展的基础上被提出和发展的，主要的相关技术：\n  - 数据库等信息技术的发展\n  - 统计学深入应用\n  - 人工智能技术的研究和应用\n\n \n\n- 衡量关联规则挖掘结果的有效性，应该从多种综合角度来考虑：\n  - 准确性：挖掘出的规则必须反映数据的实际情况。\n  - 实用性：挖掘出的规则必须是简洁可用的。\n  - 新颖性：挖掘出的关联规则可以为用户提供新的有价值信息。\n\n-  约束的常见类型有：\n\n  ​\t单调性约束;\n\n  ​\t反单调性约束;\n\n  ​\t可转变的约束;\n\n  ​\t简洁性约束.\n\n- 根据规则中涉及到的层次，多层次关联规则可以分为：\n\n  ​\t同层关联规则：如果一个关联规则对应的项目是同一个粒度层次，那么它是同层关联规则。\n\n  ​\t层间关联规则：如果在不同的粒度层次上考虑问题，那么可能得到的是层间关联规\n\n \n\n-  按照聚类分析算法的主要思路，聚类方法可以被归纳为如下几种。\n\n  - 划分法：基于一定标准构建数据的划分。\n\n  - 属于该类的聚类方法有：k-means、k-modes、k-prototypes、k-medoids、PAM、CLARA、CLARANS等。\n  - 层次法：对给定数据对象集合进行层次的分解\n  - 密度法：基于数据对象的相连密度评价。\n  - 网格法：将数据空间划分成为有限个单元(Cell)的网格结构，基于网格结构进行聚类。\n  - 模型法：给每一个簇假定一个模型，然后去寻找能够很好的满足这个模型的数据集。\n\n- 类间距离的度量主要有：\n  - 最短距离法：定义两个类中最靠近的两个元素间的距离为类间距离。\n  - 最长距离法：定义两个类中最远的两个元素间的距离为类间距离。\n  - 中心法：定义两类的两个中心间的距离为类间距离。\n  - 类平均法：它计算两个类中任意两个元素间的距离，并且综合他们为类间距离：离差平方和。\n\n \n\n- 层次聚类方法具体可分为：\n  - 凝聚的层次聚类：一种自底向上的策略，首先将每个对象作为一个簇，然后合并这些原子簇为越来越大的簇，直到某个终结条件被满足。\n  - 分裂的层次聚类：采用自顶向下的策略，它首先将所有对象置于一个簇中，然后逐渐细分为越来越小的簇，直到达到了某个终结条件。\n  - 层次凝聚的代表是AGNES算法。层次分裂的代表是DIANA算法。\n\n-   文本挖掘(TD)的方式和目标是多种多样的，基本层次有：\n  - 关键词检索：最简单的方式，它和传统的搜索技术类似。\n  - 挖掘项目关联：聚焦在页面的信息(包括关键词)之间的关联信息挖掘上。\n  - 信息分类和聚类：利用数据挖掘的分类和聚类技术实现页面的分类，将页面在一个更到层次上进行抽象和整理。\n  - 自然语言处理：揭示自然语言处理技术中的语义，实现Web内容的更精确处理。 \n\n-  在web访问挖掘中常用的技术：\n  - 路径分析\n    - 路径分析最常用的应用是用于判定在一个Web站点中最频繁访问的路径，这样的知识对于一个电子商务网站或者信息安全评估是非常重要的。\n  - 关联规则发现\n    - 使用关联规则发现方法可以从Web访问事务集中，找到一般性的关联知识。\n  - 序列模式发现\n    - 在时间戳有序的事务集中，序列模式的发现就是指找到那些如“一些项跟随另一个项”这样的内部事务模式。\n  - 分类\n    - 发现分类规则可以给出识别一个特殊群体的公共属性的描述。这种描述可以用于分类新的项。\n  - 聚类\n    - 可以从Web Usage数据中聚集出具有相似特性的那些客户。在Web事务日志中，聚类顾客信息或数据项，就能够便于开发和执行未来的市场战略。\n\n\n\n- 根据功能和侧重点不同，数据挖掘语言可以分为三种类型：\n  - 数据挖掘查询语言：希望以一种像SQL这样的数据库查询语言完成数据挖掘的任务。\n  - 数据挖掘建模语言：对数据挖掘模型进行描述和定义的语言，设计一种标准的数据挖掘建模语言，使得数据挖掘系统在模型定义和描述方面有标准可以遵循。\n  - 通用数据挖掘语言：通用数据挖掘语言合并了上述两种语言的特点，既具有定义模型的功能，又能作为查询语言与数据挖掘系统通信，进行交互式挖掘。通用数据挖掘语言标准化是目前解决数据挖掘行业出现问题的颇具吸引力的研究方向。\n\n \n\n- 规则归纳有四种策略：减法、加法，先加后减、先减后加策略。\n  - 减法策略：以具体例子为出发点，对例子进行推广或泛化，推广即减除条件(属性值)或减除合取项(为了方便，我们不考虑增加析取项的推广)，使推广后的例子或规则不覆盖任何反例。\n  - 加法策略：起始假设规则的条件部分为空(永真规则)，如果该规则覆盖了反例，则不停地向规则增加条件或合取项，直到该规则不再覆盖反例。\n  - 先加后减策略：由于属性间存在相关性，因此可能某个条件的加入会导致前面加入的条件没什么作用，因此需要减除前面的条件。\n  - 先减后加策略：道理同先加后减，也是为了处理属性间的相关性。\n\n \n\n-  数据挖掘定义有广义和狭义之分。\n  - 从广义的观点，数据挖掘是从大型数据集(可能是不完全的、有噪声的、不确定性的、各种存储形式的)中，挖掘隐含在其中的、人们事先不知道的、对决策有用的知识的过程。\n  - 从这种狭义的观点上，我们可以定义数据挖掘是从特定形式的数据集中提炼知识的过程。\n\n \n\n- web挖掘的含义： \n  - 针对包括Web页面内容、页面之间的结构、用户访问信息、电子商务信息等在内的各种Web数据，应用数据挖掘方法以帮助人们从因特网中提取知识，为访问者、站点经营者以及包括电子商务在内的基于因特网的商务活动提供决策支持。\n\n\n\n- K-近邻分类算法(K Nearest Neighbors，简称KNN)的定义：通过计算每个训练数据到待分类元组的距离，取和待分类元组距离最近的K个训练数据，K个数据中哪个类别的训练数据占多数，则待分类元组就属于哪个类别。\n\n- K-means算法的性能分析：\n\n  主要优点：\n\n  - 是解决聚类问题的一种经典算法，简单、快速。\n  - 对处理大数据集，该算法是相对可伸缩和高效率的。\n  - 当结果簇是密集的，它的效果较好。\n\n  主要缺点\n\n  - 在簇的平均值被定义的情况下才能使用，可能不适用于某些应用。\n  - 必须事先给出k(要生成的簇的数目)，而且对初值敏感，对于不同的初始值，可能会导致不同结果。\n  - 不适合于发现非凸面形状的簇或者大小差别很大的簇。而且，它对于“躁声”和孤立点数据是敏感的。\n\n- ID3算法的性能分析：\n  - ID3算法的假设空间包含所有的决策树，它是关于现有属性的有限离散值函数的一个完整空间。所以ID3算法避免了搜索不完整假设空间的一个主要风险：假设空间可能不包含目标函数。\n  - ID3算法在搜索的每一步都使用当前的所有训练样例，大大降低了对个别训练样例错误的敏感性。因此，通过修改终止准则，可以容易地扩展到处理含有噪声的训练数据。\n  - ID3算法在搜索过程中不进行回溯。所以，它易受无回溯的爬山搜索中的常见风险影响：收敛到局部最优而不是全局最优。\n\n \n\n- Apriori算法有两个致命的性能瓶颈:\n\n  - 多次扫描事务数据库，需要很大的I/O负载：\n\n    对每次k循环，侯选集Ck中的每个元素都必须通过扫描数据库一次来验证其是否加入Lk。假如有一个频繁大项目集包含10个项的话，那么就至少需要扫描事务数据库10遍。\n\n  - 可能产生庞大的侯选集：\n\n    由Lk-1产生k-侯选集Ck是指数增长的，例如104个1-频繁项目集就有可能产生接近107个元素的2-侯选集。如此大的侯选集对时间和主存空间都是一种挑战。a基于数据分割的方法：基本原理是“在一个划分中的支持度小于最小支持度的k-项集不可能是全局频繁的”。\n\n\n\n- 改善Apriori算法适应性和效率的主要的改进方法有：\n  - 基于数据分割(Partition)的方法：基本原理是“在一个划分中的支持度小于最小支持度的k-项集不可能是全局频繁的”。\n  - 基于散列的方法：基本原理是“在一个hash桶内支持度小于最小支持度的k-项集不可能是全局频繁的”。\n  - 基于采样的方法：基本原理是“通过采样技术，评估被采样的子集中，并依次来估计k-项集的全局频度”。\n  - 其他：如，动态删除没有用的事务：“不包含任何Lk的事务对未来的扫描结果不会产生影响，因而可以删除”。\n\n \n\n-  面向Web的数据挖掘比面向数据库和数据仓库的数据挖掘要复杂得多：\n  - 异构数据源环境：Web网站上的信息是异构: 每个站点的信息和组织都不一样;存在大量的无结构的文本信息、复杂的多媒体信息;站点使用和安全性、私密性要求各异等等。\n  - 数据的是复杂性：有些是无结构的(如Web页)，通常都是用长的句子或短语来表达文档类信息;有些可能是半结构的(如Email，HTML页)。当然有些具有很好的结构(如电子表格)。揭开这些复合对象蕴涵的一般性描述特征成为数据挖掘的不可推卸的责任。\n  - 动态变化的应用环境：\n  - Web的信息是频繁变化的，像新闻、股票等信息是实时更新的。\n  - 这种高变化也体现在页面的动态链接和随机存取上。\n  - Web上的用户是难以预测的。\n  - Web上的数据环境是高噪音的。\n\n- 简述知识发现项目的过程化管理I-MIN过程模型。\n  - MIN过程模型把KDD过程分成IM1、IM2、…、IM6等步骤处理，在每个步骤里，集中讨论几个问题，并按一定的质量标准来控制项目的实施。\n  - IM1任务与目的：它是KDD项目的计划阶段，确定企业的挖掘目标，选择知识发现模式，编译知识发现模式得到的元数据;其目的是将企业的挖掘目标嵌入到对应的知识模式中。\n  - IM2任务与目的：它是KDD的预处理阶段，可以用IM2a、IM2b、IM2c等分别对应于数据清洗、数据选择和数据转换等阶段。其目的是生成高质量的目标数据。\n  - IM3任务与目的：它是KDD的挖掘准备阶段，数据挖掘工程师进行挖掘实验，反复测试和验证模型的有效性。其目的是通过实验和训练得到浓缩知识(Knowledge Concentrate)，为最终用户提供可使用的模型。\n  - IM4任务与目的：它是KDD的数据挖掘阶段，用户通过指定数据挖掘算法得到对应的知识。\n  - IM5任务与目的：它是KDD的知识表示阶段，按指定要求形成规格化的知识。\n  - IM6任务与目的：它是KDD的知识解释与使用阶段，其目的是根据用户要求直观地输出知识或集成到企业的知识库中。\n\n-  改善Apriori算法适应性和效率的主要的改进方法有：\n  - 基于数据分割(Partition)的方法：基本原理是“在一个划分中的支持度小于最小支持度的k-项集不可能是全局频繁的”。\n  - 基于散列(Hash)的方法：基本原理是“在一个hash桶内支持度小于最小支持度的k-项集不可能是全局频繁的”。\n  - 基于采样(Sampling)的方法：基本原理是“通过采样技术，评估被采样的子集中，并依次来估计k-项集的全局频度”。\n  - 其他：如，动态删除没有用的事务：“不包含任何Lk的事务对未来的扫描结果不会产生影响，因而可以删除”。\n\n \n\n- 数据分类的两个步骤是什么?\n  - 建立一个模型，描述预定的数据类集或概念集\n  - 数据元组也称作样本、实例或对象。\n  - 为建立模型而被分析的数据元组形成训练数据集。\n  - 训练数据集中的单个元组称作训练样本，由于提供了每个训练样本的类标号，因此也称作有指导的学习。\n  - 通过分析训练数据集来构造分类模型，可用分类规则、决策树或数学公式等形式提供。\n  - 使用模型进行分类\n  - 首先评估模型(分类法)的预测准确率。\n  - 如果认为模型的准确率可以接受，就可以用它对类标号未知的数据元组或对象进行分类。\n\n- web访问信息挖掘的特点：\n  - Web访问数据容量大、分布广、内涵丰富和形态多样\n  - 一个中等大小的网站每天可以记载几兆的用户访问信息。\n  - 广泛分布于世界各处。\n  - 访问信息形态多样。\n  - 访问信息具有丰富的内涵。\n  - Web访问数据包含决策可用的信息\n  - 每个用户的访问特点可以被用来识别该用户和网站访问的特性。\n  - 同一类用户的访问，代表同一类用户的个性。\n  - 一段时期的访问数据代表了群体用户的行为和群体用户的共性。\n  - Web访问信息数据是网站的设计者和访问者进行沟通的桥梁。\n  - Web访问信息数据是开展数据挖掘研究的良好的对象。\n  - Web访问信息挖掘对象的特点\n  - 访问事务的元素是Web页面，事务元素之间存在着丰富的结构信息。\n  - 访问事务的元素代表的是每个访问者的顺序关系，事务元素之间存在着丰富的顺序信息。\n  - 每个页面的内容可以被抽象出不同的概念，访问顺序和访问量部分决定概念。\n  - 用户对页面存在不同的访问时长，访问长代表了用户的访问兴趣。\n\n \n\n- web页面内文本信息的挖掘：\n  - 挖掘的目标是对页面进行摘要和分类。\n    - 页面摘要：对每一个页面应用传统的文本摘要方法可以得到相应的摘要信息。\n    - 页面分类：分类器输入的是一个Web页面集(训练集)，再根据页面文本信息内容进行监督学习，然后就可以把学成的分类器用于分类每一个新输入的页面。\n\n{在文本学习中常用的方法是TFIDF向量表示法，它是一种文档的词集(Bag-of-Words)表示法，所有的词从文档中抽取出来，而不考虑词间的次序和文本的结构。这种构造二维表的方法是：\n\n- 每一列为一个词，列集(特征集)为辞典中的所有有区分价值的词，所以整个列集可能有几十万列之多。\n- 每一行存储一个页面内词的信息，这时，该页面中的所有词对应到列集(特征集)上。列集中的每一个列(词)，如果在该页面中不出现，则其值为0;如果出现k次，那么其值就为k;页面中的词如果不出现在列集上，可以被放弃。这种方法可以表征出页面中词的频度。\n\n对中文页面来说，还需先分词然后再进行以上两步处理。\n\n这样构造的二维表表示的是Web页面集合的词的统计信息，最终就可以采用Naive Bayesian方法或k-Nearest Neighbor等方法进行分类挖掘。\n\n在挖掘之前，一般要先进行特征子集的选取，以降低维数。\n\n**转自：数据在线；**\n\n\n","tags":["机器学习"]},{"title":"栈应用","url":"/2018/10/21/栈应用/","content":"# 栈应用\n\n## 栈数据结构的定义\n\n```python\nclass Stack:\n     def __init__(self):\n         self.items = []\n\n     def isEmpty(self):\n         return self.items == []\n\n     def push(self, item):\n         self.items.append(item)\n\n     def pop(self):\n         return self.items.pop()\n\n     def peek(self):\n         return self.items[len(self.items)-1]\n\n     def size(self):\n         return len(self.items)\n```\n\n```python\ns=Stack()\n\nprint(s.isEmpty())\ns.push(4)\ns.push('dog')\nprint(s.peek())\ns.push(True)\nprint(s.size())\nprint(s.isEmpty())\ns.push(8.4)\nprint(s.pop())\nprint(s.pop())\n```\n\n```\nTrue\ndog\n3\nFalse\n8.4\nTrue\n```\n\n## 简单的符号匹配\n\n```python\ndef parChecker(symbolString):\n    s = Stack()\n    balanced = True\n    index = 0\n    while index < len(symbolString) and balanced:\n        symbol = symbolString[index]\n        if symbol == \"(\":\n            s.push(symbol)\n        else:\n            if s.isEmpty():\n                balanced = False\n            else:\n                s.pop()\n\n        index = index + 1\n\n    if balanced and s.isEmpty():\n        return True\n    else:\n        return False\n\nprint(parChecker('((()))'))\nprint(parChecker('(()'))\n```\n\n```\nTrue\nFalse\n```\n\n## 符号匹配\n\n```python\ndef parChecker(symbolString):\n    s = Stack()\n    balanced = True\n    index = 0\n    while index < len(symbolString) and balanced:\n        symbol = symbolString[index]\n        if symbol in \"([{\":\n            s.push(symbol)\n        else:\n            if s.isEmpty():\n                balanced = False\n            else:\n                top = s.pop()\n                if not matches(top,symbol):\n                       balanced = False\n        index = index + 1\n    if balanced and s.isEmpty():\n        return True\n    else:\n        return False\n\ndef matches(open,close):\n    opens = \"([{\"\n    closers = \")]}\"\n    return opens.index(open) == closers.index(close)\n\n\nprint(parChecker('{{([][])}()}'))\nprint(parChecker('[{()]'))\n```\n\n```\nTrue\nFalse\n```\n\n## 十进制转二进制\n\n```python\ndef divideBy2(decNumber):\n    remstack = Stack()\n\n    while decNumber > 0:\n        rem = decNumber % 2\n        remstack.push(rem)\n        decNumber = decNumber // 2\n\n    binString = \"\"\n    while not remstack.isEmpty():\n        binString = binString + str(remstack.pop())\n\n    return binString\n\nprint(divideBy2(42))\n```\n\n```\n101010\n```\n\n## 十进制转化为多进制\n\n```python\ndef baseConverter(decNumber,base):\n    digits = \"0123456789ABCDEF\"\n\n    remstack = Stack()\n\n    while decNumber > 0:\n        rem = decNumber % base\n        remstack.push(rem)\n        decNumber = decNumber // base\n\n    newString = \"\"\n    while not remstack.isEmpty():\n        newString = newString + digits[remstack.pop()]\n\n    return newString\n\nprint(baseConverter(25,2))\nprint(baseConverter(25,16))\n```\n\n```\n11001\n19\n```\n\n## 中缀转后缀通用法\n\n```python\ndef infixToPostfix(infixexpr):\n    prec = {}\n    prec[\"*\"] = 3\n    prec[\"/\"] = 3\n    prec[\"+\"] = 2\n    prec[\"-\"] = 2\n    prec[\"(\"] = 1\n    opStack = Stack()\n    postfixList = []\n    tokenList = infixexpr.split()\n\n    for token in tokenList:\n        if token in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" or token in \"0123456789\":\n            postfixList.append(token)\n        elif token == '(':\n            opStack.push(token)\n        elif token == ')':\n            topToken = opStack.pop()\n            while topToken != '(':\n                postfixList.append(topToken)\n                topToken = opStack.pop()\n        else:\n            while (not opStack.isEmpty()) and \\\n               (prec[opStack.peek()] >= prec[token]):\n                  postfixList.append(opStack.pop())\n            opStack.push(token)\n\n    while not opStack.isEmpty():\n        postfixList.append(opStack.pop())\n    return \" \".join(postfixList)\n\nprint(infixToPostfix(\"A * B + C * D\"))\nprint(infixToPostfix(\"( A + B ) * C - ( D - E ) * ( F + G )\"))\n```\n\n```\nA B * C D * +\nA B + C * D E - F G + * -\n```\n\n## 后缀表达式求值\n\n```python\ndef postfixEval(postfixExpr):\n    operandStack = Stack()\n    tokenList = postfixExpr.split()\n\n    for token in tokenList:\n        if token in \"0123456789\":\n            operandStack.push(int(token))\n        else:\n            operand2 = operandStack.pop()\n            operand1 = operandStack.pop()\n            result = doMath(token,operand1,operand2)\n            operandStack.push(result)\n    return operandStack.pop()\n\ndef doMath(op, op1, op2):\n    if op == \"*\":\n        return op1 * op2\n    elif op == \"/\":\n        return op1 / op2\n    elif op == \"+\":\n        return op1 + op2\n    else:\n        return op1 - op2\n\nprint(postfixEval('7 8 + 3 2 + /'))\n```\n\n```\n3.0\n```\n\n\n","tags":["算法"]},{"title":"计算函数运行效率","url":"/2018/10/21/计算函数运行效率/","content":"计算函数时间\n\n```python\n\n```\n\n要捕获我们的每个函数执行所需的时间，我们将使用 Python 的 timeit 模块。timeit 模块旨在允许 Python 开发人员通过在一致的环境中运行函数并使用尽可能相似的操作系统的时序机制来进行跨平台时序测量。\n要使用 timeit，你需要创建一个 Timer 对象，其参数是两个 Python 语句。第一个参数是一个你想要执行时间的 Python 语句; 第二个参数是一个将运行一次以设置测试的语句。然后 timeit 模块将计算执行语句所需的时间。默认情况下，timeit 将尝试运行语句一百万次。 当它完成时，它返回时间作为表示总秒数的浮点值。由于它执行语句一百万次，可以读取结果作为执行测试一次的微秒数。你还可以传递 timeit 一个参数名字为 number，允许你指定执行测试语句的次数。以下显示了运行我们的每个测试功能 1000 次需要多长时间。\n\n```python\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndef test1():\n    l = []\n    for i in range(1000):\n        l = l + [i]\n\ndef test2():\n    l = []\n    for i in range(1000):\n        l.append(i)\n\ndef test3():\n    l = [i for i in range(1000)]\n\ndef test4():\n    l = list(range(1000))\n```\n\n```python\nimport timeit\nfrom timeit import Timer\nt1 = Timer(\"test1()\", \"from __main__ import test1\")\nprint(\"concat \",t1.timeit(number=1000), \"milliseconds\")\nt2 = Timer(\"test2()\", \"from __main__ import test2\")\nprint(\"append \",t2.timeit(number=1000), \"milliseconds\")\nt3 = Timer(\"test3()\", \"from __main__ import test3\")\nprint(\"comprehension \",t3.timeit(number=1000), \"milliseconds\")\nt4 = Timer(\"test4()\", \"from __main__ import test4\")\nprint(\"list range \",t4.timeit(number=1000), \"milliseconds\")\n\n\n```\n\n```\nconcat  1.0196415490549953 milliseconds\nappend  0.08268737967887319 milliseconds\ncomprehension  0.03390247851893946 milliseconds\nlist range  0.013240015113296977 milliseconds\n```\n\n作为一种演示性能差异的方法，我们用 timeit 来做一个实验。我们的目标是验证从列表从末尾 pop 元素和从开始 pop 元b素的性能。同样，我们也想测量不同列表大小对这个时间的影响。我们期望看到的是，从列表末尾处弹出所需时间将保持不变，即使列表不断增长。而从列表开始处弹出元素时间将随列表增长而增加。\n\n\n\n```python\npopzero = Timer(\"x.pop(0)\",\n                \"from __main__ import x\")\npopend = Timer(\"x.pop()\",\n               \"from __main__ import x\")\ndata=[]\nprint(\"pop(0)   pop()\")\nfor i in range(1000000,100000001,1000000):\n    x = list(range(i))\n    pt = popend.timeit(number=1000)\n    x = list(range(i))\n    pz = popzero.timeit(number=1000)\n    print(\"%15.5f, %15.5f\" %(pz,pt))\n    one_data={}\n    one_data['time']=i\n    one_data['pop(0)']=pz\n    one_data['pop()']=pt\n    data.append(one_data)\n    \n```\n\n```\npop(0)   pop()\n        0.56428,         0.00026\n        1.46132,         0.00009\n        2.27953,         0.00007\n        3.11111,         0.00025\n        4.03941,         0.00007\n        5.01496,         0.00011\n        5.72766,         0.00008\n        6.41760,         0.00007\n        6.98101,         0.00016\n        7.69488,         0.00009\n        8.59127,         0.00008\n        9.39142,         0.00008\n       10.03456,         0.00007\n       10.85724,         0.00011\n       11.69828,         0.00008\n       12.37023,         0.00008\n       13.12709,         0.00010\n       13.89727,         0.00008\n       14.65594,         0.00007\n       15.49640,         0.00008\n       16.35027,         0.00008\n       17.00838,         0.00010\n       17.72517,         0.00008\n       19.29465,         0.00010\n       21.02476,         0.00008\n       21.60967,         0.00008\n       22.32767,         0.00008\n       22.62550,         0.00008\n       25.15527,         0.00009\n       24.14118,         0.00008\n       25.48768,         0.00009\n       24.84720,         0.00007\n       26.40644,         0.00008\n       26.62662,         0.00008\n       28.86800,         0.00008\n       27.79692,         0.00008\n\n```\n\n将比较列表和字典之间的 contains 操作的性能。在此过程中，我们将确认列表的 contains 操作符是 O(n)，字典的 contains 操作符是 O(1)。我们将在实验中列出一系列数字。然后随机选择数字，并检查数字是否在列表中。如果我们的性能表是正确的，列表越大，确定列表中是否包含任意一个数字应该花费的时间越长。\n\n```python\nimport timeit\nimport random\ndata=[]\nfor i in range(10000,1000001,20000):\n    t = timeit.Timer(\"random.randrange(%d) in x\"%i,\n                     \"from __main__ import random,x\")\n    x = list(range(i))\n    lst_time = t.timeit(number=1000)\n    x = {j:None for j in range(i)}\n    d_time = t.timeit(number=1000)\n    print(\"%d,%10.3f,%10.3f\" % (i, lst_time, d_time))\n    one_data={}\n    one_data['time']=i\n    one_data['lis_contains']=lst_time\n    one_data['dict_contains']=d_time\n    data.append(one_data)\ndata\n```\n\n```\n10000,     0.075,     0.001\n30000,     0.149,     0.001\n50000,     0.235,     0.001\n70000,     0.353,     0.001\n90000,     0.449,     0.001\n110000,     0.526,     0.001\n130000,     0.614,     0.001\n\n\n```\n\n\n\n\n\n```\n\n```\n\n\n\n```python\nimport pandas as pd \nrow_data=pd.DataFrame.from_dict(data=data)\nrow_data.head()\n```\n\n\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dict_contains</th>\n      <th>lis_contains</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000865</td>\n      <td>0.074577</td>\n      <td>10000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000848</td>\n      <td>0.149025</td>\n      <td>30000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000817</td>\n      <td>0.235177</td>\n      <td>50000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.001056</td>\n      <td>0.352916</td>\n      <td>70000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000893</td>\n      <td>0.448741</td>\n      <td>90000</td>\n    </tr>\n  </tbody>\n</table>\n\n\n\n```python\npd.pivot_table(data=row_data,index='time').plot()\n```\n\n\n\n```\n<matplotlib.axes._subplots.AxesSubplot at 0x1a28403b4a8>\n```\n\n\n\n![png](/image/output_10_1.png)\n\n\n","tags":["算法"]},{"title":"skill_in_python","url":"/2018/10/20/skill-in-python/","content":"# python编程技巧\n\n\n\n## 可变类型和不可变类型\n\nPython提供两种内置或用户定义的类型。可变类型允许内容的内部修改。典型的动态类型 包括列表与字典：列表都有可变方法，如 `list.append()` 和 `list.pop()`， 并且能就地修改。字典也是一样。不可变类型没有修改自身内容的方法。比如，赋值为整数 6的变量 x 并没有 \"自增\" 方法，如果需要计算 x + 1，必须创建另一个整数变量并给其命名。\n\n这种差异导致的一个后果就是，可变类型是不 '稳定 '的，因而不能作为字典的键使用。合理地 使用可变类型与不可变类型有助于阐明代码的意图。例如与列表相似的不可变类型是元组， 创建方式为 `(1, 2)`。元组是不可修改的，并能作为字典的键使用。\n\nPython 中一个可能会让初学者惊讶的特性是：字符串是不可变类型。这意味着当需要组合一个 字符串时，将每一部分放到一个可变列表里，使用字符串时再组合 ('join') 起来的做法更高效。 值得注意的是，使用列表推导的构造方式比在循环中调用 `append()` 来构造列表更好也更快。\n\n差\n\n```\n# 创建将0到19连接起来的字符串 (例 \"012..1819\")\nnums = \"\"\nfor n in range(20):\n    nums += str(n)   # 慢且低效\nprint nums\n```\n\n好\n\n```\n# 创建将0到19连接起来的字符串 (例 \"012..1819\")\nnums = []\nfor n in range(20):\n    nums.append(str(n))\nprint \"\".join(nums)  # 更高效\n```\n\n## 避免对不同类型的对象使用同一个变量名\n\n差\n\n```\na = 1\na = 'a string'\ndef a():\n    pass  # 实现代码\n```\n\n好\n\n```\ncount = 1\nmsg = 'a string'\ndef func():\n    pass  # 实现代码\n```\n\n更好\n\n```\n# 创建将0到19连接起来的字符串 (例 \"012..1819\")\nnums = [str(n) for n in range(20)]\nprint \"\".join(nums)\n```\n\n极佳\n\n```\n# 创建将0到19连接起来的字符串 (例 \"012..1819\")\nnums = map(str, range(20))\nprint \"\".join(nums)\n```\n\n\n\n最后关于字符串的说明的一点是，使用 `join()` 并不总是最好的选择。比如当用预先 确定数量的字符串创建一个新的字符串时，使用加法操作符确实更快，但在上文提到的情况 下或添加到已存在字符串的情况下，使用 `join()` 是更好\n\n```\nfoo = 'foo'\nbar = 'bar'\n\nfoobar = foo + bar  # 好的做法\nfoo += 'ooo'  # 不好的做法, 应该这么做:\nfoo = ''.join([foo, 'ooo'])\n```\n\n除了 [`str.join()`](https://docs.python.org/3/library/stdtypes.html#str.join) 和 `+`，您也可以使用 [%](https://docs.python.org/3/library/string.html#string-formatting) 格式运算符来连接确定数量的字符串，但 [**PEP 3101**](https://www.python.org/dev/peps/pep-3101) 建议使用 [`str.format()`](https://docs.python.org/3/library/stdtypes.html#str.format) 替代 `%` 操作符。\n\n```\nfoo = 'foo'\nbar = 'bar'\n\nfoobar = '%s%s' % (foo, bar) # 可行\nfoobar = '{0}{1}'.format(foo, bar) # 更好\nfoobar = '{foo}{bar}'.format(foo=foo, bar=bar) # 最好\n```\n\n使用简短的函数或方法能降低对不相关对象使用同一个名称的风险。即使是相关的不同 类型的对象，也更建议\n\n使用不同命名：\n\n```\nitems = 'a b c d'  # 首先指向字符串...\nitems = items.split(' ')  # ...变为列表\nitems = set(items)  # ...再变为集合\n```\n\n重复使用命名对效率并没有提升：赋值时无论如何都要创建新的对象。然而随着复杂度的 提升，赋值语句被其他代码包括 'if' 分支和循环分开，使得更难查明指定变量的类型。 在某些代码的做法中，例如函数编程，推荐的是从不重复对同一个变量命名赋值。Java 内的实现方式是使用 'final' 关键字。Python并没有 'final' 关键字而且这与它的哲学 相悖。尽管如此，避免给同一个变量命名重复赋值仍是是个好的做法，并且有助于掌握 可变与不可变类型的概念。\n\n## 单行描述单行代码\n\n每一行一个语句，尤其在复杂的逻辑表达式的时候，这样会清晰很容易阅读。\n\n差\n\n```\nprint \"one\";print \"two\"\nif x == 1;print \"one\"\n```\n\n好\n\n```\nprint \"one\"\nprint \"two\"\nif x == 1:\n\tprint \"one\"\n\t\n```\n\n\n\n## 技巧\n\n使用enumerate() 将列表中的每个项提供两个元素的元组，一个是下标，一个是值。\n\n\n\n```\nfor index , iteme in enumerate(sorted_list):\n\tprint(index,iteme)\n```\n\n交换变量\n\n```\na,b=b,a\n```\n\n## **访问字典元素**\n\n不要使用该dict.has_key()方法。相反使用语法或传递默认参数 比如x in dict ，dict.get(k,default_value)\n\n差\n\n```\nd = {“hello”: \"world\"}\nif d.has_key(\"hello\"):\n\tprint(d['hello'])\nelse:\n\tprint(\"fault values\")\n```\n\n好\n\n\n\n```\nd = {“hello”: \"world\"}\nprint(d.get(\"hello\"),\"fault vluse\")\n\n# or\nif \"hello\" in d:\n\tprint(d[“hello”])\n```\n\n\n","tags":["python"]},{"title":"shadowsocks_config","url":"/2018/10/20/shadowsocks-config/","content":"# 搭建shadowsocks server 和 client端\n\n## sever 端\n\n```\nsudo apt update\nsudo apt install shadowsocks\n```\n\n下载完成后，配置shadowsocks 文件，\n\n```\nvim /etc/shadowsocks/config.json\n```\n\n```\n{\n    \"server\":\"ethan2lee.online\", #you server ip\n    \"server_port\":8888,\n    \"local_address\": \"127.0.0.1\",\n    \"local_port\":1080,\n    \"password\":\"your_password\",\n    \"timeout\":300,\n    \"method\":\"aes-256-cfb\"\n}\n```\n\n一般来说，我们只需要更改“server”和“password”字段的值即可，根据自己实际情况配置。更改完后，保存。\n\n接下来就是启动server端了，先设置开机自启，再启动。\n\n```\n➜  ~ sudo systemctl enable  shadowsocks.service\n\nshadowsocks.service is not a native service, redirecting to systemd-sysv-install\nExecuting /lib/systemd/systemd-sysv-install enable shadowsocks\n```\n\n\n\n```\n➜  ~ sudo systemctl start   shadowsocks.service\n```\n\n\n\n查看服务状态：\n\n```\n➜  ~ sudo systemctl status  shadowsocks.service\n● shadowsocks.service - LSB: Fast tunnel proxy that helps you bypass firewalls\n   Loaded: loaded (/etc/init.d/shadowsocks; bad; vendor preset: enabled)\n   Active: active (exited) since Sat 2018-10-20 04:59:16 UTC; 4s ago\n​     Docs: man:systemd-sysv-generator(8)\n  Process: 634 ExecStart=/etc/init.d/shadowsocks start (code=exited, status=0/SUCCESS)\n​    Tasks: 0\n   Memory: 0B\n​      CPU: 0\n\nOct 20 04:59:16 ethan systemd[1]: Starting LSB: Fast tunnel proxy that helps you bypass firewalls...\nOct 20 04:59:16 ethan systemd[1]: Started LSB: Fast tunnel proxy that helps you bypass firewalls.\n```\n\n这样我们shadowsocks的server端就配置完成了。\n\n## client端\n\n在需要用shadowsocks的server服务的机子也安装shaodowsocks。\n\n```\nsudo apt update\nsudo apt install shadowsocks\n```\n\n\n\n\n\n同样需要配置一下shadowsocks的配置文件，一般跟server端的配置差不多，但是需要自行更改“local_port”的字段值。\n\n```\nvim /etc/shadowsocks/config.json\n```\n\n\n\n```\n{\n    \"server\":\"ethan2lee.online\", #you server ip\n    \"server_port\":8888,\n    \"local_address\": \"127.0.0.1\",\n    \"local_port\":1080, ### 在clien端可以自主配置的\n    \"password\":\"your_password\",\n    \"timeout\":300,\n    \"method\":\"aes-256-cfb\"\n}\n```\n\n配置完成之手进行保存。\n\n开启client 端，这样我们只要使用sock5协议通过代理端口1080访问网络，就可以完全使用server的网络环境了。\n\n```\n➜  ~ sslocal -c /etc/shadowsocks/config.json\nINFO: loading config from /etc/shadowsocks/config.json\nshadowsocks 2.1.0\n2018-10-20 05:10:08 INFO     starting local at 127.0.0.1:1080\n```\n\n打开之后，shadowsocks的client 端是运行在前台的。我们先按Ctr +c 终止进行，再输入以下命令，让它在后台运行：\n\n```\n➜  ~ nohup sslocal -c /etc/shadowsocks/config.json &\n[1] 9479\nnohup: ignoring input and appending output to 'nohup.out'\n```\n\n这样它就作为守护进程在运行着了。\n\n## proxychains\n\n安装完shadowsocks之后，如果想让终端命令也经常代理，那么我们就需要proxychains来帮忙了。\n\n安装proxychains\n\n```\nsudo apt install proxychains\n```\n\n修改proxychains 配置文件，直接找到最后一行。\n\n```\n➜  ~ vim /etc/proxychains.conf\n```\n\n将`socks4 127.0.0.1 9095`改为\n\n```\nsocks5 127.0.0.1 1080 \n```\n\n1080 为你刚刚配置shadowsocks client端的“local_host”字段的值，配置完成后保存。\n\n\n\n\n\n经过以上所有步骤，我们可以通过一下命令来查看我们是否配置正确了：\n\n\n\n```\nproxychains wget www.google.com\n```\n\n如果有一下输出，那么就证明你配置成功了：\n\n```\n➜  ~ proxychains wget www.google.com\nProxyChains-3.1 (http://proxychains.sf.net)\n--2018-10-20 05:28:01--  http://www.google.com/\nResolving www.google.com (www.google.com)... |DNS-request| www.google.com\n|S-chain|-<>-127.0.0.1:1080-<><>-4.2.2.2:53-<><>-OK\n|DNS-response| www.google.com is 74.125.20.103\n74.125.20.103\nConnecting to www.google.com (www.google.com)|74.125.20.103|:80... |S-chain|-<>-127.0.0.1:1080-<><>-74.125.20.103:80-<><>-OK\nconnected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: ‘index.html’\n\nindex.html                        [ <=>                                           ]  11.05K  --.-KB/s    in 0s\n\n2018-10-20 05:28:02 (63.6 MB/s) - ‘index.html’ saved [11319]\n\n```\n\n\n\n如果没有，就只能继续百度了。\n","tags":["vpn"]},{"title":"更换ubuntu18源","url":"/2018/10/20/更换ubuntu18源/","content":"# Ubuntu 18.04换国内源 \n\n\n\n更换源前，先对本来的源文件做好备份。\n\n```\nmv /etc/apt/sources.list  /etc/apt/sourceslist-save\n```\n\n备份完成后，我们就可再建立source.list\n\n```\nvim /etc/apt/sources.list \n```\n\n添加以下内容\n\n```\n##中科大源\n\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multivers\n```\n\n然后执行以下命令：\n\n```\nsudo apt update\nsudo apt upgrade\n```\n\n\n\n\n\n更改其他源：\n\n阿里源\n\n```\ndeb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\n```\n\n360源\n\n```\ndeb http://mirrors.163.com/ubuntu/ bionic main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiverse\n```\n\n清华源\n\n```\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\n```\n\n\n","tags":["linux"]},{"title":"统计量分析示例","url":"/2018/10/19/统计量分析示例/","content":"\n# \n\n# 描述性统计分析基础\n\n- 数据集描述与属性说明\n\n- ID 客户编号\n\n- Suc_flag   成功入网标识\n\n- ARPU   入网后ARPU\n\n- PromCnt12  12个月内的营销次数\n\n- PromCnt36  36个月内的营销次数\n\n- PromCntMsg12   12个月内发短信的次数\n\n- PromCntMsg36   36个月内发短信的次数\n\n- Class  客户重要性等级(根据前运营商消费情况)\n\n- Age    年龄\n\n- Gender 性别\n\n- HomeOwner  是否拥有住房\n\n- AvgARPU    当地平均ARPU\n\n- AvgHomeValue   当地房屋均价\n\n- AvgIncome  当地人均收入\n\n```\nimport os\nimport pandas as pd\nimport numpy as np\n\npd.set_option('display.max_columns', None)\n#os.chdir('Q:/data')\n#os.getcwd()\n```\n\n读取数据\n\n```\ncamp= pd.read_csv('teleco_camp.csv')\ncamp.head(10)\n```\n\n数据预处理\n\n```\ncamp.dtypes\n```\n\n```\nID                 int64\nSuc_flag        category\nARPU             float64\nPromCnt12        float64\nPromCnt36        float64\nPromCntMsg12     float64\nPromCntMsg36     float64\nClass           category\nAge              float64\nGender            object\nHomeOwner         object\nAvgARPU          float64\nAvgHomeValue     float64\nAvgIncome        float64\ndtype: object\n```\n\ncamp.describe(include='all')\n\n|        | ID            | Suc_flag | ARPU        | PromCnt12   | PromCnt36   | PromCntMsg12 | PromCntMsg36 | Class  | Age         | Gender | HomeOwner | AvgARPU     | AvgHomeValue  | AvgIncome     |\n| ------ | ------------- | -------- | ----------- | ----------- | ----------- | ------------ | ------------ | ------ | ----------- | ------ | --------- | ----------- | ------------- | ------------- |\n| count  | 9686.000000   | 9686.0   | 4843.000000 | 9686.000000 | 9686.000000 | 9686.000000  | 9686.000000  | 9686.0 | 7279.000000 | 9686   | 9686      | 9686.000000 | 9583.000000   | 7329.000000   |\n| unique | NaN           | 2.0      | NaN         | NaN         | NaN         | NaN          | NaN          | 4.0    | NaN         | 3      | 2         | NaN         | NaN           | NaN           |\n| top    | NaN           | 1.0      | NaN         | NaN         | NaN         | NaN          | NaN          | 2.0    | NaN         | F      | H         | NaN         | NaN           | NaN           |\n| freq   | NaN           | 4843.0   | NaN         | NaN         | NaN         | NaN          | NaN          | 3303.0 | NaN         | 5223   | 5377      | NaN         | NaN           | NaN           |\n| mean   | 97975.474086  | NaN      | 78.121722   | 3.447212    | 7.337059    | 1.178402     | 2.390935     | NaN    | 59.150845   | NaN    | NaN       | 52.905156   | 112179.202755 | 53513.457361  |\n| std    | 56550.171120  | NaN      | 62.225686   | 1.231890    | 1.952436    | 0.287226     | 0.914314     | NaN    | 16.516400   | NaN    | NaN       | 4.993775    | 98522.888583  | 19805.168339  |\n| min    | 12.000000     | NaN      | 5.000000    | 0.750000    | 1.000000    | 0.200000     | 0.400000     | NaN    | 0.000000    | NaN    | NaN       | 46.138968   | 7500.000000   | 2499.000000   |\n| 25%    | 48835.500000  | NaN      | 50.000000   | 2.900000    | 6.250000    | 1.000000     | 1.400000     | NaN    | 47.000000   | NaN    | NaN       | 49.760116   | 53200.000000  | 40389.000000  |\n| 50%    | 99106.000000  | NaN      | 65.000000   | 3.250000    | 7.750000    | 1.200000     | 2.600000     | NaN    | 60.000000   | NaN    | NaN       | 50.876672   | 77700.000000  | 48699.000000  |\n| 75%    | 148538.750000 | NaN      | 100.000000  | 3.650000    | 8.250000    | 1.400000     | 3.200000     | NaN    | 73.000000   | NaN    | NaN       | 54.452822   | 129350.000000 | 62385.000000  |\n| max    | 191779.000000 | NaN      | 1000.000000 | 15.150000   | 19.500000   | 3.600000     | 5.600000     | NaN    | 87.000000   | NaN    | NaN       | 99.444787   | 600000.000000 | 200001.000000 |\n\n\n\n\n\n\n# 描述性统计与探索型数据分析\n\n## 分类变量分析\n\n可以查看列原因元素的种类\n\n```\ncamp['Suc_flag'].groupby(camp['Suc_flag']).count()\n```\n\n```\nSuc_flag\n0    4843\n1    4843\nName: Suc_flag, dtype: int64\n```\n\n\n\n\n\n## 连续变量分析\n### 数据的集中趋势\n#### ARPU的均值与中位数\n```\n\nfs = camp['ARPU'] # 可以使用camp.ARPU \nprint('mean = %6.4f' %fs.mean())                     # 求fs的均值\nprint('median = %6.4f' %fs.median() )                # 求fs的中位数\nprint('quantiles\\n', fs.quantile([0.25, 0.5, 0.75])) # 求a的上下四分位数与中位数\n```\n\n```\nmad = 38.1896\nrange = 995.0000\nvar = 3872.0359\nstd = 62.2257\n```\n\n\n\n```\nget_ipython().run_line_magic('matplotlib', 'inline'）\n\nfs.plot(kind='hist')\n```\n\n```\n(array([2.510e+03, 1.978e+03, 2.160e+02, 9.800e+01, 4.000e+00, 7.000e+00,\n        0.000e+00, 2.500e+01, 0.000e+00, 0.000e+00, 0.000e+00, 1.000e+00,\n        0.000e+00, 0.000e+00, 4.000e+00]),\n array([   5.        ,   71.33333333,  137.66666667,  204.        ,\n         270.33333333,  336.66666667,  403.        ,  469.33333333,\n         535.66666667,  602.        ,  668.33333333,  734.66666667,\n         801.        ,  867.33333333,  933.66666667, 1000.        ]),\n <a list of 15 Patch objects>)\n```\n\n![](image/output_11_1.png)\n\n\n### 数据的离散程度\n\n```\nprint ('mad = %6.4f' %fs.mad())      # 求平均绝对偏差 mad = np.abs(fs - fs.mean()).mean()\nprint ('range = %6.4f' %(fs.max(skipna=True) - fs.min(skipna=True))) # 求极差\nprint ('var = %6.4f' %fs.var())   # 求方差\nprint ('std = %6.4f' %fs.std())   # 求标准差\n```\n\n\n\n\n\n### 数据的偏度与峰度\n```\nimport matplotlib.pyplot as plt\n\nplt.hist(fs.dropna(), bins=15)\n```\n\n![](image/output_15_1.png)\n\n```\nprint ('skewness = %6.4f' %fs.skew(skipna=True))\nprint ('kurtosis = %6.4f' %fs.kurt(skipna=True))\n```\n\n```\nskewness = 5.1695\nkurtosis = 52.8509\n```\n\n\n### apply\\map\\groupby及其它相关\n\n```\ndata = pd.DataFrame(data={'a':range(1,11), 'b':np.random.randn(10)})\ndata.T\n```\n\n|      | 0         | 1        | 2        | 3        | 4        | 5         | 6        | 7       | 8        | 9         |\n| ---- | --------- | -------- | -------- | -------- | -------- | --------- | -------- | ------- | -------- | --------- |\n| a    | 1.000000  | 2.000000 | 3.000000 | 4.000000 | 5.000000 | 6.000000  | 7.000000 | 8.0000  | 9.000000 | 10.000000 |\n| b    | -0.087919 | 0.903531 | 0.603965 | 0.203005 | 0.282077 | -1.420298 | 0.283303 | -0.0565 | 1.047595 | -0.787566 |\n\n```\ndata.apply(np.mean) # 等价于data.mean()，是其完整形式\n```\n\n```\na    5.500000\nb    0.097119\ndtype: float64\n```\n\n```\ndata.apply(lambda x: x.astype('str')).dtypes # DataFrame没有astype方法，只有Series有\n```\n","tags":["数据分析"]},{"title":"data_clearning","url":"/2018/10/17/data-clearning/","content":"# 数据清洗\n\n> 数据清洗， 是整个数据分析过程中不可缺少的一个环节，其结果质量直接关系到模型效果和最终结论。在实际操作中，数据清洗通常会占据分析过程的50%—80%的时间。\n\n## 去除重复数据\n\n在获取到的数据中，我们发现会有重复数据。我们使用Pandas 提供的方法 duplicated 和 drop_duplicates来去重。\n\n\n\n```\nsample =  pd.DataFrame({'id':[1,1,1,3,4,5],\n                        'name':['Bob','Bob','Mark','Miki','Sully','Rose'],\n                        'score':[99,99,87,77,77,np.nan],\n                        'group':[1,1,1,2,1,2]})\nsample\nOut[85]: \n   id   name  score  group\n0   1    Bob   99.0      1\n1   1    Bob   99.0      1\n2   1   Mark   87.0      1\n3   3   Miki   77.0      2\n4   4  Sully   77.0      1\n5   5   Rose    NaN      2\n\n```\n\n查找重复数据：\n\n```\nsample[sample.duplicated()]\nOut[86]: \n   id name  score  group\n1   1  Bob   99.0      1\n```\n\n需要去重时：\n\n```\nsample.drop_duplicates()\nOut[87]: \n   id   name  score  group\n0   1    Bob   99.0      1\n2   1   Mark   87.0      1\n3   3   Miki   77.0      2\n4   4  Sully   77.0      1\n5   5   Rose    NaN      2\n```\n\n按列去重时，需要加入列索引：\n\n```\nsample.drop_duplicates('id')\nOut[88]: \n   id   name  score  group\n0   1    Bob   99.0      1\n3   3   Miki   77.0      2\n4   4  Sully   77.0      1\n5   5   Rose    NaN      2\n```\n\n## 缺失值处理\n\n在数据挖掘中，面对数据中存在缺失值时，我们一般采取以下几种办法：\n\n1. 缺失值较多的特征处理\n\n当缺失值处于20%~80%，每个缺失值可以生成一个指示哑变量，参与后续的建模。\n\n2.缺失较少时\n\n首先需要根据业务理解处理缺失值，弄清楚缺失值产生的原因，是故意缺失还是随机缺失。可以依靠业务经验进行填补。连续变量可以使用均值或中位数进行填补。\n\n- 把 NaN 直接作为一个特征，假设0表示\n\n```\ndf.fillna(0) \n```\n\n\n\n- 用均值填充\n\n```\n# 将所有行用各自的均值填充 \ndf.fillna(df.mean())  \n# 将所有行用各自的均值填充 \ndf.fillna(df.mean()['collum_name])\n```\n\n\n\n- 用上下数据填充\n\n```\n# 用前一个数据替代NaN\ndf.fillnan(method=\"pad\")\n\n# 与pad相反，bfill表示用后一个数据代替NaN\ndf.fillna(method=)\n```\n\n\n\n- 用插值填充\n\n```\n# 插值法就是通过两点（x0,y0）,(x1,y1)估计中间点的值\ndf.interpolate() \n```\n\n\n\n\n\n- 查看数据值缺失情况，我们可有构造一个lambda 函数来查看缺失值。\n\n```\nsample =  pd.DataFrame({'id':[1,1,1,3,4,np.nan],\n                        'name':['Bob','Bob','Mark','Miki','Sully',np.nan],\n                        'score':[99,99,87,77,77,np.nan],\n                        'group':[1,1,1,2,1,np.nan]})\nsample\nsample.apply(lambda col:sum(col.isnull())/col.size)\nOut[91]: \nid       0.166667\nname     0.166667\nscore    0.166667\ngroup    0.166667\ndtype: float64\n```\n\n### 已指定值填补\n\n均值\n\n```\nsample.score.fillna(sample.score.mean())\nOut[93]: \n0    99.0\n1    99.0\n2    87.0\n3    77.0\n4    77.0\n5    87.8\nName: score, dtype: float64\n```\n\n中位数\n\n```\nsample.score.fillna(sample.score.median())\nOut[94]: \n0    99.0\n1    99.0\n2    87.0\n3    77.0\n4    77.0\n5    87.0\nName: score, dtype: float64\n```\n\n### 缺失值指示变量\n\nPanda DataFrame 对象可以直接调用方法isnull 产生缺失值指示变量，例如产生score变量的缺失值变量：\n\n```\n# 指示变量\nsample.score.isnull()\nOut[95]: \n0    False\n1    False\n2    False\n3    False\n4    False\n5     True\nName: score, dtype: bool\n```\n\n若想转化为数据0，1型指示变量，可以使用apply方法，int表示将该列替换为 int 类型：\n\n```\nsample.score.isnull().apply(int)\nOut[97]: \n0    0\n1    0\n2    0\n3    0\n4    0\n5    1\n```\n\n\n\n\n\n## 噪声处理\n\n噪声值是指数据中有一个或多个数据与其他数值相比差异性比较大，又称异常值，离群值。\n\n对于单变量，我们可以采用盖帽法，分箱法；\n\n对于多变量，我们可以采用就聚类。\n\n### 盖帽法\n\n盖帽法将连续变量均值上下三倍标准差范围外的记录替换为均值上下三倍标准差值。\n\n```\n\ndef cap(df,quantile=[0.01,0.99]):\n    \"\"\"\n    盖帽法处理异常值\n    :param df: pd.Series 列，连续变量\n    :param quantile: 指定盖帽法的上下分位数范围\n    :return: \n    \"\"\"\n    Q01 ,Q99 = df.quantile(quantile).values.tolist() # 生成分位数\n    # 替代异常值\n    if Q01 > df.min():\n        x = df.copy()\n        x.loc[x<Q01] = Q01\n    if Q99 < df.max():\n        x = df.copy()\n        x.loc[x > Q99] = Q99\n    return(x)\n```\n\n生成一组服从正态分布的随机数，sample.hsit 为直方图。\n\n```\nimport matplotlib.pyplot as plt\nsample = pd.DataFrame({'normall':np.random.randn(1000)})\nsample.hist(bins =50)\nplt.show()\n```\n\n![处理前](\\image\\before_handle.png)\n\n\n\n对 sample 数据所有列进行盖帽法转换，，下图可以看出盖帽后极端值频数的变化。\n\n```\nnew =sample.apply(cap,quantile=[0.001,0.99])\nnew.hist(bins=50)\nplt.show()\n```\n\n![](\\image\\after_handle.png)\n\n\n\n\n\n### 分箱法\n\n分箱法通过考察数据的\"近邻\"来光滑有序数据的值。有序值分布到一些桶或箱中。\n\n深分箱，即每个分箱中的样本量一致；\n\n等宽分箱，即每个分箱中的取值范围一致。直方图就是首先对数据进行了等宽分箱，再计算频数画图。\n\n分箱法可以将异常数据包含再箱子中，在进行建模的时候，不直接进行到模型中，因而可以达到处理异常值的目的。\n\n```\n# 生成10个标准正态分布的随机数\nsample = pd.DataFrame({\"normal\":np.random.randn(10)})\nsample\nOut[118]: \n     normal\n0 -0.028929\n1  0.327508\n2 -0.596384\n3 -2.036334\n4  1.452605\n5 -0.403936\n6  0.315138\n7  0.252127\n8 -0.775113\n9  0.171641\n```\n\n#### 等宽分箱\n\n现将sample 按照宽度分位5份，下限中，cut 函数自动选择小于列最小值的一个数值未下限，最大值为上限，等分为5份。\n\n```\npd.cut(sample.normal,5)\nOut[119]: \n0     (-0.641, 0.057]\n1      (0.057, 0.755]\n2     (-0.641, 0.057]\n3     (-2.04, -1.339]\n4      (0.755, 1.453]\n5     (-0.641, 0.057]\n6      (0.057, 0.755]\n7      (0.057, 0.755]\n8    (-1.339, -0.641]\n9      (0.057, 0.755]\nName: normal, dtype: category\nCategories (5, interval[float64]): [(-2.04, -1.339] < (-1.339, -0.641] < (-0.641, 0.057] <\n                                    (0.057, 0.755] < (0.755, 1.453]]\n\n```\n\n使用labels参数指定分箱后的各个水平的标签，\n\n```\npd.cut(sample.normal,bins=5,labels=[1,2,3,4,5])\nOut[120]: \n0    3\n1    4\n2    3\n3    1\n4    5\n5    3\n6    4\n7    4\n8    2\n9    4\nName: normal, dtype: category\nCategories (5, int64): [1 < 2 < 3 < 4 < 5]\n```\n\n### 等深分箱\n\n等深分箱中，各个箱的宽度可能不一，但频数是几乎相等，所以可以采用数据的分位数来分箱。现对sample数据进行等深度分二箱，首先要找到2箱的分位数：\n\n```\nsample.normal.quantile([0,0.5,1])\nOut[121]: \n0.0   -2.036334\n0.5    0.071356\n1.0    1.452605\nName: normal, dtype: float64\n```\n\n在bins参数中设定分位数区间，为将下边界包含，include_lowest= True，完成等深分箱：\n\n```\npd.cut(sample.normal,bins=sample.normal.quantile([0,0.5,1]),include_lowest=True)\nOut[124]: \n0    (-2.037, 0.0714]\n1     (0.0714, 1.453]\n2    (-2.037, 0.0714]\n3    (-2.037, 0.0714]\n4     (0.0714, 1.453]\n5    (-2.037, 0.0714]\n6     (0.0714, 1.453]\n7     (0.0714, 1.453]\n8    (-2.037, 0.0714]\n9     (0.0714, 1.453]\nName: normal, dtype: category\nCategories (2, interval[float64]): [(-2.037, 0.0714] < (0.0714, 1.453]]\n```\n\n可以对分组进行标签化。\n\n```\npd.cut(sample.normal,bins=sample.normal.quantile([0,0.5,1]),include_lowest=True,labels=['bad','good'])\nOut[125]: \n0     bad\n1    good\n2     bad\n3     bad\n4    good\n5     bad\n6    good\n7    good\n8     bad\n9    good\nName: normal, dtype: category\nCategories (2, object): [bad < good]\n\n```\n\n\n\n\n\n### 多变量异常值处理-聚类法\n\n通过快速聚类法将数据对象分组成为多个簇，在同一个簇中的对象具有较高的相似度，而不同簇之间的对象差别较大。聚类分析可以挖掘孤立点以发现噪声数据，因为噪声本身就是孤立点。\n","tags":["数据清洗"],"categories":["数据挖掘"]},{"title":"RFM模型分析用户行为","url":"/2018/10/17/RFM模型分析用户行为/","content":"# RFM模型分析用户行为\n\n根据美国数据库营销研究所Arthur Hughes的研究，客户数据库中有三个神奇的要素，这三个要素构成了数据分析最好的指标：最近一次消费(Recency)、消费频率(Frequency)、消费金额(Monetary)。\n\nRFM模型：\n\n- R(Recency)表示客户最近一次购买的时间有多远，对消费时间越近的客户，提供即时的商品或服务也最有可能有所反应。\n\n- F(Frequency)表示客户在最近一段时间内购买的次数，经常买的客户也是满意度最高的客户。\n\n- M  (Monetary)表示客户在最近一段时间内购买的金额，消费金额是最近消费的平均金额，是体现客户短期价值的中重要变量。如果预算不多，那么我们酒的将服务信息提供给收入贡献较高的那些人。\n\n一般原始数据为3个字段：客户ID、购买时间（日期格式）、购买金额，用数据挖掘软件处理，加权（考虑权重）得到RFM得分，进而可以进行客户细分，客户等级分类，Customer Level Value得分排序等，实现数据库营销！\n\n![](\\image\\RFM.png)\n\n\n\n（编号次序RFM,1代表高，0代表低）\n\n重要价值客户（111）：最近消费时间近、消费频次和消费金额都很高，必须是VIP啊！\n\n重要保持客户（011）：最近消费时间较远，但消费频次和金额都很高，说明这是个一段时间没来的忠实客户，我们需要主动和他保持联系。\n\n重要发展客户（101）：最近消费时间较近、消费金额高，但频次不高，忠诚度不高，很有潜力的用户，必须重点发展。\n\n重要挽留客户（001）：最近消费时间较远、消费频次不高，但消费金额高的用户，可能是将要流失或者已经要流失的用户，应当基于挽留措施。\n\nRFM模型的应用在于建立一个用户行为报告，这个报告会成为维系顾客的一个重要指标。\n\n\n\n现在我们以某淘宝店家做客户激活为案例，[RFM_TRAD_FLOW.csv](/data/RFM_TRAD_FLOW.csv) 为某段时间内客户消费记录\n\n```\nimport pandas as pd\ntrad_flow = pd.read_csv('data/RFM_TRAD_FLOW.csv', encoding='gbk')\ntrad_flow.head(10)\n```\n\n数据部分展示：\n\n| transID | cumid | time | amount | type_label | type |\n| ------- | ----- | ---- | ------ | ---------- | ---- |\n|9407|\t10001|\t14JUN09:17:58:34|\t199\t|正常|\tNormal|\n|9625\t|10001\t|16JUN09:15:09:13\t|369\t|正常\t|Normal|\n|11837\t|10001|\t01JUL09:14:50:36\t|369\t|正常|\tNormal|\n|26629\t|10001\t|14DEC09:18:05:32\t|359\t|正常\t|Normal|\n|30850|\t10001\t|12APR10:13:02:20|\t399|\t正常|\tNormal|\n|32007\t|10001|\t04MAY10:16:45:58|\t269\t|正常|\tNormal|\n|36637\t|10001\t|04JUN10:20:03:06|\t0\t|赠送|\tPresented|\n|43108\t|10001|\t06JUL10:16:56:40|\t381\t|正常|\tNormal|\n\n1. 计算F 反应顾客对打折的偏好程度\n\n通过计算F反应客户对打折产品的偏好\n\n```\nF=trad_flow.groupby(['cumid','type'])[['transID']].count()\nF.head()\n```\n\n建立数据透视表\n\n```\nF_trans=pd.pivot_table(F,index='cumid',columns='type',values='transID')\nF_trans.head()\n```\n\n对缺失的数据填补为零\n\n```\nF_trans['Special_offer']= F_trans['Special_offer'].fillna(0)\nF_trans.head()\n```\n\n计算兴趣用户比例\n\n```\nF_trans[\"interest\"]=F_trans['Special_offer']/(F_trans['Special_offer']+F_trans['Normal'])\nF_trans.head()\n```\n\n2. 计算M反应客户的价值信息\n\n通过计算M反应客户的价值信息\n\n```\nM=trad_flow.groupby(['cumid','type'])[['amount']].sum()\nM.head()\n```\n\n数据透视，缺失值补零，计算价值用户\n\n```\nM_trans=pd.pivot_table(M,index='cumid',columns='type',values='amount')\nM_trans['Special_offer']= M_trans['Special_offer'].fillna(0)\nM_trans['returned_goods']= M_trans['returned_goods'].fillna(0)\nM_trans[\"value\"]=M_trans['Normal']+M_trans['Special_offer']+M_trans['returned_goods']\nM_trans.head()\n```\n\n3. 通过计算R反应客户是否为沉默客户\n\n定义一个从文本转化为时间的函数\n\n```\nfrom datetime import datetime\nimport time\ndef to_time(t):\n    out_t=time.mktime(time.strptime(t, '%d%b%y:%H:%M:%S'))  \n    return out_t\t\t\n```\n\n将时间进行转化\n\n```\nrad_flow[\"time_new\"]= trad_flow.time.apply(to_time)\n```\n\n获取高频消费客户\n\n```\nR=trad_flow.groupby(['cumid'])[['time_new']].max()\nR.head()\n```\n\n4. 构建模型，筛选目标客户\n\n```\n# In[12]\nfrom sklearn import preprocessing\nthreshold = pd.qcut(F_trans['interest'], 2, retbins=True)[1][1]\nbinarizer = preprocessing.Binarizer(threshold=threshold)\ninterest_q = pd.DataFrame(binarizer.transform(F_trans['interest'].values.reshape(-1, 1)))\ninterest_q.index=F_trans.index\ninterest_q.columns=[\"interest\"]\n# In[12]\nthreshold = pd.qcut(M_trans['value'], 2, retbins=True)[1][1]\nbinarizer = preprocessing.Binarizer(threshold=threshold)\nvalue_q = pd.DataFrame(binarizer.transform(M_trans['value'].values.reshape(-1, 1)))\nvalue_q.index=M_trans.index\nvalue_q.columns=[\"value\"]\n# In[12]\nthreshold = pd.qcut(R[\"time_new\"], 2, retbins=True)[1][1]\nbinarizer = preprocessing.Binarizer(threshold=threshold)\ntime_new_q = pd.DataFrame(binarizer.transform(R[\"time_new\"].values.reshape(-1, 1)))\ntime_new_q.index=R.index\ntime_new_q.columns=[\"time\"]\n# In[12]\nanalysis=pd.concat([interest_q, value_q,time_new_q], axis=1)\n# In[12]\n#analysis['rank']=analysis.interest_q+analysis.interest_q\nanalysis = analysis[['interest','value','time']]\nanalysis.head()\n\nlabel = {\n    (0,0,0):'无兴趣-低价值-沉默',\n    (1,0,0):'有兴趣-低价值-沉默',\n    (1,0,1):'有兴趣-低价值-活跃',\n    (0,0,1):'无兴趣-低价值-活跃',\n    (0,1,0):'无兴趣-高价值-沉默',\n    (1,1,0):'有兴趣-高价值-沉默',\n    (1,1,1):'有兴趣-高价值-活跃',\n    (0,1,1):'无兴趣-高价值-活跃'\n}\nanalysis['label'] = analysis[['interest','value','time']].apply(lambda x: label[(x[0],x[1],x[2])], axis = 1)\nanalysis.head()\n```\n\n\n"},{"title":"markdown_picture","url":"/2018/10/16/markdown-picture/","content":"# 序列图\n\n``` sequence\ntitle: 序列图sequence(示例)\nparticipant A\nparticipant B\nparticipant C\n\nnote left of A: A左侧说明\nnote over B: 覆盖B的说明\nnote right of C: C右侧说明\n\nA->A:自己到自己\nA->B:实线实箭头\nA-->C:虚线实箭头\nB->>C:实线虚箭头\nB-->>A:虚线虚箭头\t\t\t\t\t\n```\n\n关键词:\n\n1. title, 定义该序列图的标题\n2. participant, 定义时序图中的对象\n3. note, 定义对时序图中的部分说明\n4. {actor}, 表示时序图中的具体对象（名称自定义）\n\n针对note的方位控制主要包含以下几种关键词：\n\n1. left of, 表示当前对象的左侧\n2. right of, 表示当前对象的右侧\n3. over, 表示覆盖在当前对象（们）的上面\n\n针对{actor}的箭头分为以下几种：\n\n1. -> 表示实线实箭头\n2. –> 表示虚线实箭头\n3. ->> 表示实线虚箭头\n4. –>> 表示虚线虚箭头\n\n# 流程图\n\n```flow\nst=>start: 开始\ne=>end: 结束\nop=>operation: 操作\nsub=>subroutine: 子程序\ncond=>condition: 是或者不是?\nio=>inputoutput: 输出\n\nst(right)->op->cond\ncond(yes)->io(right)->e\ncond(no)->sub(right)->op\n```\n\n- start,end, 表示程序的开始与结束\n- operation, 表示程序的处理块\n- subroutine, 表示子程序块\n- condition, 表示程序的条件判断\n- inputoutput, 表示程序的出入输出\n- right,left, 表示箭头在当前模块上的起点(默认箭头从下端开始)\n- yes,no, 表示condition判断的分支(其可以和right,left同时使用)\n\n模块定义(模块标识与模块名称可以任意定义名称,关键词不可随意取名)如下:\n\n```\n模块标识=>模块关键词: 模块名称\n```\n\n连接定义如下:\n\n```\n模块标识1->模块标识2\n模块标识1->模块标识2->模块标识3\n... ...\n```\n\n```flow\nst=>start: Start\ne=>end: End\nop1=>operation: My Operation\nop2=>operation: Stuff\nsub1=>subroutine: My Subroutine\ncond=>condition: Yes\nor No?\nc2=>condition: Good idea\nio=>inputoutput: catch something...\n\nst->op1(right)->cond\ncond(yes, right)->c2\ncond(no)->sub1(left)->op1\nc2(yes)->io->e\nc2(no)->op2->e\n```\n\n![Alt text](https://g.gravizo.com/svg?\n  digraph G {\n​    aize =\"4,4\";\n​    main [shape=box];\n​    main -> parse [weight=8];\n​    parse -> execute;\n​    main -> init [style=dotted];\n​    main -> cleanup;\n​    execute -> { make_string; printf}\n​    init -> make_string;\n​    edge [color=red];\n​    main -> printf [style=bold,label=\"100 times\"];\n​    make_string [label=\"make a string\"];\n​    node [shape=box,style=filled,color=\".7 .3 1.0\"];\n​    execute -> compare;\n  }\n)\n","tags":["markdown"]},{"title":"data_integration","url":"/2018/10/16/data-integration/","content":"# 数据整合\n\n## 行列操作\n\n```python\nimport pandas as pd\nimport numpy as np\nsample = pd.DataFrame(np.random.randn(4,5),columns=['a','b','c','d','e'])\nsample\n```\n\n```\nOut[2]: \n          a         b         c         d         e\n0 -0.776807  2.355071 -0.940921  0.164487 -1.025772\n1  0.596704  0.962625  1.848441 -1.122676 -0.359290\n2 -0.092755 -0.124250 -0.259899 -0.111997 -1.816197\n3  0.372941  0.297850 -0.409256  0.485376 -2.790929\n```\n\n### 选择单列\n\n```\nsample['a']\n```\n\n\n\n```\nOut[4]: \n0   -0.776807\n1    0.596704\n2   -0.092755\n3    0.372941\nName: a, dtype: float64\n```\n\n数据框的ix，iloc，ioc方法都可以选择行，列，iloc方法只能使用数值作为索引来选择行列，loc方法在选择时能使用字符串索引，ix方法则可以使用两种索引\n\n```\nsample.ix[:,'a']\n```\n\n```\nOut[5]: \n0   -0.776807\n1    0.596704\n2   -0.092755\n3    0.372941\nName: a, dtype: float64\n```\n\n或者单选列\n\n```\nsample[['a']]\nOut[6]: \n          a\n0 -0.776807\n1  0.596704\n2 -0.092755\n3  0.372941\n```\n\n\n\n### 选择多行多列\n```\nsample.ix[0:2,0:2]\n```\n\n```\nOut[7]: \n          a         b\n0 -0.776807  2.355071\n1  0.596704  0.962625\n2 -0.092755 -0.124250\n\n```\n\n```\nsample.iloc[0:2,0:2]\n\n```\n\n### 创建，删除列\n\n第一种方式\n\n```\nsample['new_col1']=sample['a']-sample['b']\nsample\n```\n\n```\nOut[10]: \n          a         b         c         d         e  new_col1\n0 -0.776807  2.355071 -0.940921  0.164487 -1.025772 -3.131877\n1  0.596704  0.962625  1.848441 -1.122676 -0.359290 -0.365921\n2 -0.092755 -0.124250 -0.259899 -0.111997 -1.816197  0.031495\n3  0.372941  0.297850 -0.409256  0.485376 -2.790929  0.075091\n```\n\n第二种方式\n\n```\nsample.assign(new_col2=sample['a']-sample['b'],new_col3=sample['a']+sample['b'])\n```\n\n```\nOut[11]: \n          a         b         c    ...     new_col1  new_col2  new_col3\n0 -0.776807  2.355071 -0.940921    ...    -3.131877 -3.131877  1.578264\n1  0.596704  0.962625  1.848441    ...    -0.365921 -0.365921  1.559329\n2 -0.092755 -0.124250 -0.259899    ...     0.031495  0.031495 -0.217004\n3  0.372941  0.297850 -0.409256    ...     0.075091  0.075091  0.670792\n[4 rows x 8 columns]\n```\n\n删除列，第一种方式\n\n```\nsample.drop('a',axis=1)\n```\n\n```\nOut[12]: \n          b         c         d         e  new_col1\n0  2.355071 -0.940921  0.164487 -1.025772 -3.131877\n1  0.962625  1.848441 -1.122676 -0.359290 -0.365921\n2 -0.124250 -0.259899 -0.111997 -1.816197  0.031495\n3  0.297850 -0.409256  0.485376 -2.790929  0.075091\n```\n\n第二种方式，\n\n```\n# In\nsample.drop(['a','b'],axis=1)\n```\n\n```\n      c         d         e  new_col1\n\n0 -0.940921  0.164487 -1.025772 -3.131877\n1  1.848441 -1.122676 -0.359290 -0.365921\n```\n\n## 条件查询\n\n### 生成示例数据\n\n```\n# In[]\nsample = pd.DataFrame({'name':['Bob','Lindy','Mark',\"Miki\",'Sully','Rose'],\n                       'score':[98,78,88,77,69,69],\n                       'group':[1,1,1,2,1,2]})\nsample\n```\n\n```\nOut[14]: \n    name  score  group\n0    Bob     98      1\n1  Lindy     78      1\n2   Mark     88      1\n3   Miki     77      2\n4  Sully     69      1\n5   Rose     69      2\n```\n\n### 单条件查询\n\n涉及单条件查询时，一般会使用比较运算符，产生布尔类型的索引可用于条件查询。\n\n```\nsample.score >66\n```\n\n\n\n```\nOut[15]: \n0    True\n1    True\n2    True\n3    True\n4    True\n5    True\n```\n\n再通过指定的索引进行条件查询，返回bool值为True的数据：\n\n```\nsample[sample.score >66]\n\nOut[16]: \n    name  score  group\n0    Bob     98      1\n1  Lindy     78      1\n2   Mark     88      1\n3   Miki     77      2\n4  Sully     69      1\n5   Rose     69      2\n```\n\n### 多条件查询\n```\nsample[(sample.score >66) & (sample.group==1)]\nOut[17]: \n    name  score  group\n0    Bob     98      1\n1  Lindy     78      1\n2   Mark     88      1\n4  Sully     69      1\n```\n\n### 使用 qurey\n\n```\nsample.query('score > 90')\nOut[20]: \n  name  score  group\n0  Bob     98      1\n```\n\n其他查询\n\n查询sample中70到80之间的记录，并且将边界包含进来（inclusive=True）\n\n```\n# In[]\nsample[sample['score'].between(70,80,inclusive=True)]\nOut[21]: \n    name  score  group\n1  Lindy     78      1\n3   Miki     77      2\n```\n\n对于字符串列来说，可以使用isin 方法进行查询：\n\n```\nsample[sample['name'].isin(['Bob','Lindy'])]\nOut[24]: \n    name  score  group\n0    Bob     98      1\n1  Lindy     78      1\n```\n\n使用正则表达式匹配进行查询，例如查询姓名以M开头的人的所有记录：\n\n```\nsample[sample['name'].str.contains('[M]+')]\nOut[26]: \n   name  score  group\n2  Mark     88      1\n3  Miki     77      2\n```\n\n## 横向连接\n\nPandas Data Frame 提供 merge 方法以完成各种表格的横向连接操作，这种连接操作跟SQL语句的连接操作类似。\n\n```\ndf1 =pd.DataFrame({'id':[1,2,3],\n                   'col1':['a','b','c']})\ndf2 = pd.DataFrame({'id':[4,3],\n                    'col2':['d','e']})\ndf1\nOut[29]: \n   id col1\n0   1    a\n1   2    b\n2   3    c\ndf2\nOut[30]: \n   id col2\n0   4    d\n1   3    e\n```\n\n内连接使用merge函数示例，根据公共字段保留两表的共有信息，`how = 'innner'`参数表示使用内连接，`on`表示两表的公共字段，若公共字段再两表名称不一致时，可以通过 `left_on`和`right_on`指定：\n\n```\ndf1.merge(df2,how='inner',on='id')\nOut[32]: \n   id col1 col2\n0   3    c    e\ndf1.merge(df2,how='inner',left_on='id',right_on='id')\nOut[33]: \n   id col1 col2\n0   3    c    e\n```\n\n### 外连接\n\n外连接包括左连接，全连接，右连接\n\n#### 左连接\n\n左连接通过公共字段，保留坐标的全部信息，右表在左表缺失的信息会以NaN补全：\n\n```\ndf1.merge(df2,how='left',on='id')\nOut[34]: \n   id col1 col2\n0   1    a  NaN\n1   2    b  NaN\n2   3    c    e\n```\n\n#### 右连接\n\n右连接与左连接相对，右连接通过公共字段，保留右表的全部信息，左表在右表缺失的信息会以 NaN 补全。\n\n```\ndf1.merge(df2,how='right',on='id')\nOut[35]: \n   id col1 col2\n0   3    c    e\n1   4  NaN    d\n```\n\n#### 全连接\n\n全连接通过公共字段，保留右表的全部信息，两表相互缺失的信息会以 NaN 补全。\n\n\n\n```\ndf1.merge(df2,how='outer',on='id')\nOut[36]: \n   id col1 col2\n0   1    a  NaN\n1   2    b  NaN\n2   3    c    e\n3   4  NaN    d\n```\n\n## 行索引连接\n\n`pd.concat`可以完成横向和纵向的合并，这通过 ’axis=‘ 来控制，当参数axis= 1时表示进行横向合并。\n\n```\ndf1 = pd.DataFrame({'id':[1,2,3],\n                    'col1':['a','b','c']},\n                   index=[1,2,3])\ndf2 =pd.DataFrame({'id':[1,2,3],\n                   'col2':['aa','bb','cc']},\n                  index=[1,3,2])\npd.concat([df1,df2],axis=1)\nOut[37]: \n   id col1  id col2\n1   1    a   1   aa\n2   2    b   3   cc\n3   3    c   2   bb\n\n```\n\n## 纵向合并\n\n当参数 axis = 0 时，表示纵向合并。ignore_index= True 表示忽略df1 和 df2 的原先的行索引，合并后重新排列索引。\n\n```\npd.concat([df1,df2],ignore_index=True,axis=0)\nOut[43]: \n  col1 col2  id\n0    a  NaN   1\n1    b  NaN   2\n2    c  NaN   3\n3  NaN   aa   1\n4  NaN   bb   2\n5  NaN   cc   3\n```\n\n去除重复行\n\n```\npd.concat([df1,df2],ignore_index=True,axis=0).drop_duplicates()\n\nOut[44]: \n  col1 col2  id\n0    a  NaN   1\n1    b  NaN   2\n2    c  NaN   3\n3  NaN   aa   1\n4  NaN   bb   2\n5  NaN   cc   3\n```\n\n\n\n\n\n## 排序\n\n按照学生成绩降序排列数据，第一个参数表示排序的依据，ascending = False 代表降序排列，na_position='last'表示缺失值数据排列在数据的最后位置。\n\n```\nsample = pd.DataFrame({'name':['Bob','Lindy','Mark',\"Miki\",'Sully','Rose'],\n                       'score':[98,78,88,77,69,np.nan],\n                       'group':[1,1,1,2,1,2]})\nsample\n###\nsample.sort_values('score',ascending= False,na_position='last')\nOut[46]: \n    name  score  group\n0    Bob   98.0      1\n2   Mark   88.0      1\n1  Lindy   78.0      1\n3   Miki   77.0      2\n4  Sully   69.0      1\n5   Rose    NaN      2\n```\n\n## 分组汇总\n\n数据准备\n\n```\nsample = pd.read_csv('./sample.csv',encoding='utf-8')\nsample\nOut[58]: \n   chinese   class  grade  math   name\n0        88      1      1  98.0    Bob\n1        78      1      1  78.0  Lindy\n2        68      1      1  78.0   Miki\n3        56      2      2  77.0   Mark\n4        77      1      2  77.0  Sully\n5        56      2      2   NaN   Rose\n\n```\n\n分组汇总操作中，会涉及分组变量，度量变量和汇总统计量。pandas 提供了 groupby 方法进行分组汇总。\n\n在sample数据中，grade为分组变量，math 为度量变量，现需要查询grade 为1，2中数学成绩最高。\n\n### 分组变量\n\n在进行分组汇总时，分组变量可以有多个。\n\n```\nsample.groupby(['grade','class'])['math'].max()\nOut[65]: \ngrade  class\n1      1        98.0\n2      1        77.0\n       2        77.0\nName: math, dtype: float64\n```\n\n\n\n### 汇总变量\n\n在进行分组汇总时，汇总变量也可以多个。\n\n```\nsample.groupby('grade',)['math','chinese'].mean()\nOut[75]: \n            math  chinese\ngrade                    \n1      84.666667       78\n2      77.000000       63\n```\n\n### 汇总统计量\n\n| 方法   | 解释   | 方法     | 解释         |\n| ------ | ------ | -------- | ------------ |\n| mean   | 均值   | mad      | 平均绝对偏差 |\n| max    | 最大值 | count    | 计数         |\n| min    | 最小值 | skew     | 偏度         |\n| median | 中位数 | quantile | 指定分位数   |\n| std    | 标准差 |          |              |\n\n以上统计量方法可以直接接 groupby 对象使用，agg方法提供了一次汇总多个统计量的方法，例如，汇总各个班级的数学成绩的均值，最大值，最小值。\n\n```\nsample.groupby('class')['math'].agg(['mean','min','max'])\nOut[78]: \n        mean   min   max\nclass                   \n1      82.75  77.0  98.0\n2      77.00  77.0  77.0\n```\n\n\n\n\n\n### 多重索引\n\n\n\n以年级，班级对学生的数学，语文成绩进行分组汇总，汇总统计量为均值。此时df中有两个行索引和两个列索引。\n\n```\ndf=sample.groupby(['class','grade'])['math','chinese'].agg(['mean','min','max'])\nOut[80]: \n                  math             chinese        \n                  mean   min   max    mean min max\nclass grade                                       \n1     1      84.666667  78.0  98.0      78  68  88\n      2      77.000000  77.0  77.0      77  77  77\n2     2      77.000000  77.0  77.0      56  56  56\n```\n\n查询各个年级、班级的数学成绩的最小值。\n\n```\ndf['math']['min']\nOut[84]: \nclass  grade\n1      1        78.0\n       2        77.0\n2      2        77.0\nName: min, dtype: float64\n```\n\n\n","tags":["数据整合"],"categories":["数据挖掘"]},{"title":"data_mining","url":"/2018/10/16/data-mining/","content":"[TOC]\n\n# Python 常用数据分析框架\n\n| 名称       |             解释             |\n| ---------- | :--------------------------: |\n| Numpy      |  数组，矩阵的存储，运算框架  |\n| Scipy      | 提供统计，线性代数等计算框架 |\n| Pandas     |  结构化数据的整合，处理框架  |\n| Statsmodel |    常见的统计分析框架模型    |\n| Matplotlib |        数据可视化框架        |\n\n\n\n# python 基础数据类型\n\n| 名称    | 解释   | 示例        |\n| ------- | ------ | ----------- |\n| str     | 字符串 | 'a', '2'    |\n| float   | 浮点数 | 1.23， 3.45 |\n| int     | 整数   | 3，5        |\n| bool    | 布尔   | Ture, False |\n| complex | 复数   | 1+2j, 2 +0j |\n\n\n\n# Python 数据格式转换\n\n| 数据类型 | 转换函数  |\n| -------- | --------- |\n| Str      | str()     |\n| Float    | float()   |\n| Int      | Int()     |\n| Bool     | bool()    |\n| Complex  | complex() |\n\n# 读取数据\n\n```\nimport pandas as pd\ncsv =  pd.read_csv('filename')\n```\n\n# 制图步骤\n\n在进行描述性图展示时，制图分为以下四步：\n\n1. 整理原始数据：对初始数据进行预处理和清洗，已达到制图得要求。\n2. 明确表达的信息： 根据初始可用数据，明确分析要表达的信息。\n3. 确定比较的类型： 明确要表达的信息中对目标比较的类型。\n4. 选择图表类型： 选择合适的图表类型，进行绘制并展示。\n\n```flow\n\na=>operation: 数据\nb=>operation: 信息\nc=>operation: 相对关系\nd=>operation: 图形\n\na(right)->b(right)->c(right)->d\n```\n\n\n","tags":["数据分析"]},{"title":"UML","url":"/2018/10/16/UML/","content":"\n\n\n# 制图步骤\n\n在进行描述性图展示时，制图分为以下四步：\n\n1. 整理原始数据：对初始数据进行预处理和清洗，已达到制图得要求。\n2. 明确表达的信息： 根据初始可用数据，明确分析要表达的信息。\n3. 确定比较的类型： 明确要表达的信息中对目标比较的类型。\n4. 选择图表类型： 选择合适的图表类型，进行绘制并展示。\n\n``` flow\n\na=>operation: 数据\nb=>operation: 信息\nc=>operation: 相对关系\nd=>operation: 图形\n\na(right)->b(right)->c(right)->d\n```\n\n\n\n\n(refer link)[http://www.gravizo.com/#howto]\n","tags":["markdown"]},{"title":"es-python-client","url":"/2018/10/12/es-python-client/","content":"# python 操作 Elasticsearch\n\n## 安装 Elasticsearch 模块\n`pip install elasticsearch`\n\n## 添加数据\n``` \nfrom elasticsearch import Elasticsearch\n\n# 默认host为localhost,port为9200.但也可以指定host与port\nes = Elasticsearch([{'host': '192.168.10.21', 'port': 9200}])\ndoc = {\n     'author': 'kimchy',\n    'text': 'Elasticsearch: cool. bonsai cool.',\n     'timestamp': localtime(),\n      }\nres = es.index(index=\"test-index\", doc_type='tweet', id=1, body=doc)\nprint(res)\n```\n如果创建成功会返回以下结果\n``` \n{'_index': 'test-index', '_type': 'tweet', '_id': '1', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 2, 'failed': 0}, '_seq_no': 0, '_primary_term': 1}\n\n```\n## 创建索引\n```\nes.indices.create(index='irisaa') \n```\n## 删除索引\n``` \nes.indices.create(index='irisaa')\n```\n## 查看集群状态\n```\nes.cluster.health(wait_for_status='yellow', request_timeout=1)\n```\n## 查询数据\n### 按照 id 来查询数据\n``` \nres = es.get(index=\"test-index\", doc_type='tweet', id=1)\nprint(res['_source'])\n```\n操作成功后返回如下结果：\n```\n{'author': 'kimchy', 'text': 'Elasticsearch: cool. bonsai cool.', 'timestamp': [2018, 10, 12, 14, 9, 44, 4, 285, 0]}\n\n```\n### 按照DSL语句查询\n```   \nres = es.search(index=\"test-index\", body={\"query\": {\"match_all\": {}}})\nprint(res)\n```\n操作成功后结果，返回如下结果：\n```\n{'took': 2, 'timed_out': False, '_shards': {'total': 5, 'successful': 5, 'skipped': 0, 'failed': 0}, 'hits': {'total': 1, 'max_score': 1.0, 'hits': [{'_index': 'test-index', '_type': 'tweet', '_id': '1', '_score': 1.0, '_source': {'author': 'kimchy', 'text': 'Elasticsearch: cool. bonsai cool.', 'timestamp': [2018, 10, 12, 14, 9, 44, 4, 285, 0]}}]}}\n```\n\n## 更新数据\n```\nes = Elasticsearch()\ndoc = {\n     'author': 'kimchy',\n    'text': 'ok.',\n     'timestamp': localtime(),\n      }\nresult = es.update(index=\"test-index\", doc_type='tweet', id=1, body=doc)\nprint(result) \n```\n\n\n## 删除数据\n```angular2html\n\nes.delete_by_query(index='twtter',doc_type='_doc',body={\n   \"query\": {\n     \"match\": {\n       \"message\": \"some message\"\n     }\n   }\n })\n```\n\n## 批量化导入es\n```\nfrom elasticsearch import helpers\ndef gendata(index, type, jsons):\n    for json in jsons:\n        yield {\n            \"_index\": index,\n            \"_type\": type,\n            \"_source\": json,\n        }\n        \nhelpers.bulk(es, gendata(index='index-test', jsons))        \n```\n","tags":["ELK"]},{"title":"ELK_docker-compose.md","url":"/2018/10/12/ELK-docker-compose-md/","content":"# elasticsearch 集群部署\n我们使用dockers-compose实现单机多节点的部署\n\n## 安装docker\n在测试或开发环境中 Docker 官方为了简化安装流程，提供了一套便捷的安装脚本，Ubuntu 系统上可以使用这套脚本安装：\n``` \n$ curl -fsSL get.docker.com -o get-docker.sh\n$ sudo sh get-docker.sh --mirror Aliyun\n```\n执行这个命令后，脚本就会自动的将一切准备工作做好，并且把 Docker CE 的 Edge 版本安装在系统中。\n### 启动 Docker CE\n \n``` \n$ sudo systemctl enable docker\n$ sudo systemctl start docker\n```\n\n### 建立 docker 用户组\n默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组。\n\n`$ sudo groupadd docker\n`\n\n将当前用户 加入 ``docker`` 组：\n\n`$ sudo usermod -aG docker $USER`\n\n### 测试dockers 是否安装正确\n```\n$ docker run hello-world\n\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\nca4f61b1923c: Pull complete\nDigest: sha256:be0cd392e45be79ffeffa6b05338b98ebb16c87b255f48e297ec7f98e123905c\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://cloud.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/engine/userguide/\n```\n\n## 安装docker-compopse\nCompose 项目是 Docker 官方的开源项目，负责实现对 Docker 容器集群的快速编排。\n它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）。\n\nCompose 中有两个重要的概念：\n\n- 服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。\n\n- 项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。\n\nCompose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。\n\nCompose 项目由 Python 编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。因此，只要所操作的平台支持 Docker API，就可以在其上利用 Compose 来进行编排管理。\n\n### PIP 安装 dockerpose\n``$ sudo pip install -U docker-compose\n``\n\n### 编写 docker-compose.yml\n编写 docker-compose.yml 文件,输入以下内容：\n```\nversion: '2.2'\nservices:\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.0\n    container_name: elasticsearch\n    environment:\n      - cluster.name=docker-cluster\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n        - esdata1:/usr/share/elasticsearch/data\n    volumes:\n      - /home/ethan/EKL/config:/usr/share/elasticsearch/config\n    ports:\n      - 9200:9200\n      - 9300:9300\n    networks:\n      - esnet\n\n  elasticsearch2:\n    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.0\n    container_name: elasticsearch2\n    environment:\n      - cluster.name=docker-cluster\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n      - \"discovery.zen.ping.unicast.hosts=elasticsearch\"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n      - esdata2:/usr/share/elasticsearch/data\n    networks:\n      - esnet\n\n  elasticsearch3:\n    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.0\n    container_name: elasticsearch3\n    environment:\n      - cluster.name=docker-cluster\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n      - \"discovery.zen.ping.unicast.hosts=elasticsearch\"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n      - esdata3:/usr/share/elasticsearch/data\n    networks:\n      - esnet\n  kibana:\n    image: docker.elastic.co/kibana/kibana:6.4.0\n    container_name: kibana\n    environment:\n      SERVER_NAME: kibana\n      ELASTICSEARCH_URL: http://elasticsearch:9200\n    ports:\n      - 5601:5601\n    networks:\n      - esnet\n\nvolumes:\n  esdata1:\n    driver: local\n  esdata2:\n    driver: local\n  esdata3:\n    driver: local\n\nnetworks:\n  esnet:\n\n```\n## 运行 eslasticsearch-kibana\n```\ndocker-compose up\n\n```\n","tags":["elasticsearch"]},{"title":"Elk初探","url":"/2018/10/10/Elk_platfrom/","content":"> 启动elaticsearch需要java环境，请自己谷歌搭建哈\n\n# elaticsearch\n下载tar包\n```powershell\nwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.4.1.tar.gz\n\n```\n\n解压\n```\ntar -zxvf elasticsearch-6.4.1.tar.gz\n```\n进入elaticsearch 可执行文件目录\n```angular2html\ncd elasticsearch-6.4.0/bin\n```\n启动elaticsearch\n```angular2html\n./elaticseaerch\n```\n如果没有抱任何错误，正常来说应该是可以通过restful API来访问的。\n````\ncurl -X GET \"localhost:9200/_cat/health?v\"\n\n````\n对应的返回结果\n```\nepoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\n1538014388 10:13:08  elasticsearch green           1         1      0   0    0    0        0             0                  -                100.0%\n```\n由此我们可以认为elaticsearch已经启动成功了。\n\n# kibana\n下载tar包\n```angular2html\nhttps://artifacts.elastic.co/downloads/kibana/kibana-6.4.1-linux-x86_64.tar.gz\n```\n解压tar包\n```\ntar -zxvf kibana-6.4.1-linux-x86_64.tar.gz\n```\n\n更改kibana配置文件\n```\nvim  /kibana-6.4.1-linux-x86_64/config/kibana.yml\n```\n添加或解注释以下内容\n```\n server.port: 5601\n server.host: \"localhost\"\n elasticsearch.url: \"http://localhost:9200\"\n\n```\n进入kibana可执行文件目录\n```\ncd kibana-6.4.1-linux-x86_64/bin/\n```\n启动kibana\n\n`./kibana`\n\n配置成功后，用浏览器访问`http://ip:5601`可以看到以下页面\n![](/image/kibana.png)\n\n\n# elaticsearc 基本操作\n## 检查集群状态\n```\ncurl -X GET \"localhost:9200/_cat/health?v\"\n\n```\n获取结果如下显示\n```\nepoch      timestamp cluster     status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\n1538019385 11:36:25  data-mining green           1         1      1   1    0    0        0             0                  -                100.0%\n\n```\n## 列出所有索引\n``` \ncurl -X GET \"localhost:9200/_cat/indices?v\"\n```\n如下图所示，我们只有一个索引\n```\nhealth status index   uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   .kibana eFUOj6qJTQCt4xQdvbv8KA   1   0          1            0        4kb            4kb\n\n```\n\n## 创建一个自定义的索引\n```angular2html\n curl -X PUT \"localhost:9200/customer?pretty\"\n```\n对插入的索引返回结果\n```\n\n{\n  \"acknowledged\" : true,\n  \"shards_acknowledged\" : true,\n  \"index\" : \"customer\"\n}\n\n```\n\n再次查看当前索引\n````angular2html\n curl -X GET \"localhost:9200/_cat/indices?v\"\n````\n可以观察到多了一个名为‘customer’的新索引\n```\nhealth status index    uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   .kibana  eFUOj6qJTQCt4xQdvbv8KA   1   0          1            0        4kb            4kb\nyellow open   customer k50FrrMLScGvwzOvfiF0fg   5   1          0            0       401b           401b\n```\n## 插入一个文档\n```angular2html\ncurl -X PUT \"localhost:9200/customer/_doc/1?pretty\" -H 'Content-Type: application/json' -d'\n{\n  \"name\": \"John Doe\"\n}\n'\n```\n如果操作正常，那么插入成功后会返回以下结果\n``` \n{\n  \"_index\" : \"customer\",\n  \"_type\" : \"_doc\",\n  \"_id\" : \"1\",\n  \"_version\" : 1,\n  \"result\" : \"created\",\n  \"_shards\" : {\n    \"total\" : 2,\n    \"successful\" : 1,\n    \"failed\" : 0\n  },\n  \"_seq_no\" : 0,\n  \"_primary_term\" : 1\n}\n```\n## 按照_id查询索引内的文档\n```\ncurl -X GET \"localhost:9200/customer/_doc/1?pretty\"\n```\n查看返回结果，\"_index\"为当前查询的索引，“_type”为查询的文档类型，“_id”为文档所在的id，\n“_source”为我们要查询的内容\n\n```\n\n{\n  \"_index\" : \"customer\",\n  \"_type\" : \"_doc\",\n  \"_id\" : \"1\",\n  \"_version\" : 1,\n  \"found\" : true,\n  \"_source\" : {\n    \"name\" : \"John Doe\"\n  }\n}\n```\n# elastcisear-head\n为了更方便的地查询es中的数据，我推荐使用es插件elasticsearch-head来快速检索es中的数据。\n\n传送门：[elastcsearch-head](https://github.com/mobz/elasticsearch-head)\n\n如果你使用的版本是在elasticsearch 5之后，还需要对elasticsearch 进行配置\n```\ncat >> elasticsearch-6.4.0/config/elasticsearch.yml << EOF\nhttp.cors.enabled: true\nhttp.cors.allow-origin: \"*\"\nEOF\n```\n\n\n## 安装 \n由于elasticsearch-head 需要nodejs，所以我们需要先安装 nodejs 以及 npm\n## nodejs 安装\n```\n$ sudo apt install nodejs\n$ sudo apt instlal npm\n```\nnodejs 安装完后，我们就可以把elasticsearch-head 下载，进行配置了。\n```\ngit clone git://github.com/mobz/elasticsearch-head.git\ncd elasticsearch-head\nnpm install\nnpm run start\n```\n操作成功后的输出显示\n```\n $ npm run start\n\n> elasticsearch-head@0.0.0 start /home/ethan/ekl/elasticsearch-head\n> grunt server\n\n(node:22696) ExperimentalWarning: The http2 module is an experimental API.\nRunning \"connect:server\" (connect) task\nWaiting forever...\nStarted connect web server on http://localhost:9100\n```\n![](/image/eshead.png)\n\n这样我们最简单的数据搜素平台就搭建成功了。\n","tags":["Elk"]},{"title":"Ethan Lee","url":"/2018/08/18/mygirl/","content":"# say something to mygirl\n\n\n<blockquote class=\"blockquote-center\">我知道到遇见你不容易,错过了会很可惜,我不希望余生都是回忆,我希望余生都是你,我爱你</blockquote>\n\n\n![](http://img02.tooopen.com/images/20160509/tooopen_sy_161967094653.jpg)\n\n[link](https://ethan2lee.github.io)\n","tags":["paper"]}]