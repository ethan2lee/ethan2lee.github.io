[{"title":"shadowsocks_config","url":"/2018/10/20/shadowsocks-config/","content":"# 搭建shadowsocks server 和 client端\n\n## sever 端\n\n```\nsudo apt update\nsudo apt install shadowsocks\n```\n\n下载完成后，配置shadowsocks 文件，\n\n```\nvim /etc/shadowsocks/config.json\n```\n\n```\n{\n    \"server\":\"ethan2lee.online\", #you server ip\n    \"server_port\":8888,\n    \"local_address\": \"127.0.0.1\",\n    \"local_port\":1080,\n    \"password\":\"your_password\",\n    \"timeout\":300,\n    \"method\":\"aes-256-cfb\"\n}\n```\n\n一般来说，我们只需要更改“server”和“password”字段的值即可，根据自己实际情况配置。更改完后，保存。\n\n接下来就是启动server端了，先设置开机自启，再启动。\n\n```\n➜  ~ sudo systemctl enable  shadowsocks.service\n\nshadowsocks.service is not a native service, redirecting to systemd-sysv-install\nExecuting /lib/systemd/systemd-sysv-install enable shadowsocks\n```\n\n\n\n```\n➜  ~ sudo systemctl start   shadowsocks.service\n```\n\n\n\n查看服务状态：\n\n```\n➜  ~ sudo systemctl status  shadowsocks.service\n● shadowsocks.service - LSB: Fast tunnel proxy that helps you bypass firewalls\n   Loaded: loaded (/etc/init.d/shadowsocks; bad; vendor preset: enabled)\n   Active: active (exited) since Sat 2018-10-20 04:59:16 UTC; 4s ago\n​     Docs: man:systemd-sysv-generator(8)\n  Process: 634 ExecStart=/etc/init.d/shadowsocks start (code=exited, status=0/SUCCESS)\n​    Tasks: 0\n   Memory: 0B\n​      CPU: 0\n\nOct 20 04:59:16 ethan systemd[1]: Starting LSB: Fast tunnel proxy that helps you bypass firewalls...\nOct 20 04:59:16 ethan systemd[1]: Started LSB: Fast tunnel proxy that helps you bypass firewalls.\n```\n\n这样我们shadowsocks的server端就配置完成了。\n\n## client端\n\n在需要用shadowsocks的server服务的机子也安装shaodowsocks。\n\n```\nsudo apt update\nsudo apt install shadowsocks\n```\n\n\n\n\n\n同样需要配置一下shadowsocks的配置文件，一般跟server端的配置差不多，但是需要自行更改“local_port”的字段值。\n\n```\nvim /etc/shadowsocks/config.json\n```\n\n\n\n```\n{\n    \"server\":\"ethan2lee.online\", #you server ip\n    \"server_port\":8888,\n    \"local_address\": \"127.0.0.1\",\n    \"local_port\":1080, ### 在clien端可以自主配置的\n    \"password\":\"your_password\",\n    \"timeout\":300,\n    \"method\":\"aes-256-cfb\"\n}\n```\n\n配置完成之手进行保存。\n\n开启client 端，这样我们只要使用sock5协议通过代理端口1080访问网络，就可以完全使用server的网络环境了。\n\n```\n➜  ~ sslocal -c /etc/shadowsocks/config.json\nINFO: loading config from /etc/shadowsocks/config.json\nshadowsocks 2.1.0\n2018-10-20 05:10:08 INFO     starting local at 127.0.0.1:1080\n```\n\n打开之后，shadowsocks的client 端是运行在前台的。我们先按Ctr +c 终止进行，再输入以下命令，让它在后台运行：\n\n```\n➜  ~ nohup sslocal -c /etc/shadowsocks/config.json &\n[1] 9479\nnohup: ignoring input and appending output to 'nohup.out'\n```\n\n这样它就作为守护进程在运行着了。\n\n## proxychains\n\n安装完shadowsocks之后，如果想让终端命令也经常代理，那么我们就需要proxychains来帮忙了。\n\n安装proxychains\n\n```\nsudo apt install proxychains\n```\n\n修改proxychains 配置文件，直接找到最后一行。\n\n```\n➜  ~ vim /etc/proxychains.conf\n```\n\n将`socks4 127.0.0.1 9095`改为\n\n```\nsocks5 127.0.0.1 1080 \n```\n\n1080 为你刚刚配置shadowsocks client端的“local_host”字段的值，配置完成后保存。\n\n\n\n\n\n经过以上所有步骤，我们可以通过一下命令来查看我们是否配置正确了：\n\n\n\n```\nproxychains wget www.google.com\n```\n\n如果有一下输出，那么就证明你配置成功了：\n\n```\n➜  ~ proxychains wget www.google.com\nProxyChains-3.1 (http://proxychains.sf.net)\n--2018-10-20 05:28:01--  http://www.google.com/\nResolving www.google.com (www.google.com)... |DNS-request| www.google.com\n|S-chain|-<>-127.0.0.1:1080-<><>-4.2.2.2:53-<><>-OK\n|DNS-response| www.google.com is 74.125.20.103\n74.125.20.103\nConnecting to www.google.com (www.google.com)|74.125.20.103|:80... |S-chain|-<>-127.0.0.1:1080-<><>-74.125.20.103:80-<><>-OK\nconnected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: ‘index.html’\n\nindex.html                        [ <=>                                           ]  11.05K  --.-KB/s    in 0s\n\n2018-10-20 05:28:02 (63.6 MB/s) - ‘index.html’ saved [11319]\n\n```\n\n\n\n如果没有，就只能继续百度了。\n","tags":["vpn"]},{"title":"更换ubuntu18源","url":"/2018/10/20/更换ubuntu18源/","content":"# Ubuntu 18.04换国内源 \n\n\n\n更换源前，先对本来的源文件做好备份。\n\n```\nmv /etc/apt/sources.list  /etc/apt/sourceslist-save\n```\n\n备份完成后，我们就可再建立source.list\n\n```\nvim /etc/apt/sources.list \n```\n\n添加以下内容\n\n```\n##中科大源\n\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multivers\n```\n\n然后执行以下命令：\n\n```\nsudo apt update\nsudo apt upgrade\n```\n\n\n\n\n\n更改其他源：\n\n阿里源\n\n```\ndeb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\n```\n\n360源\n\n```\ndeb http://mirrors.163.com/ubuntu/ bionic main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiverse\n```\n\n清华源\n\n```\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\n```\n\n\n","tags":["linux"]},{"title":"统计量分析示例","url":"/2018/10/19/统计量分析示例/","content":"\n# \n\n# 描述性统计分析基础\n\n- 数据集描述与属性说明\n\n- ID 客户编号\n\n- Suc_flag   成功入网标识\n\n- ARPU   入网后ARPU\n\n- PromCnt12  12个月内的营销次数\n\n- PromCnt36  36个月内的营销次数\n\n- PromCntMsg12   12个月内发短信的次数\n\n- PromCntMsg36   36个月内发短信的次数\n\n- Class  客户重要性等级(根据前运营商消费情况)\n\n- Age    年龄\n\n- Gender 性别\n\n- HomeOwner  是否拥有住房\n\n- AvgARPU    当地平均ARPU\n\n- AvgHomeValue   当地房屋均价\n\n- AvgIncome  当地人均收入\n\n```\nimport os\nimport pandas as pd\nimport numpy as np\n\npd.set_option('display.max_columns', None)\n#os.chdir('Q:/data')\n#os.getcwd()\n```\n\n读取数据\n\n```\ncamp= pd.read_csv('teleco_camp.csv')\ncamp.head(10)\n```\n\n数据预处理\n\n```\ncamp.dtypes\n```\n\n```\nID                 int64\nSuc_flag        category\nARPU             float64\nPromCnt12        float64\nPromCnt36        float64\nPromCntMsg12     float64\nPromCntMsg36     float64\nClass           category\nAge              float64\nGender            object\nHomeOwner         object\nAvgARPU          float64\nAvgHomeValue     float64\nAvgIncome        float64\ndtype: object\n```\n\ncamp.describe(include='all')\n\n|        | ID            | Suc_flag | ARPU        | PromCnt12   | PromCnt36   | PromCntMsg12 | PromCntMsg36 | Class  | Age         | Gender | HomeOwner | AvgARPU     | AvgHomeValue  | AvgIncome     |\n| ------ | ------------- | -------- | ----------- | ----------- | ----------- | ------------ | ------------ | ------ | ----------- | ------ | --------- | ----------- | ------------- | ------------- |\n| count  | 9686.000000   | 9686.0   | 4843.000000 | 9686.000000 | 9686.000000 | 9686.000000  | 9686.000000  | 9686.0 | 7279.000000 | 9686   | 9686      | 9686.000000 | 9583.000000   | 7329.000000   |\n| unique | NaN           | 2.0      | NaN         | NaN         | NaN         | NaN          | NaN          | 4.0    | NaN         | 3      | 2         | NaN         | NaN           | NaN           |\n| top    | NaN           | 1.0      | NaN         | NaN         | NaN         | NaN          | NaN          | 2.0    | NaN         | F      | H         | NaN         | NaN           | NaN           |\n| freq   | NaN           | 4843.0   | NaN         | NaN         | NaN         | NaN          | NaN          | 3303.0 | NaN         | 5223   | 5377      | NaN         | NaN           | NaN           |\n| mean   | 97975.474086  | NaN      | 78.121722   | 3.447212    | 7.337059    | 1.178402     | 2.390935     | NaN    | 59.150845   | NaN    | NaN       | 52.905156   | 112179.202755 | 53513.457361  |\n| std    | 56550.171120  | NaN      | 62.225686   | 1.231890    | 1.952436    | 0.287226     | 0.914314     | NaN    | 16.516400   | NaN    | NaN       | 4.993775    | 98522.888583  | 19805.168339  |\n| min    | 12.000000     | NaN      | 5.000000    | 0.750000    | 1.000000    | 0.200000     | 0.400000     | NaN    | 0.000000    | NaN    | NaN       | 46.138968   | 7500.000000   | 2499.000000   |\n| 25%    | 48835.500000  | NaN      | 50.000000   | 2.900000    | 6.250000    | 1.000000     | 1.400000     | NaN    | 47.000000   | NaN    | NaN       | 49.760116   | 53200.000000  | 40389.000000  |\n| 50%    | 99106.000000  | NaN      | 65.000000   | 3.250000    | 7.750000    | 1.200000     | 2.600000     | NaN    | 60.000000   | NaN    | NaN       | 50.876672   | 77700.000000  | 48699.000000  |\n| 75%    | 148538.750000 | NaN      | 100.000000  | 3.650000    | 8.250000    | 1.400000     | 3.200000     | NaN    | 73.000000   | NaN    | NaN       | 54.452822   | 129350.000000 | 62385.000000  |\n| max    | 191779.000000 | NaN      | 1000.000000 | 15.150000   | 19.500000   | 3.600000     | 5.600000     | NaN    | 87.000000   | NaN    | NaN       | 99.444787   | 600000.000000 | 200001.000000 |\n\n\n\n\n\n\n# 描述性统计与探索型数据分析\n\n## 分类变量分析\n\n可以查看列原因元素的种类\n\n```\ncamp['Suc_flag'].groupby(camp['Suc_flag']).count()\n```\n\n```\nSuc_flag\n0    4843\n1    4843\nName: Suc_flag, dtype: int64\n```\n\n\n\n\n\n## 连续变量分析\n### 数据的集中趋势\n#### ARPU的均值与中位数\n```\n\nfs = camp['ARPU'] # 可以使用camp.ARPU \nprint('mean = %6.4f' %fs.mean())                     # 求fs的均值\nprint('median = %6.4f' %fs.median() )                # 求fs的中位数\nprint('quantiles\\n', fs.quantile([0.25, 0.5, 0.75])) # 求a的上下四分位数与中位数\n```\n\n```\nmad = 38.1896\nrange = 995.0000\nvar = 3872.0359\nstd = 62.2257\n```\n\n\n\n```\nget_ipython().run_line_magic('matplotlib', 'inline'）\n\nfs.plot(kind='hist')\n```\n\n```\n(array([2.510e+03, 1.978e+03, 2.160e+02, 9.800e+01, 4.000e+00, 7.000e+00,\n        0.000e+00, 2.500e+01, 0.000e+00, 0.000e+00, 0.000e+00, 1.000e+00,\n        0.000e+00, 0.000e+00, 4.000e+00]),\n array([   5.        ,   71.33333333,  137.66666667,  204.        ,\n         270.33333333,  336.66666667,  403.        ,  469.33333333,\n         535.66666667,  602.        ,  668.33333333,  734.66666667,\n         801.        ,  867.33333333,  933.66666667, 1000.        ]),\n <a list of 15 Patch objects>)\n```\n\n![](image/output_11_1.png)\n\n\n### 数据的离散程度\n\n```\nprint ('mad = %6.4f' %fs.mad())      # 求平均绝对偏差 mad = np.abs(fs - fs.mean()).mean()\nprint ('range = %6.4f' %(fs.max(skipna=True) - fs.min(skipna=True))) # 求极差\nprint ('var = %6.4f' %fs.var())   # 求方差\nprint ('std = %6.4f' %fs.std())   # 求标准差\n```\n\n\n\n\n\n### 数据的偏度与峰度\n```\nimport matplotlib.pyplot as plt\n\nplt.hist(fs.dropna(), bins=15)\n```\n\n![](image/output_15_1.png)\n\n```\nprint ('skewness = %6.4f' %fs.skew(skipna=True))\nprint ('kurtosis = %6.4f' %fs.kurt(skipna=True))\n```\n\n```\nskewness = 5.1695\nkurtosis = 52.8509\n```\n\n\n### apply\\map\\groupby及其它相关\n\n```\ndata = pd.DataFrame(data={'a':range(1,11), 'b':np.random.randn(10)})\ndata.T\n```\n\n|      | 0         | 1        | 2        | 3        | 4        | 5         | 6        | 7       | 8        | 9         |\n| ---- | --------- | -------- | -------- | -------- | -------- | --------- | -------- | ------- | -------- | --------- |\n| a    | 1.000000  | 2.000000 | 3.000000 | 4.000000 | 5.000000 | 6.000000  | 7.000000 | 8.0000  | 9.000000 | 10.000000 |\n| b    | -0.087919 | 0.903531 | 0.603965 | 0.203005 | 0.282077 | -1.420298 | 0.283303 | -0.0565 | 1.047595 | -0.787566 |\n\n```\ndata.apply(np.mean) # 等价于data.mean()，是其完整形式\n```\n\n```\na    5.500000\nb    0.097119\ndtype: float64\n```\n\n```\ndata.apply(lambda x: x.astype('str')).dtypes # DataFrame没有astype方法，只有Series有\n```\n"},{"title":"data_clearning","url":"/2018/10/17/data-clearning/","content":"# 数据清洗\n\n> 数据清洗， 是整个数据分析过程中不可缺少的一个环节，其结果质量直接关系到模型效果和最终结论。在实际操作中，数据清洗通常会占据分析过程的50%—80%的时间。\n\n## 去除重复数据\n\n在获取到的数据中，我们发现会有重复数据。我们使用Pandas 提供的方法 duplicated 和 drop_duplicates来去重。\n\n\n\n```\nsample =  pd.DataFrame({'id':[1,1,1,3,4,5],\n                        'name':['Bob','Bob','Mark','Miki','Sully','Rose'],\n                        'score':[99,99,87,77,77,np.nan],\n                        'group':[1,1,1,2,1,2]})\nsample\nOut[85]: \n   id   name  score  group\n0   1    Bob   99.0      1\n1   1    Bob   99.0      1\n2   1   Mark   87.0      1\n3   3   Miki   77.0      2\n4   4  Sully   77.0      1\n5   5   Rose    NaN      2\n\n```\n\n查找重复数据：\n\n```\nsample[sample.duplicated()]\nOut[86]: \n   id name  score  group\n1   1  Bob   99.0      1\n```\n\n需要去重时：\n\n```\nsample.drop_duplicates()\nOut[87]: \n   id   name  score  group\n0   1    Bob   99.0      1\n2   1   Mark   87.0      1\n3   3   Miki   77.0      2\n4   4  Sully   77.0      1\n5   5   Rose    NaN      2\n```\n\n按列去重时，需要加入列索引：\n\n```\nsample.drop_duplicates('id')\nOut[88]: \n   id   name  score  group\n0   1    Bob   99.0      1\n3   3   Miki   77.0      2\n4   4  Sully   77.0      1\n5   5   Rose    NaN      2\n```\n\n## 缺失值处理\n\n在数据挖掘中，面对数据中存在缺失值时，我们一般采取以下几种办法：\n\n1. 缺失值较多的特征处理\n\n当缺失值处于20%~80%，每个缺失值可以生成一个指示哑变量，参与后续的建模。\n\n2.缺失较少时\n\n首先需要根据业务理解处理缺失值，弄清楚缺失值产生的原因，是故意缺失还是随机缺失。可以依靠业务经验进行填补。连续变量可以使用均值或中位数进行填补。\n\n- 把 NaN 直接作为一个特征，假设0表示\n\n```\ndf.fillna(0) \n```\n\n\n\n- 用均值填充\n\n```\n# 将所有行用各自的均值填充 \ndf.fillna(df.mean())  \n# 将所有行用各自的均值填充 \ndf.fillna(df.mean()['collum_name])\n```\n\n\n\n- 用上下数据填充\n\n```\n# 用前一个数据替代NaN\ndf.fillnan(method=\"pad\")\n\n# 与pad相反，bfill表示用后一个数据代替NaN\ndf.fillna(method=)\n```\n\n\n\n- 用插值填充\n\n```\n# 插值法就是通过两点（x0,y0）,(x1,y1)估计中间点的值\ndf.interpolate() \n```\n\n\n\n\n\n- 查看数据值缺失情况，我们可有构造一个lambda 函数来查看缺失值。\n\n```\nsample =  pd.DataFrame({'id':[1,1,1,3,4,np.nan],\n                        'name':['Bob','Bob','Mark','Miki','Sully',np.nan],\n                        'score':[99,99,87,77,77,np.nan],\n                        'group':[1,1,1,2,1,np.nan]})\nsample\nsample.apply(lambda col:sum(col.isnull())/col.size)\nOut[91]: \nid       0.166667\nname     0.166667\nscore    0.166667\ngroup    0.166667\ndtype: float64\n```\n\n### 已指定值填补\n\n均值\n\n```\nsample.score.fillna(sample.score.mean())\nOut[93]: \n0    99.0\n1    99.0\n2    87.0\n3    77.0\n4    77.0\n5    87.8\nName: score, dtype: float64\n```\n\n中位数\n\n```\nsample.score.fillna(sample.score.median())\nOut[94]: \n0    99.0\n1    99.0\n2    87.0\n3    77.0\n4    77.0\n5    87.0\nName: score, dtype: float64\n```\n\n### 缺失值指示变量\n\nPanda DataFrame 对象可以直接调用方法isnull 产生缺失值指示变量，例如产生score变量的缺失值变量：\n\n```\n# 指示变量\nsample.score.isnull()\nOut[95]: \n0    False\n1    False\n2    False\n3    False\n4    False\n5     True\nName: score, dtype: bool\n```\n\n若想转化为数据0，1型指示变量，可以使用apply方法，int表示将该列替换为 int 类型：\n\n```\nsample.score.isnull().apply(int)\nOut[97]: \n0    0\n1    0\n2    0\n3    0\n4    0\n5    1\n```\n\n\n\n\n\n## 噪声处理\n\n噪声值是指数据中有一个或多个数据与其他数值相比差异性比较大，又称异常值，离群值。\n\n对于单变量，我们可以采用盖帽法，分箱法；\n\n对于多变量，我们可以采用就聚类。\n\n### 盖帽法\n\n盖帽法将连续变量均值上下三倍标准差范围外的记录替换为均值上下三倍标准差值。\n\n```\n\ndef cap(df,quantile=[0.01,0.99]):\n    \"\"\"\n    盖帽法处理异常值\n    :param df: pd.Series 列，连续变量\n    :param quantile: 指定盖帽法的上下分位数范围\n    :return: \n    \"\"\"\n    Q01 ,Q99 = df.quantile(quantile).values.tolist() # 生成分位数\n    # 替代异常值\n    if Q01 > df.min():\n        x = df.copy()\n        x.loc[x<Q01] = Q01\n    if Q99 < df.max():\n        x = df.copy()\n        x.loc[x > Q99] = Q99\n    return(x)\n```\n\n生成一组服从正态分布的随机数，sample.hsit 为直方图。\n\n```\nimport matplotlib.pyplot as plt\nsample = pd.DataFrame({'normall':np.random.randn(1000)})\nsample.hist(bins =50)\nplt.show()\n```\n\n![处理前](\\image\\before_handle.png)\n\n\n\n对 sample 数据所有列进行盖帽法转换，，下图可以看出盖帽后极端值频数的变化。\n\n```\nnew =sample.apply(cap,quantile=[0.001,0.99])\nnew.hist(bins=50)\nplt.show()\n```\n\n![](\\image\\after_handle.png)\n\n\n\n\n\n### 分箱法\n\n分箱法通过考察数据的\"近邻\"来光滑有序数据的值。有序值分布到一些桶或箱中。\n\n深分箱，即每个分箱中的样本量一致；\n\n等宽分箱，即每个分箱中的取值范围一致。直方图就是首先对数据进行了等宽分箱，再计算频数画图。\n\n分箱法可以将异常数据包含再箱子中，在进行建模的时候，不直接进行到模型中，因而可以达到处理异常值的目的。\n\n```\n# 生成10个标准正态分布的随机数\nsample = pd.DataFrame({\"normal\":np.random.randn(10)})\nsample\nOut[118]: \n     normal\n0 -0.028929\n1  0.327508\n2 -0.596384\n3 -2.036334\n4  1.452605\n5 -0.403936\n6  0.315138\n7  0.252127\n8 -0.775113\n9  0.171641\n```\n\n#### 等宽分箱\n\n现将sample 按照宽度分位5份，下限中，cut 函数自动选择小于列最小值的一个数值未下限，最大值为上限，等分为5份。\n\n```\npd.cut(sample.normal,5)\nOut[119]: \n0     (-0.641, 0.057]\n1      (0.057, 0.755]\n2     (-0.641, 0.057]\n3     (-2.04, -1.339]\n4      (0.755, 1.453]\n5     (-0.641, 0.057]\n6      (0.057, 0.755]\n7      (0.057, 0.755]\n8    (-1.339, -0.641]\n9      (0.057, 0.755]\nName: normal, dtype: category\nCategories (5, interval[float64]): [(-2.04, -1.339] < (-1.339, -0.641] < (-0.641, 0.057] <\n                                    (0.057, 0.755] < (0.755, 1.453]]\n\n```\n\n使用labels参数指定分箱后的各个水平的标签，\n\n```\npd.cut(sample.normal,bins=5,labels=[1,2,3,4,5])\nOut[120]: \n0    3\n1    4\n2    3\n3    1\n4    5\n5    3\n6    4\n7    4\n8    2\n9    4\nName: normal, dtype: category\nCategories (5, int64): [1 < 2 < 3 < 4 < 5]\n```\n\n### 等深分箱\n\n等深分箱中，各个箱的宽度可能不一，但频数是几乎相等，所以可以采用数据的分位数来分箱。现对sample数据进行等深度分二箱，首先要找到2箱的分位数：\n\n```\nsample.normal.quantile([0,0.5,1])\nOut[121]: \n0.0   -2.036334\n0.5    0.071356\n1.0    1.452605\nName: normal, dtype: float64\n```\n\n在bins参数中设定分位数区间，为将下边界包含，include_lowest= True，完成等深分箱：\n\n```\npd.cut(sample.normal,bins=sample.normal.quantile([0,0.5,1]),include_lowest=True)\nOut[124]: \n0    (-2.037, 0.0714]\n1     (0.0714, 1.453]\n2    (-2.037, 0.0714]\n3    (-2.037, 0.0714]\n4     (0.0714, 1.453]\n5    (-2.037, 0.0714]\n6     (0.0714, 1.453]\n7     (0.0714, 1.453]\n8    (-2.037, 0.0714]\n9     (0.0714, 1.453]\nName: normal, dtype: category\nCategories (2, interval[float64]): [(-2.037, 0.0714] < (0.0714, 1.453]]\n```\n\n可以对分组进行标签化。\n\n```\npd.cut(sample.normal,bins=sample.normal.quantile([0,0.5,1]),include_lowest=True,labels=['bad','good'])\nOut[125]: \n0     bad\n1    good\n2     bad\n3     bad\n4    good\n5     bad\n6    good\n7    good\n8     bad\n9    good\nName: normal, dtype: category\nCategories (2, object): [bad < good]\n\n```\n\n\n\n\n\n### 多变量异常值处理-聚类法\n\n通过快速聚类法将数据对象分组成为多个簇，在同一个簇中的对象具有较高的相似度，而不同簇之间的对象差别较大。聚类分析可以挖掘孤立点以发现噪声数据，因为噪声本身就是孤立点。\n","tags":["数据清洗"],"categories":["数据挖掘"]},{"title":"RFM模型分析用户行为","url":"/2018/10/17/RFM模型分析用户行为/","content":"# RFM模型分析用户行为\n\n根据美国数据库营销研究所Arthur Hughes的研究，客户数据库中有三个神奇的要素，这三个要素构成了数据分析最好的指标：最近一次消费(Recency)、消费频率(Frequency)、消费金额(Monetary)。\n\nRFM模型：\n\n- R(Recency)表示客户最近一次购买的时间有多远，对消费时间越近的客户，提供即时的商品或服务也最有可能有所反应。\n\n- F(Frequency)表示客户在最近一段时间内购买的次数，经常买的客户也是满意度最高的客户。\n\n- M  (Monetary)表示客户在最近一段时间内购买的金额，消费金额是最近消费的平均金额，是体现客户短期价值的中重要变量。如果预算不多，那么我们酒的将服务信息提供给收入贡献较高的那些人。\n\n一般原始数据为3个字段：客户ID、购买时间（日期格式）、购买金额，用数据挖掘软件处理，加权（考虑权重）得到RFM得分，进而可以进行客户细分，客户等级分类，Customer Level Value得分排序等，实现数据库营销！\n\n![](\\image\\RFM.png)\n\n\n\n（编号次序RFM,1代表高，0代表低）\n\n重要价值客户（111）：最近消费时间近、消费频次和消费金额都很高，必须是VIP啊！\n\n重要保持客户（011）：最近消费时间较远，但消费频次和金额都很高，说明这是个一段时间没来的忠实客户，我们需要主动和他保持联系。\n\n重要发展客户（101）：最近消费时间较近、消费金额高，但频次不高，忠诚度不高，很有潜力的用户，必须重点发展。\n\n重要挽留客户（001）：最近消费时间较远、消费频次不高，但消费金额高的用户，可能是将要流失或者已经要流失的用户，应当基于挽留措施。\n\nRFM模型的应用在于建立一个用户行为报告，这个报告会成为维系顾客的一个重要指标。\n\n\n\n现在我们以某淘宝店家做客户激活为案例，[RFM_TRAD_FLOW.csv](/data/RFM_TRAD_FLOW.csv) 为某段时间内客户消费记录\n\n```\nimport pandas as pd\ntrad_flow = pd.read_csv('data/RFM_TRAD_FLOW.csv', encoding='gbk')\ntrad_flow.head(10)\n```\n\n数据部分展示：\n\n| transID | cumid | time | amount | type_label | type |\n| ------- | ----- | ---- | ------ | ---------- | ---- |\n|9407|\t10001|\t14JUN09:17:58:34|\t199\t|正常|\tNormal|\n|9625\t|10001\t|16JUN09:15:09:13\t|369\t|正常\t|Normal|\n|11837\t|10001|\t01JUL09:14:50:36\t|369\t|正常|\tNormal|\n|26629\t|10001\t|14DEC09:18:05:32\t|359\t|正常\t|Normal|\n|30850|\t10001\t|12APR10:13:02:20|\t399|\t正常|\tNormal|\n|32007\t|10001|\t04MAY10:16:45:58|\t269\t|正常|\tNormal|\n|36637\t|10001\t|04JUN10:20:03:06|\t0\t|赠送|\tPresented|\n|43108\t|10001|\t06JUL10:16:56:40|\t381\t|正常|\tNormal|\n\n1. 计算F 反应顾客对打折的偏好程度\n\n通过计算F反应客户对打折产品的偏好\n\n```\nF=trad_flow.groupby(['cumid','type'])[['transID']].count()\nF.head()\n```\n\n建立数据透视表\n\n```\nF_trans=pd.pivot_table(F,index='cumid',columns='type',values='transID')\nF_trans.head()\n```\n\n对缺失的数据填补为零\n\n```\nF_trans['Special_offer']= F_trans['Special_offer'].fillna(0)\nF_trans.head()\n```\n\n计算兴趣用户比例\n\n```\nF_trans[\"interest\"]=F_trans['Special_offer']/(F_trans['Special_offer']+F_trans['Normal'])\nF_trans.head()\n```\n\n2. 计算M反应客户的价值信息\n\n通过计算M反应客户的价值信息\n\n```\nM=trad_flow.groupby(['cumid','type'])[['amount']].sum()\nM.head()\n```\n\n数据透视，缺失值补零，计算价值用户\n\n```\nM_trans=pd.pivot_table(M,index='cumid',columns='type',values='amount')\nM_trans['Special_offer']= M_trans['Special_offer'].fillna(0)\nM_trans['returned_goods']= M_trans['returned_goods'].fillna(0)\nM_trans[\"value\"]=M_trans['Normal']+M_trans['Special_offer']+M_trans['returned_goods']\nM_trans.head()\n```\n\n3. 通过计算R反应客户是否为沉默客户\n\n定义一个从文本转化为时间的函数\n\n```\nfrom datetime import datetime\nimport time\ndef to_time(t):\n    out_t=time.mktime(time.strptime(t, '%d%b%y:%H:%M:%S'))  \n    return out_t\t\t\n```\n\n将时间进行转化\n\n```\nrad_flow[\"time_new\"]= trad_flow.time.apply(to_time)\n```\n\n获取高频消费客户\n\n```\nR=trad_flow.groupby(['cumid'])[['time_new']].max()\nR.head()\n```\n\n4. 构建模型，筛选目标客户\n\n```\n# In[12]\nfrom sklearn import preprocessing\nthreshold = pd.qcut(F_trans['interest'], 2, retbins=True)[1][1]\nbinarizer = preprocessing.Binarizer(threshold=threshold)\ninterest_q = pd.DataFrame(binarizer.transform(F_trans['interest'].values.reshape(-1, 1)))\ninterest_q.index=F_trans.index\ninterest_q.columns=[\"interest\"]\n# In[12]\nthreshold = pd.qcut(M_trans['value'], 2, retbins=True)[1][1]\nbinarizer = preprocessing.Binarizer(threshold=threshold)\nvalue_q = pd.DataFrame(binarizer.transform(M_trans['value'].values.reshape(-1, 1)))\nvalue_q.index=M_trans.index\nvalue_q.columns=[\"value\"]\n# In[12]\nthreshold = pd.qcut(R[\"time_new\"], 2, retbins=True)[1][1]\nbinarizer = preprocessing.Binarizer(threshold=threshold)\ntime_new_q = pd.DataFrame(binarizer.transform(R[\"time_new\"].values.reshape(-1, 1)))\ntime_new_q.index=R.index\ntime_new_q.columns=[\"time\"]\n# In[12]\nanalysis=pd.concat([interest_q, value_q,time_new_q], axis=1)\n# In[12]\n#analysis['rank']=analysis.interest_q+analysis.interest_q\nanalysis = analysis[['interest','value','time']]\nanalysis.head()\n\nlabel = {\n    (0,0,0):'无兴趣-低价值-沉默',\n    (1,0,0):'有兴趣-低价值-沉默',\n    (1,0,1):'有兴趣-低价值-活跃',\n    (0,0,1):'无兴趣-低价值-活跃',\n    (0,1,0):'无兴趣-高价值-沉默',\n    (1,1,0):'有兴趣-高价值-沉默',\n    (1,1,1):'有兴趣-高价值-活跃',\n    (0,1,1):'无兴趣-高价值-活跃'\n}\nanalysis['label'] = analysis[['interest','value','time']].apply(lambda x: label[(x[0],x[1],x[2])], axis = 1)\nanalysis.head()\n```\n\n\n"},{"title":"markdown_picture","url":"/2018/10/16/markdown-picture/","content":"# 序列图\n\n``` sequence\ntitle: 序列图sequence(示例)\nparticipant A\nparticipant B\nparticipant C\n\nnote left of A: A左侧说明\nnote over B: 覆盖B的说明\nnote right of C: C右侧说明\n\nA->A:自己到自己\nA->B:实线实箭头\nA-->C:虚线实箭头\nB->>C:实线虚箭头\nB-->>A:虚线虚箭头\t\t\t\t\t\n```\n\n关键词:\n\n1. title, 定义该序列图的标题\n2. participant, 定义时序图中的对象\n3. note, 定义对时序图中的部分说明\n4. {actor}, 表示时序图中的具体对象（名称自定义）\n\n针对note的方位控制主要包含以下几种关键词：\n\n1. left of, 表示当前对象的左侧\n2. right of, 表示当前对象的右侧\n3. over, 表示覆盖在当前对象（们）的上面\n\n针对{actor}的箭头分为以下几种：\n\n1. -> 表示实线实箭头\n2. –> 表示虚线实箭头\n3. ->> 表示实线虚箭头\n4. –>> 表示虚线虚箭头\n\n# 流程图\n\n```flow\nst=>start: 开始\ne=>end: 结束\nop=>operation: 操作\nsub=>subroutine: 子程序\ncond=>condition: 是或者不是?\nio=>inputoutput: 输出\n\nst(right)->op->cond\ncond(yes)->io(right)->e\ncond(no)->sub(right)->op\n```\n\n- start,end, 表示程序的开始与结束\n- operation, 表示程序的处理块\n- subroutine, 表示子程序块\n- condition, 表示程序的条件判断\n- inputoutput, 表示程序的出入输出\n- right,left, 表示箭头在当前模块上的起点(默认箭头从下端开始)\n- yes,no, 表示condition判断的分支(其可以和right,left同时使用)\n\n模块定义(模块标识与模块名称可以任意定义名称,关键词不可随意取名)如下:\n\n```\n模块标识=>模块关键词: 模块名称\n```\n\n连接定义如下:\n\n```\n模块标识1->模块标识2\n模块标识1->模块标识2->模块标识3\n... ...\n```\n\n```flow\nst=>start: Start\ne=>end: End\nop1=>operation: My Operation\nop2=>operation: Stuff\nsub1=>subroutine: My Subroutine\ncond=>condition: Yes\nor No?\nc2=>condition: Good idea\nio=>inputoutput: catch something...\n\nst->op1(right)->cond\ncond(yes, right)->c2\ncond(no)->sub1(left)->op1\nc2(yes)->io->e\nc2(no)->op2->e\n```\n\n![Alt text](https://g.gravizo.com/svg?\n  digraph G {\n​    aize =\"4,4\";\n​    main [shape=box];\n​    main -> parse [weight=8];\n​    parse -> execute;\n​    main -> init [style=dotted];\n​    main -> cleanup;\n​    execute -> { make_string; printf}\n​    init -> make_string;\n​    edge [color=red];\n​    main -> printf [style=bold,label=\"100 times\"];\n​    make_string [label=\"make a string\"];\n​    node [shape=box,style=filled,color=\".7 .3 1.0\"];\n​    execute -> compare;\n  }\n)\n","tags":["markdown"]},{"title":"data_integration","url":"/2018/10/16/data-integration/","content":"# 数据整合\n\n## 行列操作\n\n```python\nimport pandas as pd\nimport numpy as np\nsample = pd.DataFrame(np.random.randn(4,5),columns=['a','b','c','d','e'])\nsample\n```\n\n```\nOut[2]: \n          a         b         c         d         e\n0 -0.776807  2.355071 -0.940921  0.164487 -1.025772\n1  0.596704  0.962625  1.848441 -1.122676 -0.359290\n2 -0.092755 -0.124250 -0.259899 -0.111997 -1.816197\n3  0.372941  0.297850 -0.409256  0.485376 -2.790929\n```\n\n### 选择单列\n\n```\nsample['a']\n```\n\n\n\n```\nOut[4]: \n0   -0.776807\n1    0.596704\n2   -0.092755\n3    0.372941\nName: a, dtype: float64\n```\n\n数据框的ix，iloc，ioc方法都可以选择行，列，iloc方法只能使用数值作为索引来选择行列，loc方法在选择时能使用字符串索引，ix方法则可以使用两种索引\n\n```\nsample.ix[:,'a']\n```\n\n```\nOut[5]: \n0   -0.776807\n1    0.596704\n2   -0.092755\n3    0.372941\nName: a, dtype: float64\n```\n\n或者单选列\n\n```\nsample[['a']]\nOut[6]: \n          a\n0 -0.776807\n1  0.596704\n2 -0.092755\n3  0.372941\n```\n\n\n\n### 选择多行多列\n```\nsample.ix[0:2,0:2]\n```\n\n```\nOut[7]: \n          a         b\n0 -0.776807  2.355071\n1  0.596704  0.962625\n2 -0.092755 -0.124250\n\n```\n\n```\nsample.iloc[0:2,0:2]\n\n```\n\n### 创建，删除列\n\n第一种方式\n\n```\nsample['new_col1']=sample['a']-sample['b']\nsample\n```\n\n```\nOut[10]: \n          a         b         c         d         e  new_col1\n0 -0.776807  2.355071 -0.940921  0.164487 -1.025772 -3.131877\n1  0.596704  0.962625  1.848441 -1.122676 -0.359290 -0.365921\n2 -0.092755 -0.124250 -0.259899 -0.111997 -1.816197  0.031495\n3  0.372941  0.297850 -0.409256  0.485376 -2.790929  0.075091\n```\n\n第二种方式\n\n```\nsample.assign(new_col2=sample['a']-sample['b'],new_col3=sample['a']+sample['b'])\n```\n\n```\nOut[11]: \n          a         b         c    ...     new_col1  new_col2  new_col3\n0 -0.776807  2.355071 -0.940921    ...    -3.131877 -3.131877  1.578264\n1  0.596704  0.962625  1.848441    ...    -0.365921 -0.365921  1.559329\n2 -0.092755 -0.124250 -0.259899    ...     0.031495  0.031495 -0.217004\n3  0.372941  0.297850 -0.409256    ...     0.075091  0.075091  0.670792\n[4 rows x 8 columns]\n```\n\n删除列，第一种方式\n\n```\nsample.drop('a',axis=1)\n```\n\n```\nOut[12]: \n          b         c         d         e  new_col1\n0  2.355071 -0.940921  0.164487 -1.025772 -3.131877\n1  0.962625  1.848441 -1.122676 -0.359290 -0.365921\n2 -0.124250 -0.259899 -0.111997 -1.816197  0.031495\n3  0.297850 -0.409256  0.485376 -2.790929  0.075091\n```\n\n第二种方式，\n\n```\n# In\nsample.drop(['a','b'],axis=1)\n```\n\n```\n      c         d         e  new_col1\n\n0 -0.940921  0.164487 -1.025772 -3.131877\n1  1.848441 -1.122676 -0.359290 -0.365921\n```\n\n## 条件查询\n\n### 生成示例数据\n\n```\n# In[]\nsample = pd.DataFrame({'name':['Bob','Lindy','Mark',\"Miki\",'Sully','Rose'],\n                       'score':[98,78,88,77,69,69],\n                       'group':[1,1,1,2,1,2]})\nsample\n```\n\n```\nOut[14]: \n    name  score  group\n0    Bob     98      1\n1  Lindy     78      1\n2   Mark     88      1\n3   Miki     77      2\n4  Sully     69      1\n5   Rose     69      2\n```\n\n### 单条件查询\n\n涉及单条件查询时，一般会使用比较运算符，产生布尔类型的索引可用于条件查询。\n\n```\nsample.score >66\n```\n\n\n\n```\nOut[15]: \n0    True\n1    True\n2    True\n3    True\n4    True\n5    True\n```\n\n再通过指定的索引进行条件查询，返回bool值为True的数据：\n\n```\nsample[sample.score >66]\n\nOut[16]: \n    name  score  group\n0    Bob     98      1\n1  Lindy     78      1\n2   Mark     88      1\n3   Miki     77      2\n4  Sully     69      1\n5   Rose     69      2\n```\n\n### 多条件查询\n```\nsample[(sample.score >66) & (sample.group==1)]\nOut[17]: \n    name  score  group\n0    Bob     98      1\n1  Lindy     78      1\n2   Mark     88      1\n4  Sully     69      1\n```\n\n### 使用 qurey\n\n```\nsample.query('score > 90')\nOut[20]: \n  name  score  group\n0  Bob     98      1\n```\n\n其他查询\n\n查询sample中70到80之间的记录，并且将边界包含进来（inclusive=True）\n\n```\n# In[]\nsample[sample['score'].between(70,80,inclusive=True)]\nOut[21]: \n    name  score  group\n1  Lindy     78      1\n3   Miki     77      2\n```\n\n对于字符串列来说，可以使用isin 方法进行查询：\n\n```\nsample[sample['name'].isin(['Bob','Lindy'])]\nOut[24]: \n    name  score  group\n0    Bob     98      1\n1  Lindy     78      1\n```\n\n使用正则表达式匹配进行查询，例如查询姓名以M开头的人的所有记录：\n\n```\nsample[sample['name'].str.contains('[M]+')]\nOut[26]: \n   name  score  group\n2  Mark     88      1\n3  Miki     77      2\n```\n\n## 横向连接\n\nPandas Data Frame 提供 merge 方法以完成各种表格的横向连接操作，这种连接操作跟SQL语句的连接操作类似。\n\n```\ndf1 =pd.DataFrame({'id':[1,2,3],\n                   'col1':['a','b','c']})\ndf2 = pd.DataFrame({'id':[4,3],\n                    'col2':['d','e']})\ndf1\nOut[29]: \n   id col1\n0   1    a\n1   2    b\n2   3    c\ndf2\nOut[30]: \n   id col2\n0   4    d\n1   3    e\n```\n\n内连接使用merge函数示例，根据公共字段保留两表的共有信息，`how = 'innner'`参数表示使用内连接，`on`表示两表的公共字段，若公共字段再两表名称不一致时，可以通过 `left_on`和`right_on`指定：\n\n```\ndf1.merge(df2,how='inner',on='id')\nOut[32]: \n   id col1 col2\n0   3    c    e\ndf1.merge(df2,how='inner',left_on='id',right_on='id')\nOut[33]: \n   id col1 col2\n0   3    c    e\n```\n\n### 外连接\n\n外连接包括左连接，全连接，右连接\n\n#### 左连接\n\n左连接通过公共字段，保留坐标的全部信息，右表在左表缺失的信息会以NaN补全：\n\n```\ndf1.merge(df2,how='left',on='id')\nOut[34]: \n   id col1 col2\n0   1    a  NaN\n1   2    b  NaN\n2   3    c    e\n```\n\n#### 右连接\n\n右连接与左连接相对，右连接通过公共字段，保留右表的全部信息，左表在右表缺失的信息会以 NaN 补全。\n\n```\ndf1.merge(df2,how='right',on='id')\nOut[35]: \n   id col1 col2\n0   3    c    e\n1   4  NaN    d\n```\n\n#### 全连接\n\n全连接通过公共字段，保留右表的全部信息，两表相互缺失的信息会以 NaN 补全。\n\n\n\n```\ndf1.merge(df2,how='outer',on='id')\nOut[36]: \n   id col1 col2\n0   1    a  NaN\n1   2    b  NaN\n2   3    c    e\n3   4  NaN    d\n```\n\n## 行索引连接\n\n`pd.concat`可以完成横向和纵向的合并，这通过 ’axis=‘ 来控制，当参数axis= 1时表示进行横向合并。\n\n```\ndf1 = pd.DataFrame({'id':[1,2,3],\n                    'col1':['a','b','c']},\n                   index=[1,2,3])\ndf2 =pd.DataFrame({'id':[1,2,3],\n                   'col2':['aa','bb','cc']},\n                  index=[1,3,2])\npd.concat([df1,df2],axis=1)\nOut[37]: \n   id col1  id col2\n1   1    a   1   aa\n2   2    b   3   cc\n3   3    c   2   bb\n\n```\n\n## 纵向合并\n\n当参数 axis = 0 时，表示纵向合并。ignore_index= True 表示忽略df1 和 df2 的原先的行索引，合并后重新排列索引。\n\n```\npd.concat([df1,df2],ignore_index=True,axis=0)\nOut[43]: \n  col1 col2  id\n0    a  NaN   1\n1    b  NaN   2\n2    c  NaN   3\n3  NaN   aa   1\n4  NaN   bb   2\n5  NaN   cc   3\n```\n\n去除重复行\n\n```\npd.concat([df1,df2],ignore_index=True,axis=0).drop_duplicates()\n\nOut[44]: \n  col1 col2  id\n0    a  NaN   1\n1    b  NaN   2\n2    c  NaN   3\n3  NaN   aa   1\n4  NaN   bb   2\n5  NaN   cc   3\n```\n\n\n\n\n\n## 排序\n\n按照学生成绩降序排列数据，第一个参数表示排序的依据，ascending = False 代表降序排列，na_position='last'表示缺失值数据排列在数据的最后位置。\n\n```\nsample = pd.DataFrame({'name':['Bob','Lindy','Mark',\"Miki\",'Sully','Rose'],\n                       'score':[98,78,88,77,69,np.nan],\n                       'group':[1,1,1,2,1,2]})\nsample\n###\nsample.sort_values('score',ascending= False,na_position='last')\nOut[46]: \n    name  score  group\n0    Bob   98.0      1\n2   Mark   88.0      1\n1  Lindy   78.0      1\n3   Miki   77.0      2\n4  Sully   69.0      1\n5   Rose    NaN      2\n```\n\n## 分组汇总\n\n数据准备\n\n```\nsample = pd.read_csv('./sample.csv',encoding='utf-8')\nsample\nOut[58]: \n   chinese   class  grade  math   name\n0        88      1      1  98.0    Bob\n1        78      1      1  78.0  Lindy\n2        68      1      1  78.0   Miki\n3        56      2      2  77.0   Mark\n4        77      1      2  77.0  Sully\n5        56      2      2   NaN   Rose\n\n```\n\n分组汇总操作中，会涉及分组变量，度量变量和汇总统计量。pandas 提供了 groupby 方法进行分组汇总。\n\n在sample数据中，grade为分组变量，math 为度量变量，现需要查询grade 为1，2中数学成绩最高。\n\n### 分组变量\n\n在进行分组汇总时，分组变量可以有多个。\n\n```\nsample.groupby(['grade','class'])['math'].max()\nOut[65]: \ngrade  class\n1      1        98.0\n2      1        77.0\n       2        77.0\nName: math, dtype: float64\n```\n\n\n\n### 汇总变量\n\n在进行分组汇总时，汇总变量也可以多个。\n\n```\nsample.groupby('grade',)['math','chinese'].mean()\nOut[75]: \n            math  chinese\ngrade                    \n1      84.666667       78\n2      77.000000       63\n```\n\n### 汇总统计量\n\n| 方法   | 解释   | 方法     | 解释         |\n| ------ | ------ | -------- | ------------ |\n| mean   | 均值   | mad      | 平均绝对偏差 |\n| max    | 最大值 | count    | 计数         |\n| min    | 最小值 | skew     | 偏度         |\n| median | 中位数 | quantile | 指定分位数   |\n| std    | 标准差 |          |              |\n\n以上统计量方法可以直接接 groupby 对象使用，agg方法提供了一次汇总多个统计量的方法，例如，汇总各个班级的数学成绩的均值，最大值，最小值。\n\n```\nsample.groupby('class')['math'].agg(['mean','min','max'])\nOut[78]: \n        mean   min   max\nclass                   \n1      82.75  77.0  98.0\n2      77.00  77.0  77.0\n```\n\n\n\n\n\n### 多重索引\n\n\n\n以年级，班级对学生的数学，语文成绩进行分组汇总，汇总统计量为均值。此时df中有两个行索引和两个列索引。\n\n```\ndf=sample.groupby(['class','grade'])['math','chinese'].agg(['mean','min','max'])\nOut[80]: \n                  math             chinese        \n                  mean   min   max    mean min max\nclass grade                                       \n1     1      84.666667  78.0  98.0      78  68  88\n      2      77.000000  77.0  77.0      77  77  77\n2     2      77.000000  77.0  77.0      56  56  56\n```\n\n查询各个年级、班级的数学成绩的最小值。\n\n```\ndf['math']['min']\nOut[84]: \nclass  grade\n1      1        78.0\n       2        77.0\n2      2        77.0\nName: min, dtype: float64\n```\n\n\n","tags":["数据整合"],"categories":["数据挖掘"]},{"title":"data_mining","url":"/2018/10/16/data-mining/","content":"[TOC]\n\n# Python 常用数据分析框架\n\n| 名称       |             解释             |\n| ---------- | :--------------------------: |\n| Numpy      |  数组，矩阵的存储，运算框架  |\n| Scipy      | 提供统计，线性代数等计算框架 |\n| Pandas     |  结构化数据的整合，处理框架  |\n| Statsmodel |    常见的统计分析框架模型    |\n| Matplotlib |        数据可视化框架        |\n\n\n\n# python 基础数据类型\n\n| 名称    | 解释   | 示例        |\n| ------- | ------ | ----------- |\n| str     | 字符串 | 'a', '2'    |\n| float   | 浮点数 | 1.23， 3.45 |\n| int     | 整数   | 3，5        |\n| bool    | 布尔   | Ture, False |\n| complex | 复数   | 1+2j, 2 +0j |\n\n\n\n# Python 数据格式转换\n\n| 数据类型 | 转换函数  |\n| -------- | --------- |\n| Str      | str()     |\n| Float    | float()   |\n| Int      | Int()     |\n| Bool     | bool()    |\n| Complex  | complex() |\n\n# 读取数据\n\n```\nimport pandas as pd\ncsv =  pd.read_csv('filename')\n```\n\n# 制图步骤\n\n在进行描述性图展示时，制图分为以下四步：\n\n1. 整理原始数据：对初始数据进行预处理和清洗，已达到制图得要求。\n2. 明确表达的信息： 根据初始可用数据，明确分析要表达的信息。\n3. 确定比较的类型： 明确要表达的信息中对目标比较的类型。\n4. 选择图表类型： 选择合适的图表类型，进行绘制并展示。\n\n```flow\n\na=>operation: 数据\nb=>operation: 信息\nc=>operation: 相对关系\nd=>operation: 图形\n\na(right)->b(right)->c(right)->d\n```\n\n\n","tags":["数据分析"]},{"title":"UML","url":"/2018/10/16/UML/","content":"\n\n\n# 制图步骤\n\n在进行描述性图展示时，制图分为以下四步：\n\n1. 整理原始数据：对初始数据进行预处理和清洗，已达到制图得要求。\n2. 明确表达的信息： 根据初始可用数据，明确分析要表达的信息。\n3. 确定比较的类型： 明确要表达的信息中对目标比较的类型。\n4. 选择图表类型： 选择合适的图表类型，进行绘制并展示。\n\n``` flow\n\na=>operation: 数据\nb=>operation: 信息\nc=>operation: 相对关系\nd=>operation: 图形\n\na(right)->b(right)->c(right)->d\n```\n\n\n\n\n(refer link)[http://www.gravizo.com/#howto]\n","tags":["markdown"]},{"title":"es-python-client","url":"/2018/10/12/es-python-client/","content":"# python 操作 Elasticsearch\n\n## 安装 Elasticsearch 模块\n`pip install elasticsearch`\n\n## 添加数据\n``` \nfrom elasticsearch import Elasticsearch\n\n# 默认host为localhost,port为9200.但也可以指定host与port\nes = Elasticsearch([{'host': '192.168.10.21', 'port': 9200}])\ndoc = {\n     'author': 'kimchy',\n    'text': 'Elasticsearch: cool. bonsai cool.',\n     'timestamp': localtime(),\n      }\nres = es.index(index=\"test-index\", doc_type='tweet', id=1, body=doc)\nprint(res)\n```\n如果创建成功会返回以下结果\n``` \n{'_index': 'test-index', '_type': 'tweet', '_id': '1', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 2, 'failed': 0}, '_seq_no': 0, '_primary_term': 1}\n\n```\n## 创建索引\n```\nes.indices.create(index='irisaa') \n```\n## 删除索引\n``` \nes.indices.create(index='irisaa')\n```\n## 查看集群状态\n```\nes.cluster.health(wait_for_status='yellow', request_timeout=1)\n```\n## 查询数据\n### 按照 id 来查询数据\n``` \nres = es.get(index=\"test-index\", doc_type='tweet', id=1)\nprint(res['_source'])\n```\n操作成功后返回如下结果：\n```\n{'author': 'kimchy', 'text': 'Elasticsearch: cool. bonsai cool.', 'timestamp': [2018, 10, 12, 14, 9, 44, 4, 285, 0]}\n\n```\n### 按照DSL语句查询\n```   \nres = es.search(index=\"test-index\", body={\"query\": {\"match_all\": {}}})\nprint(res)\n```\n操作成功后结果，返回如下结果：\n```\n{'took': 2, 'timed_out': False, '_shards': {'total': 5, 'successful': 5, 'skipped': 0, 'failed': 0}, 'hits': {'total': 1, 'max_score': 1.0, 'hits': [{'_index': 'test-index', '_type': 'tweet', '_id': '1', '_score': 1.0, '_source': {'author': 'kimchy', 'text': 'Elasticsearch: cool. bonsai cool.', 'timestamp': [2018, 10, 12, 14, 9, 44, 4, 285, 0]}}]}}\n```\n\n## 更新数据\n```\nes = Elasticsearch()\ndoc = {\n     'author': 'kimchy',\n    'text': 'ok.',\n     'timestamp': localtime(),\n      }\nresult = es.update(index=\"test-index\", doc_type='tweet', id=1, body=doc)\nprint(result) \n```\n\n\n## 删除数据\n```angular2html\n\nes.delete_by_query(index='twtter',doc_type='_doc',body={\n   \"query\": {\n     \"match\": {\n       \"message\": \"some message\"\n     }\n   }\n })\n```\n\n## 批量化导入es\n```\nfrom elasticsearch import helpers\ndef gendata(index, type, jsons):\n    for json in jsons:\n        yield {\n            \"_index\": index,\n            \"_type\": type,\n            \"_source\": json,\n        }\n        \nhelpers.bulk(es, gendata(index='index-test', jsons))        \n```\n","tags":["ELK"]},{"title":"ELK_docker-compose.md","url":"/2018/10/12/ELK-docker-compose-md/","content":"# elasticsearch 集群部署\n我们使用dockers-compose实现单机多节点的部署\n\n## 安装docker\n在测试或开发环境中 Docker 官方为了简化安装流程，提供了一套便捷的安装脚本，Ubuntu 系统上可以使用这套脚本安装：\n``` \n$ curl -fsSL get.docker.com -o get-docker.sh\n$ sudo sh get-docker.sh --mirror Aliyun\n```\n执行这个命令后，脚本就会自动的将一切准备工作做好，并且把 Docker CE 的 Edge 版本安装在系统中。\n### 启动 Docker CE\n \n``` \n$ sudo systemctl enable docker\n$ sudo systemctl start docker\n```\n\n### 建立 docker 用户组\n默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组。\n\n`$ sudo groupadd docker\n`\n\n将当前用户 加入 ``docker`` 组：\n\n`$ sudo usermod -aG docker $USER`\n\n### 测试dockers 是否安装正确\n```\n$ docker run hello-world\n\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\nca4f61b1923c: Pull complete\nDigest: sha256:be0cd392e45be79ffeffa6b05338b98ebb16c87b255f48e297ec7f98e123905c\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://cloud.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/engine/userguide/\n```\n\n## 安装docker-compopse\nCompose 项目是 Docker 官方的开源项目，负责实现对 Docker 容器集群的快速编排。\n它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）。\n\nCompose 中有两个重要的概念：\n\n- 服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。\n\n- 项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。\n\nCompose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。\n\nCompose 项目由 Python 编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。因此，只要所操作的平台支持 Docker API，就可以在其上利用 Compose 来进行编排管理。\n\n### PIP 安装 dockerpose\n``$ sudo pip install -U docker-compose\n``\n\n### 编写 docker-compose.yml\n编写 docker-compose.yml 文件,输入以下内容：\n```\nversion: '2.2'\nservices:\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.0\n    container_name: elasticsearch\n    environment:\n      - cluster.name=docker-cluster\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n        - esdata1:/usr/share/elasticsearch/data\n    volumes:\n      - /home/ethan/EKL/config:/usr/share/elasticsearch/config\n    ports:\n      - 9200:9200\n      - 9300:9300\n    networks:\n      - esnet\n\n  elasticsearch2:\n    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.0\n    container_name: elasticsearch2\n    environment:\n      - cluster.name=docker-cluster\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n      - \"discovery.zen.ping.unicast.hosts=elasticsearch\"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n      - esdata2:/usr/share/elasticsearch/data\n    networks:\n      - esnet\n\n  elasticsearch3:\n    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.0\n    container_name: elasticsearch3\n    environment:\n      - cluster.name=docker-cluster\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n      - \"discovery.zen.ping.unicast.hosts=elasticsearch\"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n      - esdata3:/usr/share/elasticsearch/data\n    networks:\n      - esnet\n  kibana:\n    image: docker.elastic.co/kibana/kibana:6.4.0\n    container_name: kibana\n    environment:\n      SERVER_NAME: kibana\n      ELASTICSEARCH_URL: http://elasticsearch:9200\n    ports:\n      - 5601:5601\n    networks:\n      - esnet\n\nvolumes:\n  esdata1:\n    driver: local\n  esdata2:\n    driver: local\n  esdata3:\n    driver: local\n\nnetworks:\n  esnet:\n\n```\n## 运行 eslasticsearch-kibana\n```\ndocker-compose up\n\n```\n","tags":["elasticsearch"]},{"title":"Elk初探","url":"/2018/10/10/Elk_platfrom/","content":"> 启动elaticsearch需要java环境，请自己谷歌搭建哈\n\n# elaticsearch\n下载tar包\n```powershell\nwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.4.1.tar.gz\n\n```\n\n解压\n```\ntar -zxvf elasticsearch-6.4.1.tar.gz\n```\n进入elaticsearch 可执行文件目录\n```angular2html\ncd elasticsearch-6.4.0/bin\n```\n启动elaticsearch\n```angular2html\n./elaticseaerch\n```\n如果没有抱任何错误，正常来说应该是可以通过restful API来访问的。\n````\ncurl -X GET \"localhost:9200/_cat/health?v\"\n\n````\n对应的返回结果\n```\nepoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\n1538014388 10:13:08  elasticsearch green           1         1      0   0    0    0        0             0                  -                100.0%\n```\n由此我们可以认为elaticsearch已经启动成功了。\n\n# kibana\n下载tar包\n```angular2html\nhttps://artifacts.elastic.co/downloads/kibana/kibana-6.4.1-linux-x86_64.tar.gz\n```\n解压tar包\n```\ntar -zxvf kibana-6.4.1-linux-x86_64.tar.gz\n```\n\n更改kibana配置文件\n```\nvim  /kibana-6.4.1-linux-x86_64/config/kibana.yml\n```\n添加或解注释以下内容\n```\n server.port: 5601\n server.host: \"localhost\"\n elasticsearch.url: \"http://localhost:9200\"\n\n```\n进入kibana可执行文件目录\n```\ncd kibana-6.4.1-linux-x86_64/bin/\n```\n启动kibana\n\n`./kibana`\n\n配置成功后，用浏览器访问`http://ip:5601`可以看到以下页面\n![](/image/kibana.png)\n\n\n# elaticsearc 基本操作\n## 检查集群状态\n```\ncurl -X GET \"localhost:9200/_cat/health?v\"\n\n```\n获取结果如下显示\n```\nepoch      timestamp cluster     status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\n1538019385 11:36:25  data-mining green           1         1      1   1    0    0        0             0                  -                100.0%\n\n```\n## 列出所有索引\n``` \ncurl -X GET \"localhost:9200/_cat/indices?v\"\n```\n如下图所示，我们只有一个索引\n```\nhealth status index   uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   .kibana eFUOj6qJTQCt4xQdvbv8KA   1   0          1            0        4kb            4kb\n\n```\n\n## 创建一个自定义的索引\n```angular2html\n curl -X PUT \"localhost:9200/customer?pretty\"\n```\n对插入的索引返回结果\n```\n\n{\n  \"acknowledged\" : true,\n  \"shards_acknowledged\" : true,\n  \"index\" : \"customer\"\n}\n\n```\n\n再次查看当前索引\n````angular2html\n curl -X GET \"localhost:9200/_cat/indices?v\"\n````\n可以观察到多了一个名为‘customer’的新索引\n```\nhealth status index    uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   .kibana  eFUOj6qJTQCt4xQdvbv8KA   1   0          1            0        4kb            4kb\nyellow open   customer k50FrrMLScGvwzOvfiF0fg   5   1          0            0       401b           401b\n```\n## 插入一个文档\n```angular2html\ncurl -X PUT \"localhost:9200/customer/_doc/1?pretty\" -H 'Content-Type: application/json' -d'\n{\n  \"name\": \"John Doe\"\n}\n'\n```\n如果操作正常，那么插入成功后会返回以下结果\n``` \n{\n  \"_index\" : \"customer\",\n  \"_type\" : \"_doc\",\n  \"_id\" : \"1\",\n  \"_version\" : 1,\n  \"result\" : \"created\",\n  \"_shards\" : {\n    \"total\" : 2,\n    \"successful\" : 1,\n    \"failed\" : 0\n  },\n  \"_seq_no\" : 0,\n  \"_primary_term\" : 1\n}\n```\n## 按照_id查询索引内的文档\n```\ncurl -X GET \"localhost:9200/customer/_doc/1?pretty\"\n```\n查看返回结果，\"_index\"为当前查询的索引，“_type”为查询的文档类型，“_id”为文档所在的id，\n“_source”为我们要查询的内容\n\n```\n\n{\n  \"_index\" : \"customer\",\n  \"_type\" : \"_doc\",\n  \"_id\" : \"1\",\n  \"_version\" : 1,\n  \"found\" : true,\n  \"_source\" : {\n    \"name\" : \"John Doe\"\n  }\n}\n```\n# elastcisear-head\n为了更方便的地查询es中的数据，我推荐使用es插件elasticsearch-head来快速检索es中的数据。\n\n传送门：[elastcsearch-head](https://github.com/mobz/elasticsearch-head)\n\n如果你使用的版本是在elasticsearch 5之后，还需要对elasticsearch 进行配置\n```\ncat >> elasticsearch-6.4.0/config/elasticsearch.yml << EOF\nhttp.cors.enabled: true\nhttp.cors.allow-origin: \"*\"\nEOF\n```\n\n\n## 安装 \n由于elasticsearch-head 需要nodejs，所以我们需要先安装 nodejs 以及 npm\n## nodejs 安装\n```\n$ sudo apt install nodejs\n$ sudo apt instlal npm\n```\nnodejs 安装完后，我们就可以把elasticsearch-head 下载，进行配置了。\n```\ngit clone git://github.com/mobz/elasticsearch-head.git\ncd elasticsearch-head\nnpm install\nnpm run start\n```\n操作成功后的输出显示\n```\n $ npm run start\n\n> elasticsearch-head@0.0.0 start /home/ethan/ekl/elasticsearch-head\n> grunt server\n\n(node:22696) ExperimentalWarning: The http2 module is an experimental API.\nRunning \"connect:server\" (connect) task\nWaiting forever...\nStarted connect web server on http://localhost:9100\n```\n![](/image/eshead.png)\n\n这样我们最简单的数据搜素平台就搭建成功了。\n","tags":["Elk"]},{"title":"Ethan Lee","url":"/2018/08/18/mygirl/","content":"# say something to mygirl\n\n\n<blockquote class=\"blockquote-center\">我知道到遇见你不容易,错过了会很可惜,我不希望余生都是回忆,我希望余生都是你,我爱你</blockquote>\n\n\n![](http://img02.tooopen.com/images/20160509/tooopen_sy_161967094653.jpg)\n\n[link](https://ethan2lee.github.io)\n","tags":["paper"]}]