[{"title":"markdown_picture","url":"/2018/10/16/markdown-picture/","content":"# 序列图\n\n``` sequence\ntitle: 序列图sequence(示例)\nparticipant A\nparticipant B\nparticipant C\n\nnote left of A: A左侧说明\nnote over B: 覆盖B的说明\nnote right of C: C右侧说明\n\nA->A:自己到自己\nA->B:实线实箭头\nA-->C:虚线实箭头\nB->>C:实线虚箭头\nB-->>A:虚线虚箭头\t\t\t\t\t\n```\n\n关键词:\n\n1. title, 定义该序列图的标题\n2. participant, 定义时序图中的对象\n3. note, 定义对时序图中的部分说明\n4. {actor}, 表示时序图中的具体对象（名称自定义）\n\n针对note的方位控制主要包含以下几种关键词：\n\n1. left of, 表示当前对象的左侧\n2. right of, 表示当前对象的右侧\n3. over, 表示覆盖在当前对象（们）的上面\n\n针对{actor}的箭头分为以下几种：\n\n1. -> 表示实线实箭头\n2. –> 表示虚线实箭头\n3. ->> 表示实线虚箭头\n4. –>> 表示虚线虚箭头\n\n# 流程图\n\n```flow\nst=>start: 开始\ne=>end: 结束\nop=>operation: 操作\nsub=>subroutine: 子程序\ncond=>condition: 是或者不是?\nio=>inputoutput: 输出\n\nst(right)->op->cond\ncond(yes)->io(right)->e\ncond(no)->sub(right)->op\n```\n\n- start,end, 表示程序的开始与结束\n- operation, 表示程序的处理块\n- subroutine, 表示子程序块\n- condition, 表示程序的条件判断\n- inputoutput, 表示程序的出入输出\n- right,left, 表示箭头在当前模块上的起点(默认箭头从下端开始)\n- yes,no, 表示condition判断的分支(其可以和right,left同时使用)\n\n模块定义(模块标识与模块名称可以任意定义名称,关键词不可随意取名)如下:\n\n```\n模块标识=>模块关键词: 模块名称\n```\n\n连接定义如下:\n\n```\n模块标识1->模块标识2\n模块标识1->模块标识2->模块标识3\n... ...\n```\n\n```flow\nst=>start: Start\ne=>end: End\nop1=>operation: My Operation\nop2=>operation: Stuff\nsub1=>subroutine: My Subroutine\ncond=>condition: Yes\nor No?\nc2=>condition: Good idea\nio=>inputoutput: catch something...\n\nst->op1(right)->cond\ncond(yes, right)->c2\ncond(no)->sub1(left)->op1\nc2(yes)->io->e\nc2(no)->op2->e\n```\n\n![Alt text](https://g.gravizo.com/svg?\n  digraph G {\n​    aize =\"4,4\";\n​    main [shape=box];\n​    main -> parse [weight=8];\n​    parse -> execute;\n​    main -> init [style=dotted];\n​    main -> cleanup;\n​    execute -> { make_string; printf}\n​    init -> make_string;\n​    edge [color=red];\n​    main -> printf [style=bold,label=\"100 times\"];\n​    make_string [label=\"make a string\"];\n​    node [shape=box,style=filled,color=\".7 .3 1.0\"];\n​    execute -> compare;\n  }\n)\n","tags":["markdown"]},{"title":"data_integration","url":"/2018/10/16/data-integration/","content":"# 数据整合\n\n## 行列操作\n\n```python\nimport pandas as pd\nimport numpy as np\nsample = pd.DataFrame(np.random.randn(4,5),columns=['a','b','c','d','e'])\nsample\n```\n\n```\nOut[2]: \n          a         b         c         d         e\n0 -0.776807  2.355071 -0.940921  0.164487 -1.025772\n1  0.596704  0.962625  1.848441 -1.122676 -0.359290\n2 -0.092755 -0.124250 -0.259899 -0.111997 -1.816197\n3  0.372941  0.297850 -0.409256  0.485376 -2.790929\n```\n\n### 选择单列\n\n```\nsample['a']\n```\n\n\n\n```\nOut[4]: \n0   -0.776807\n1    0.596704\n2   -0.092755\n3    0.372941\nName: a, dtype: float64\n```\n\n数据框的ix，iloc，ioc方法都可以选择行，列，iloc方法只能使用数值作为索引来选择行列，loc方法在选择时能使用字符串索引，ix方法则可以使用两种索引\n\n```\nsample.ix[:,'a']\n```\n\n```\nOut[5]: \n0   -0.776807\n1    0.596704\n2   -0.092755\n3    0.372941\nName: a, dtype: float64\n```\n\n或者单选列\n\n```\nsample[['a']]\nOut[6]: \n          a\n0 -0.776807\n1  0.596704\n2 -0.092755\n3  0.372941\n```\n\n\n\n### 选择多行多列\n```\nsample.ix[0:2,0:2]\n```\n\n```\nOut[7]: \n          a         b\n0 -0.776807  2.355071\n1  0.596704  0.962625\n2 -0.092755 -0.124250\n\n```\n\n```\nsample.iloc[0:2,0:2]\n\n```\n\n### 创建，删除列\n\n第一种方式\n\n```\nsample['new_col1']=sample['a']-sample['b']\nsample\n```\n\n```\nOut[10]: \n          a         b         c         d         e  new_col1\n0 -0.776807  2.355071 -0.940921  0.164487 -1.025772 -3.131877\n1  0.596704  0.962625  1.848441 -1.122676 -0.359290 -0.365921\n2 -0.092755 -0.124250 -0.259899 -0.111997 -1.816197  0.031495\n3  0.372941  0.297850 -0.409256  0.485376 -2.790929  0.075091\n```\n\n第二种方式\n\n```\nsample.assign(new_col2=sample['a']-sample['b'],new_col3=sample['a']+sample['b'])\n```\n\n```\nOut[11]: \n          a         b         c    ...     new_col1  new_col2  new_col3\n0 -0.776807  2.355071 -0.940921    ...    -3.131877 -3.131877  1.578264\n1  0.596704  0.962625  1.848441    ...    -0.365921 -0.365921  1.559329\n2 -0.092755 -0.124250 -0.259899    ...     0.031495  0.031495 -0.217004\n3  0.372941  0.297850 -0.409256    ...     0.075091  0.075091  0.670792\n[4 rows x 8 columns]\n```\n\n删除列，第一种方式\n\n```\nsample.drop('a',axis=1)\n```\n\n```\nOut[12]: \n          b         c         d         e  new_col1\n0  2.355071 -0.940921  0.164487 -1.025772 -3.131877\n1  0.962625  1.848441 -1.122676 -0.359290 -0.365921\n2 -0.124250 -0.259899 -0.111997 -1.816197  0.031495\n3  0.297850 -0.409256  0.485376 -2.790929  0.075091\n```\n\n第二种方式，\n\n```\n# In\nsample.drop(['a','b'],axis=1)\n```\n\n```\n      c         d         e  new_col1\n\n0 -0.940921  0.164487 -1.025772 -3.131877\n1  1.848441 -1.122676 -0.359290 -0.365921\n```\n\n## 条件查询\n\n### 生成示例数据\n\n```\n# In[]\nsample = pd.DataFrame({'name':['Bob','Lindy','Mark',\"Miki\",'Sully','Rose'],\n                       'score':[98,78,88,77,69,69],\n                       'group':[1,1,1,2,1,2]})\nsample\n```\n\n```\nOut[14]: \n    name  score  group\n0    Bob     98      1\n1  Lindy     78      1\n2   Mark     88      1\n3   Miki     77      2\n4  Sully     69      1\n5   Rose     69      2\n```\n\n### 单条件查询\n\n涉及单条件查询时，一般会使用比较运算符，产生布尔类型的索引可用于条件查询。\n\n```\nsample.score >66\n```\n\n\n\n```\nOut[15]: \n0    True\n1    True\n2    True\n3    True\n4    True\n5    True\n```\n\n再通过指定的索引进行条件查询，返回bool值为True的数据：\n\n```\nsample[sample.score >66]\n\nOut[16]: \n    name  score  group\n0    Bob     98      1\n1  Lindy     78      1\n2   Mark     88      1\n3   Miki     77      2\n4  Sully     69      1\n5   Rose     69      2\n```\n\n### 多条件查询\n```\nsample[(sample.score >66) & (sample.group==1)]\nOut[17]: \n    name  score  group\n0    Bob     98      1\n1  Lindy     78      1\n2   Mark     88      1\n4  Sully     69      1\n```\n\n### 使用 qurey\n\n```\nsample.query('score > 90')\nOut[20]: \n  name  score  group\n0  Bob     98      1\n```\n\n其他查询\n\n查询sample中70到80之间的记录，并且将边界包含进来（inclusive=True）\n\n```\n# In[]\nsample[sample['score'].between(70,80,inclusive=True)]\nOut[21]: \n    name  score  group\n1  Lindy     78      1\n3   Miki     77      2\n```\n\n对于字符串列来说，可以使用isin 方法进行查询：\n\n```\nsample[sample['name'].isin(['Bob','Lindy'])]\nOut[24]: \n    name  score  group\n0    Bob     98      1\n1  Lindy     78      1\n```\n\n使用正则表达式匹配进行查询，例如查询姓名以M开头的人的所有记录：\n\n```\nsample[sample['name'].str.contains('[M]+')]\nOut[26]: \n   name  score  group\n2  Mark     88      1\n3  Miki     77      2\n```\n\n## 横向连接\n\nPandas Data Frame 提供 merge 方法以完成各种表格的横向连接操作，这种连接操作跟SQL语句的连接操作类似。\n\n```\ndf1 =pd.DataFrame({'id':[1,2,3],\n                   'col1':['a','b','c']})\ndf2 = pd.DataFrame({'id':[4,3],\n                    'col2':['d','e']})\ndf1\nOut[29]: \n   id col1\n0   1    a\n1   2    b\n2   3    c\ndf2\nOut[30]: \n   id col2\n0   4    d\n1   3    e\n```\n\n内连接使用merge函数示例，根据公共字段保留两表的共有信息，`how = 'innner'`参数表示使用内连接，`on`表示两表的公共字段，若公共字段再两表名称不一致时，可以通过 `left_on`和`right_on`指定：\n\n```\ndf1.merge(df2,how='inner',on='id')\nOut[32]: \n   id col1 col2\n0   3    c    e\ndf1.merge(df2,how='inner',left_on='id',right_on='id')\nOut[33]: \n   id col1 col2\n0   3    c    e\n```\n\n### 外连接\n\n外连接包括左连接，全连接，右连接\n\n#### 左连接\n\n左连接通过公共字段，保留坐标的全部信息，右表在左表缺失的信息会以NaN补全：\n\n```\ndf1.merge(df2,how='left',on='id')\nOut[34]: \n   id col1 col2\n0   1    a  NaN\n1   2    b  NaN\n2   3    c    e\n```\n\n#### 右连接\n\n右连接与左连接相对，右连接通过公共字段，保留右表的全部信息，左表在右表缺失的信息会以 NaN 补全。\n\n```\ndf1.merge(df2,how='right',on='id')\nOut[35]: \n   id col1 col2\n0   3    c    e\n1   4  NaN    d\n```\n\n#### 全连接\n\n全连接通过公共字段，保留右表的全部信息，两表相互缺失的信息会以 NaN 补全。\n\n\n\n```\ndf1.merge(df2,how='outer',on='id')\nOut[36]: \n   id col1 col2\n0   1    a  NaN\n1   2    b  NaN\n2   3    c    e\n3   4  NaN    d\n```\n\n## 行索引连接\n\n`pd.concat`可以完成横向和纵向的合并，这通过 ’axis=‘ 来控制，当参数axis= 1时表示进行横向合并。\n\n```\ndf1 = pd.DataFrame({'id':[1,2,3],\n                    'col1':['a','b','c']},\n                   index=[1,2,3])\ndf2 =pd.DataFrame({'id':[1,2,3],\n                   'col2':['aa','bb','cc']},\n                  index=[1,3,2])\npd.concat([df1,df2],axis=1)\nOut[37]: \n   id col1  id col2\n1   1    a   1   aa\n2   2    b   3   cc\n3   3    c   2   bb\n\n```\n\n## 纵向合并\n\n当参数 axis = 0 时，表示纵向合并。ignore_index= True 表示忽略df1 和 df2 的原先的行索引，合并后重新排列索引。\n\n```\npd.concat([df1,df2],ignore_index=True,axis=0)\nOut[43]: \n  col1 col2  id\n0    a  NaN   1\n1    b  NaN   2\n2    c  NaN   3\n3  NaN   aa   1\n4  NaN   bb   2\n5  NaN   cc   3\n```\n\n去除重复行\n\n```\npd.concat([df1,df2],ignore_index=True,axis=0).drop_duplicates()\n\nOut[44]: \n  col1 col2  id\n0    a  NaN   1\n1    b  NaN   2\n2    c  NaN   3\n3  NaN   aa   1\n4  NaN   bb   2\n5  NaN   cc   3\n```\n\n\n\n\n\n## 排序\n\n按照学生成绩降序排列数据，第一个参数表示排序的依据，ascending = False 代表降序排列，na_position='last'表示缺失值数据排列在数据的最后位置。\n\n```\nsample = pd.DataFrame({'name':['Bob','Lindy','Mark',\"Miki\",'Sully','Rose'],\n                       'score':[98,78,88,77,69,np.nan],\n                       'group':[1,1,1,2,1,2]})\nsample\n###\nsample.sort_values('score',ascending= False,na_position='last')\nOut[46]: \n    name  score  group\n0    Bob   98.0      1\n2   Mark   88.0      1\n1  Lindy   78.0      1\n3   Miki   77.0      2\n4  Sully   69.0      1\n5   Rose    NaN      2\n```\n\n## 分组汇总\n\n数据准备\n\n```\nsample = pd.read_csv('./sample.csv',encoding='utf-8')\nsample\nOut[58]: \n   chinese   class  grade  math   name\n0        88      1      1  98.0    Bob\n1        78      1      1  78.0  Lindy\n2        68      1      1  78.0   Miki\n3        56      2      2  77.0   Mark\n4        77      1      2  77.0  Sully\n5        56      2      2   NaN   Rose\n\n```\n\n分组汇总操作中，会涉及分组变量，度量变量和汇总统计量。pandas 提供了 groupby 方法进行分组汇总。\n\n在sample数据中，grade为分组变量，math 为度量变量，现需要查询grade 为1，2中数学成绩最高。\n\n### 分组变量\n\n在进行分组汇总时，分组变量可以有多个。\n\n```\nsample.groupby(['grade','class'])['math'].max()\nOut[65]: \ngrade  class\n1      1        98.0\n2      1        77.0\n       2        77.0\nName: math, dtype: float64\n```\n\n\n\n### 汇总变量\n\n在进行分组汇总时，汇总变量也可以多个。\n\n```\nsample.groupby('grade',)['math','chinese'].mean()\nOut[75]: \n            math  chinese\ngrade                    \n1      84.666667       78\n2      77.000000       63\n```\n\n### 汇总统计量\n\n| 方法   | 解释   | 方法     | 解释         |\n| ------ | ------ | -------- | ------------ |\n| mean   | 均值   | mad      | 平均绝对偏差 |\n| max    | 最大值 | count    | 计数         |\n| min    | 最小值 | skew     | 偏度         |\n| median | 中位数 | quantile | 指定分位数   |\n| std    | 标准差 |          |              |\n\n以上统计量方法可以直接接 groupby 对象使用，agg方法提供了一次汇总多个统计量的方法，例如，汇总各个班级的数学成绩的均值，最大值，最小值。\n\n```\nsample.groupby('class')['math'].agg(['mean','min','max'])\nOut[78]: \n        mean   min   max\nclass                   \n1      82.75  77.0  98.0\n2      77.00  77.0  77.0\n```\n\n\n\n\n\n### 多重索引\n\n\n\n以年级，班级对学生的数学，语文成绩进行分组汇总，汇总统计量为均值。此时df中有两个行索引和两个列索引。\n\n```\ndf=sample.groupby(['class','grade'])['math','chinese'].agg(['mean','min','max'])\nOut[80]: \n                  math             chinese        \n                  mean   min   max    mean min max\nclass grade                                       \n1     1      84.666667  78.0  98.0      78  68  88\n      2      77.000000  77.0  77.0      77  77  77\n2     2      77.000000  77.0  77.0      56  56  56\n```\n\n查询各个年级、班级的数学成绩的最小值。\n\n```\ndf['math']['min']\nOut[84]: \nclass  grade\n1      1        78.0\n       2        77.0\n2      2        77.0\nName: min, dtype: float64\n```\n\n\n","tags":["数据分析，数据整合"]},{"title":"data_mining","url":"/2018/10/16/data-mining/","content":"[TOC]\n\n# Python 常用数据分析框架\n\n| 名称       |             解释             |\n| ---------- | :--------------------------: |\n| Numpy      |  数组，矩阵的存储，运算框架  |\n| Scipy      | 提供统计，线性代数等计算框架 |\n| Pandas     |  结构化数据的整合，处理框架  |\n| Statsmodel |    常见的统计分析框架模型    |\n| Matplotlib |        数据可视化框架        |\n\n\n\n# python 基础数据类型\n\n| 名称    | 解释   | 示例        |\n| ------- | ------ | ----------- |\n| str     | 字符串 | 'a', '2'    |\n| float   | 浮点数 | 1.23， 3.45 |\n| int     | 整数   | 3，5        |\n| bool    | 布尔   | Ture, False |\n| complex | 复数   | 1+2j, 2 +0j |\n\n\n\n# Python 数据格式转换\n\n| 数据类型 | 转换函数  |\n| -------- | --------- |\n| Str      | str()     |\n| Float    | float()   |\n| Int      | Int()     |\n| Bool     | bool()    |\n| Complex  | complex() |\n\n# 读取数据\n\n```\nimport pandas as pd\ncsv =  pd.read_csv('filename')\n```\n\n# 制图步骤\n\n在进行描述性图展示时，制图分为以下四步：\n\n1. 整理原始数据：对初始数据进行预处理和清洗，已达到制图得要求。\n2. 明确表达的信息： 根据初始可用数据，明确分析要表达的信息。\n3. 确定比较的类型： 明确要表达的信息中对目标比较的类型。\n4. 选择图表类型： 选择合适的图表类型，进行绘制并展示。\n\n```flow\n\na=>operation: 数据\nb=>operation: 信息\nc=>operation: 相对关系\nd=>operation: 图形\n\na(right)->b(right)->c(right)->d\n```\n\n\n","tags":["数据分析"]},{"title":"UML","url":"/2018/10/16/UML/","content":"\n\n\n# 制图步骤\n\n在进行描述性图展示时，制图分为以下四步：\n\n1. 整理原始数据：对初始数据进行预处理和清洗，已达到制图得要求。\n2. 明确表达的信息： 根据初始可用数据，明确分析要表达的信息。\n3. 确定比较的类型： 明确要表达的信息中对目标比较的类型。\n4. 选择图表类型： 选择合适的图表类型，进行绘制并展示。\n\n``` flow\n\na=>operation: 数据\nb=>operation: 信息\nc=>operation: 相对关系\nd=>operation: 图形\n\na(right)->b(right)->c(right)->d\n```\n\n\n\n\n(refer link)[http://www.gravizo.com/#howto]\n","tags":["markdown"]},{"title":"es-python-client","url":"/2018/10/12/es-python-client/","content":"# python 操作 Elasticsearch\n\n## 安装 Elasticsearch 模块\n`pip install elasticsearch`\n\n## 添加数据\n``` \nfrom elasticsearch import Elasticsearch\n\n# 默认host为localhost,port为9200.但也可以指定host与port\nes = Elasticsearch([{'host': '192.168.10.21', 'port': 9200}])\ndoc = {\n     'author': 'kimchy',\n    'text': 'Elasticsearch: cool. bonsai cool.',\n     'timestamp': localtime(),\n      }\nres = es.index(index=\"test-index\", doc_type='tweet', id=1, body=doc)\nprint(res)\n```\n如果创建成功会返回以下结果\n``` \n{'_index': 'test-index', '_type': 'tweet', '_id': '1', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 2, 'failed': 0}, '_seq_no': 0, '_primary_term': 1}\n\n```\n## 创建索引\n```\nes.indices.create(index='irisaa') \n```\n## 删除索引\n``` \nes.indices.create(index='irisaa')\n```\n## 查看集群状态\n```\nes.cluster.health(wait_for_status='yellow', request_timeout=1)\n```\n## 查询数据\n### 按照 id 来查询数据\n``` \nres = es.get(index=\"test-index\", doc_type='tweet', id=1)\nprint(res['_source'])\n```\n操作成功后返回如下结果：\n```\n{'author': 'kimchy', 'text': 'Elasticsearch: cool. bonsai cool.', 'timestamp': [2018, 10, 12, 14, 9, 44, 4, 285, 0]}\n\n```\n### 按照DSL语句查询\n```   \nres = es.search(index=\"test-index\", body={\"query\": {\"match_all\": {}}})\nprint(res)\n```\n操作成功后结果，返回如下结果：\n```\n{'took': 2, 'timed_out': False, '_shards': {'total': 5, 'successful': 5, 'skipped': 0, 'failed': 0}, 'hits': {'total': 1, 'max_score': 1.0, 'hits': [{'_index': 'test-index', '_type': 'tweet', '_id': '1', '_score': 1.0, '_source': {'author': 'kimchy', 'text': 'Elasticsearch: cool. bonsai cool.', 'timestamp': [2018, 10, 12, 14, 9, 44, 4, 285, 0]}}]}}\n```\n\n## 更新数据\n```\nes = Elasticsearch()\ndoc = {\n     'author': 'kimchy',\n    'text': 'ok.',\n     'timestamp': localtime(),\n      }\nresult = es.update(index=\"test-index\", doc_type='tweet', id=1, body=doc)\nprint(result) \n```\n\n\n## 删除数据\n```angular2html\n\nes.delete_by_query(index='twtter',doc_type='_doc',body={\n   \"query\": {\n     \"match\": {\n       \"message\": \"some message\"\n     }\n   }\n })\n```\n\n## 批量化导入es\n```\nfrom elasticsearch import helpers\ndef gendata(index, type, jsons):\n    for json in jsons:\n        yield {\n            \"_index\": index,\n            \"_type\": type,\n            \"_source\": json,\n        }\n        \nhelpers.bulk(es, gendata(index='index-test', jsons))        \n```\n","tags":["elasticsearch , ELK"]},{"title":"ELK_docker-compose.md","url":"/2018/10/12/ELK-docker-compose-md/","content":"# elasticsearch 集群部署\n我们使用dockers-compose实现单机多节点的部署\n\n## 安装docker\n在测试或开发环境中 Docker 官方为了简化安装流程，提供了一套便捷的安装脚本，Ubuntu 系统上可以使用这套脚本安装：\n``` \n$ curl -fsSL get.docker.com -o get-docker.sh\n$ sudo sh get-docker.sh --mirror Aliyun\n```\n执行这个命令后，脚本就会自动的将一切准备工作做好，并且把 Docker CE 的 Edge 版本安装在系统中。\n### 启动 Docker CE\n \n``` \n$ sudo systemctl enable docker\n$ sudo systemctl start docker\n```\n\n### 建立 docker 用户组\n默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组。\n\n`$ sudo groupadd docker\n`\n\n将当前用户 加入 ``docker`` 组：\n\n`$ sudo usermod -aG docker $USER`\n\n### 测试dockers 是否安装正确\n```\n$ docker run hello-world\n\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\nca4f61b1923c: Pull complete\nDigest: sha256:be0cd392e45be79ffeffa6b05338b98ebb16c87b255f48e297ec7f98e123905c\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://cloud.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/engine/userguide/\n```\n\n## 安装docker-compopse\nCompose 项目是 Docker 官方的开源项目，负责实现对 Docker 容器集群的快速编排。\n它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）。\n\nCompose 中有两个重要的概念：\n\n- 服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。\n\n- 项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。\n\nCompose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。\n\nCompose 项目由 Python 编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。因此，只要所操作的平台支持 Docker API，就可以在其上利用 Compose 来进行编排管理。\n\n### PIP 安装 dockerpose\n``$ sudo pip install -U docker-compose\n``\n\n### 编写 docker-compose.yml\n编写 docker-compose.yml 文件,输入以下内容：\n```\nversion: '2.2'\nservices:\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.0\n    container_name: elasticsearch\n    environment:\n      - cluster.name=docker-cluster\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n        - esdata1:/usr/share/elasticsearch/data\n    volumes:\n      - /home/ethan/EKL/config:/usr/share/elasticsearch/config\n    ports:\n      - 9200:9200\n      - 9300:9300\n    networks:\n      - esnet\n\n  elasticsearch2:\n    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.0\n    container_name: elasticsearch2\n    environment:\n      - cluster.name=docker-cluster\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n      - \"discovery.zen.ping.unicast.hosts=elasticsearch\"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n      - esdata2:/usr/share/elasticsearch/data\n    networks:\n      - esnet\n\n  elasticsearch3:\n    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.0\n    container_name: elasticsearch3\n    environment:\n      - cluster.name=docker-cluster\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n      - \"discovery.zen.ping.unicast.hosts=elasticsearch\"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n      - esdata3:/usr/share/elasticsearch/data\n    networks:\n      - esnet\n  kibana:\n    image: docker.elastic.co/kibana/kibana:6.4.0\n    container_name: kibana\n    environment:\n      SERVER_NAME: kibana\n      ELASTICSEARCH_URL: http://elasticsearch:9200\n    ports:\n      - 5601:5601\n    networks:\n      - esnet\n\nvolumes:\n  esdata1:\n    driver: local\n  esdata2:\n    driver: local\n  esdata3:\n    driver: local\n\nnetworks:\n  esnet:\n\n```\n## 运行 eslasticsearch-kibana\n```\ndocker-compose up\n\n```\n","tags":["Elk"]},{"title":"Elk初探","url":"/2018/10/10/Elk_platfrom/","content":"> 启动elaticsearch需要java环境，请自己谷歌搭建哈\n\n# elaticsearch\n下载tar包\n```powershell\nwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.4.1.tar.gz\n\n```\n\n解压\n```\ntar -zxvf elasticsearch-6.4.1.tar.gz\n```\n进入elaticsearch 可执行文件目录\n```angular2html\ncd elasticsearch-6.4.0/bin\n```\n启动elaticsearch\n```angular2html\n./elaticseaerch\n```\n如果没有抱任何错误，正常来说应该是可以通过restful API来访问的。\n````\ncurl -X GET \"localhost:9200/_cat/health?v\"\n\n````\n对应的返回结果\n```\nepoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\n1538014388 10:13:08  elasticsearch green           1         1      0   0    0    0        0             0                  -                100.0%\n```\n由此我们可以认为elaticsearch已经启动成功了。\n\n# kibana\n下载tar包\n```angular2html\nhttps://artifacts.elastic.co/downloads/kibana/kibana-6.4.1-linux-x86_64.tar.gz\n```\n解压tar包\n```\ntar -zxvf kibana-6.4.1-linux-x86_64.tar.gz\n```\n\n更改kibana配置文件\n```\nvim  /kibana-6.4.1-linux-x86_64/config/kibana.yml\n```\n添加或解注释以下内容\n```\n server.port: 5601\n server.host: \"localhost\"\n elasticsearch.url: \"http://localhost:9200\"\n\n```\n进入kibana可执行文件目录\n```\ncd kibana-6.4.1-linux-x86_64/bin/\n```\n启动kibana\n\n`./kibana`\n\n配置成功后，用浏览器访问`http://ip:5601`可以看到以下页面\n![](/image/kibana.png)\n\n\n# elaticsearc 基本操作\n## 检查集群状态\n```\ncurl -X GET \"localhost:9200/_cat/health?v\"\n\n```\n获取结果如下显示\n```\nepoch      timestamp cluster     status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\n1538019385 11:36:25  data-mining green           1         1      1   1    0    0        0             0                  -                100.0%\n\n```\n## 列出所有索引\n``` \ncurl -X GET \"localhost:9200/_cat/indices?v\"\n```\n如下图所示，我们只有一个索引\n```\nhealth status index   uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   .kibana eFUOj6qJTQCt4xQdvbv8KA   1   0          1            0        4kb            4kb\n\n```\n\n## 创建一个自定义的索引\n```angular2html\n curl -X PUT \"localhost:9200/customer?pretty\"\n```\n对插入的索引返回结果\n```\n\n{\n  \"acknowledged\" : true,\n  \"shards_acknowledged\" : true,\n  \"index\" : \"customer\"\n}\n\n```\n\n再次查看当前索引\n````angular2html\n curl -X GET \"localhost:9200/_cat/indices?v\"\n````\n可以观察到多了一个名为‘customer’的新索引\n```\nhealth status index    uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   .kibana  eFUOj6qJTQCt4xQdvbv8KA   1   0          1            0        4kb            4kb\nyellow open   customer k50FrrMLScGvwzOvfiF0fg   5   1          0            0       401b           401b\n```\n## 插入一个文档\n```angular2html\ncurl -X PUT \"localhost:9200/customer/_doc/1?pretty\" -H 'Content-Type: application/json' -d'\n{\n  \"name\": \"John Doe\"\n}\n'\n```\n如果操作正常，那么插入成功后会返回以下结果\n``` \n{\n  \"_index\" : \"customer\",\n  \"_type\" : \"_doc\",\n  \"_id\" : \"1\",\n  \"_version\" : 1,\n  \"result\" : \"created\",\n  \"_shards\" : {\n    \"total\" : 2,\n    \"successful\" : 1,\n    \"failed\" : 0\n  },\n  \"_seq_no\" : 0,\n  \"_primary_term\" : 1\n}\n```\n## 按照_id查询索引内的文档\n```\ncurl -X GET \"localhost:9200/customer/_doc/1?pretty\"\n```\n查看返回结果，\"_index\"为当前查询的索引，“_type”为查询的文档类型，“_id”为文档所在的id，\n“_source”为我们要查询的内容\n\n```\n\n{\n  \"_index\" : \"customer\",\n  \"_type\" : \"_doc\",\n  \"_id\" : \"1\",\n  \"_version\" : 1,\n  \"found\" : true,\n  \"_source\" : {\n    \"name\" : \"John Doe\"\n  }\n}\n```\n# elastcisear-head\n为了更方便的地查询es中的数据，我推荐使用es插件elasticsearch-head来快速检索es中的数据。\n\n传送门：[elastcsearch-head](https://github.com/mobz/elasticsearch-head)\n\n如果你使用的版本是在elasticsearch 5之后，还需要对elasticsearch 进行配置\n```\ncat >> elasticsearch-6.4.0/config/elasticsearch.yml << EOF\nhttp.cors.enabled: true\nhttp.cors.allow-origin: \"*\"\nEOF\n```\n\n\n## 安装 \n由于elasticsearch-head 需要nodejs，所以我们需要先安装 nodejs 以及 npm\n## nodejs 安装\n```\n$ sudo apt install nodejs\n$ sudo apt instlal npm\n```\nnodejs 安装完后，我们就可以把elasticsearch-head 下载，进行配置了。\n```\ngit clone git://github.com/mobz/elasticsearch-head.git\ncd elasticsearch-head\nnpm install\nnpm run start\n```\n操作成功后的输出显示\n```\n $ npm run start\n\n> elasticsearch-head@0.0.0 start /home/ethan/ekl/elasticsearch-head\n> grunt server\n\n(node:22696) ExperimentalWarning: The http2 module is an experimental API.\nRunning \"connect:server\" (connect) task\nWaiting forever...\nStarted connect web server on http://localhost:9100\n```\n![](/image/eshead.png)\n\n这样我们最简单的数据搜素平台就搭建成功了。\n","tags":["Elk"]},{"title":"Ethan Lee","url":"/2018/08/18/mygirl/","content":"# say something to mygirl\n\n\n<blockquote class=\"blockquote-center\">我知道到遇见你不容易,错过了会很可惜,我不希望余生都是回忆,我希望余生都是你,我爱你</blockquote>\n\n\n![](http://img02.tooopen.com/images/20160509/tooopen_sy_161967094653.jpg)\n\n[link](https://ethan2lee.github.io)","tags":["paper"]}]