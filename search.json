[{"title":"deque","url":"/2018/10/22/deque/","content":"# Deque抽象数据类型\n\ndeque 抽象数据类型由以下结构和操作定义。如上所述，deque 被构造为项的有序集合，其中项从首部或尾部的任一端添加和移除。下面给出了 deque 操作。\n\n1. Deque() 创建一个空的新 deque。它不需要参数，并返回空的 deque。\n2. addFront(item) 将一个新项添加到 deque 的首部。它需要 item 参数 并不返回任何内容。\n3. addRear(item) 将一个新项添加到 deque 的尾部。它需要 item 参数并不返回任何内容。\n4. removeFront() 从 deque 中删除首项。它不需要参数并返回 item。deque 被修改。\n5. removeRear() 从 deque 中删除尾项。它不需要参数并返回 item。deque 被修改。\n6. isEmpty() 测试 deque 是否为空。它不需要参数，并返回布尔值。\n7. size() 返回 deque 中的项数。它不需要参数，并返回一个整数。\n\n## Python实现Deque\n\n```python\nclass Deque:\n    def __init__(self):\n        self.items = []\n\n    def isEmpty(self):\n        return self.items == []\n\n    def addFront(self, item):\n        self.items.append(item)\n\n    def addRear(self, item):\n        self.items.insert(0,item)\n\n    def removeFront(self):\n        return self.items.pop()\n\n    def removeRear(self):\n        return self.items.pop(0)\n\n    def size(self):\n        return len(self.items)\n```\n\n在 removeFront 中，我们使用 pop 方法从列表中删除最后一个元素。 但是，在removeRear中，pop(0)方法必须删除列表的第一个元素。同样，我们需要在 addRear 中使用insert方法（第12行），因为 append 方法在列表的末尾添加一个新元素。\n你可以看到许多与栈和队列中描述的 Python 代码相似之处。你也可能观察到，在这个实现中，从前面添加和删除项是 O(1)，而从后面添加和删除是 O(n)。 考虑到添加和删除项是出现的常见操作，这是可预期的。 同样，重要的是要确定我们知道在实现中前后都分配在哪里。\n\n## 回文检查\n\n使用 deque 数据结构可以容易地解决经典回文问题。回文是一个字符串，读取首尾相同的字符，例如，radar toot madam。 我们想构造一个算法输入一个字符串，并检查它是否是一个回文。\n该问题的解决方案将使用 deque 来存储字符串的字符。我们从左到右处理字符串，并将每个字符添加到 deque 的尾部。在这一点上，deque 像一个普通的队列。然而，我们现在可以利用 deque 的双重功能。 deque 的首部保存字符串的第一个字符，deque 的尾部保存最后一个字符\n![](https://xidianwlc.gitbooks.io/python-data-structrue-and-algrothms/content/3.%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/3.18.%E5%9B%9E%E6%96%87%E6%A3%80%E6%9F%A5/assets/3.18.%E5%9B%9E%E6%96%87%E6%A3%80%E6%9F%A5.figure2.png)\n\n```python\ndef palchecker(aString):\n    chardeque = Deque()\n\n    for ch in aString:\n        chardeque.addRear(ch)\n\n    stillEqual = True\n\n    while chardeque.size() > 1 and stillEqual:\n        first = chardeque.removeFront()\n        last = chardeque.removeRear()\n        if first != last:\n            stillEqual = False\n\n    return stillEqual\n\nprint(palchecker(\"lsdkjfskf\"))\nprint(palchecker(\"radar\"))\n```\n\n```\nFalse\nTrue\n```\n\n\n","tags":["算法"]},{"title":"队列","url":"/2018/10/22/队列/","content":"# 队列\n\n我们为了实现队列抽象数据类型创建一个新类。和前面一样，我们将使用列表集合来作为构建队列的内部表示。\n\n我们需要确定列表的哪一端作为队首，哪一端作为队尾。\n如下所示的实现假定队尾在列表中的位置为 0。\n这允许我们使用列表上的插入函数向队尾添加新元素。弹出操作可用于删除队首的元素（列表的最后一个元素）。回想一下，这也意味着入队为 O(n)，出队为 O(1)。\n\n```python\nclass Queue():\n    def __init__(self):\n        self.items=[]\n    def isEmpty(self):\n        return self.items == []\n    def enqueue(self,item):\n        self.items.insert(0,item)\n    def dequeue(self):\n        return self.items.pop()\n    def size(self):\n        return len(self.items)\n    \n\n```\n\n## 初始化队列以及基础应用\n\n```python\nq = Queue()\nq.isEmpty()\n\n```\n\n\n\n```\nTrue\n```\n\n\n\n```python\nq.enqueue(1)\nq.dequeue()\n```\n\n\n\n```\n1\n```\n\n\n\n```python\nfor i in range(10):\n    q.enqueue(i)\n```\n\n```python\nq.isEmpty()\n```\n\n\n\n```\nFalse\n```\n\n\n\n```python\nq.size()\n```\n\n\n\n```\n10\n```\n\n\n\n```python\nwhile not q.isEmpty():\n    print(q.dequeue())\n```\n\n```\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n```\n\n## 队列的应用\n\n### 烫手山芋\n\n队列的典型应用之一是模拟需要以 FIFO 方式管理数据的真实场景。首先，让我们看看孩子们的游戏烫手山芋，在这个游戏中，孩子们围成一个圈，并尽可能快的将一个山芋递给旁边的孩子。在某一个时间，动作结束，有山芋的孩子从圈中移除。游戏继续开始直到剩下最后一个孩子。\n![](https://xidianwlc.gitbooks.io/python-data-structrue-and-algrothms/content/3.%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/3.13.%E6%A8%A1%E6%8B%9F%EF%BC%9A%E7%83%AB%E6%89%8B%E5%B1%B1%E8%8A%8B/assets/3.13.%E6%A8%A1%E6%8B%9F%EF%BC%9A%E7%83%AB%E6%89%8B%E5%B1%B1%E8%8A%8B.figure2.png)\n\n为了模拟这个圈，我们使用队列。假设拿着山芋的孩子在队列的前面。当拿到山芋的时候，这个孩子将先出列再入队列，把他放在队列的最后。经过 num 次的出队入队后，前面的孩子将被永久移除队列。并且另一个周期开始，继续此过程，直到只剩下一个名字（队列的大小为 1）\n![](https://xidianwlc.gitbooks.io/python-data-structrue-and-algrothms/content/3.%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/3.13.%E6%A8%A1%E6%8B%9F%EF%BC%9A%E7%83%AB%E6%89%8B%E5%B1%B1%E8%8A%8B/assets/3.13.%E6%A8%A1%E6%8B%9F%EF%BC%9A%E7%83%AB%E6%89%8B%E5%B1%B1%E8%8A%8B.figure3.png)\n\n```python\ndef hotPotato(namelist, num):\n    simqueue = Queue()\n    for name in namelist:\n        simqueue.enqueue(name)\n\n    while simqueue.size() > 1:\n        for i in range(num):\n            simqueue.enqueue(simqueue.dequeue())\n\n        simqueue.dequeue()\n\n    return simqueue.dequeue()\n\nprint(hotPotato([\"Bill\",\"David\",\"Susan\",\"Jane\",\"Kent\",\"Brad\"],7))\n```\n\n请注意，在此示例中，计数常数的值大于列表中的名称数。这不是一个问题，因为队列像一个圈，计数会重新回到开始，直到达到计数值。另外，请注意，列表加载到队列中以使列表上的名字位于队列的前面。在这种情况下，Bill 是列表中的第一个项，因此他在队列的前面。\n\n### 打印机\n\n主要模拟步骤\n\n1. 创建打印任务的队列，每个任务都有个时间戳。队列启动的时候为空。\n2. 每秒（currentSecond）：\n   - 是否创建新的打印任务？如果是，将 currentSecond 作为时间戳添加到队列。\n   - 如果打印机不忙并且有任务在等待\n     - 从打印机队列中删除一个任务并将其分配给打印机\n     - 从 currentSecond 中减去时间戳，以计算该任务的等待时间。\n     - 将该任务的等待时间附件到列表中稍后处理。\n     - 根据打印任务的页数，确定需要多少时间。\n   - 打印机需要一秒打印，所以得从该任务的所需的等待时间减去一秒。\n   - 如果任务已经完成，换句话说，所需的时间已经达到零，打印机空闲。\n3. 模拟完成后，从生成的等待时间列表中计算平均等待时间。\n\n为了设计此模拟，我们将为上述三个真实世界对象创建类：Printer, Task, PrintQueue\n\nPrinter 类需要跟踪它当前是否有任务。\n\n如果有，则它处于忙碌状态（13-17 行），并且可以从任务的页数计算所需的时间。\n\n构造函数允许初始化每分钟页面的配置，tick 方法将内部定时器递减直到打印机设置为空闲(11 行)\n\n```python\nclass Printer:\n    def __init__(self, ppm):\n        self.pagerate = ppm\n        self.currentTask = None\n        self.timeRemaining = 0\n\n    def tick(self):\n        if self.currentTask != None:\n            self.timeRemaining = self.timeRemaining - 1\n            if self.timeRemaining <= 0:\n                self.currentTask = None\n\n    def busy(self):\n        if self.currentTask != None:\n            return True\n        else:\n            return False\n\n    def startNext(self,newtask):\n        self.currentTask = newtask\n        self.timeRemaining = newtask.getPages() * 60/self.pagerate\n```\n\nTask 类表示单个打印任务。创建任务时，随机数生成器将提供 1 到 20 页的长度。我们选择使用随机模块中的 randrange 函数\n每个任务还需要保存一个时间戳用于计算等待时间。此时间戳将表示任务被创建并放置到打印机队列中的时间。可以使用 waitTime 方法来检索在打印开始之前队列中花费的时间。\n\n```python\nimport random\n\nclass Task:\n    def __init__(self,time):\n        self.timestamp = time\n        self.pages = random.randrange(1,21)\n\n    def getStamp(self):\n        return self.timestamp\n\n    def getPages(self):\n        return self.pages\n\n    def waitTime(self, currenttime):\n        return currenttime - self.timestamp\n```\n\n以下代码实现了上述算法。PrintQueue 对象是我们现有队列 ADT 的一个实例。\nnewPrintTask 决定是否创建一个新的打印任务。我们再次选择使用随机模块的 randrange 函数返回 1 到 180 之间的随机整数。\n打印任务每 180 秒到达一次。通过从随机整数（32 行）的范围中任意选择，我们可以模拟这个随机事件。\n模拟功能允许我们设置打印机的总时间和每分钟的页数。\n\n```python\ndef simulation(numSeconds, pagesPerMinute):\n\n    labprinter = Printer(pagesPerMinute)\n    printQueue = Queue()\n    waitingtimes = []\n\n    for currentSecond in range(numSeconds):\n\n      if newPrintTask():\n         task = Task(currentSecond)\n         printQueue.enqueue(task)\n\n      if (not labprinter.busy()) and (not printQueue.isEmpty()):\n        nexttask = printQueue.dequeue()\n        waitingtimes.append(nexttask.waitTime(currentSecond))\n        labprinter.startNext(nexttask)\n\n      labprinter.tick()\n\n    averageWait=sum(waitingtimes)/len(waitingtimes)\n    print(\"Average Wait %6.2f secs %3d tasks remaining.\"%(averageWait,printQueue.size()))\n\ndef newPrintTask():\n    num = random.randrange(1,181)\n    if num == 180:\n        return True\n    else:\n        return False\n\n\n```\n当我们运行模拟时，我们不应该担心每次的结果不同。这是由于随机数的概率性质决定的。 因为模拟的参数可以被调整，我们对调整后可能发生的趋势感兴趣。 这里有一些结果。\n首先，我们将使用每分钟五页的页面速率运行模拟 60 分钟（3,600秒）。 此外，我们将进行 10 次独立试验。记住，因为模拟使用随机数，每次运行将返回不同的结果。\n\n```python\nfor i in range(10):\n    simulation(3600,5)\n```\n\n```\nAverage Wait 133.21 secs   0 tasks remaining.\nAverage Wait  69.95 secs   1 tasks remaining.\nAverage Wait  18.21 secs   0 tasks remaining.\nAverage Wait 156.15 secs   1 tasks remaining.\nAverage Wait 124.05 secs   1 tasks remaining.\nAverage Wait 194.70 secs   4 tasks remaining.\nAverage Wait  48.24 secs   1 tasks remaining.\nAverage Wait  80.65 secs   0 tasks remaining.\nAverage Wait  62.31 secs   0 tasks remaining.\nAverage Wait  85.43 secs   2 tasks remaining.\n```\n\n\n","tags":["算法"]},{"title":"数据挖掘基本知识","url":"/2018/10/22/数据挖掘基本知识/","content":"# 数据挖掘的基础知识点\n\n-  数据、信息和知识是广义数据表现的不同形式\n\n-  主要知识模式类型有：广义知识，关联知识，类知识，预测型知识，特异型知识\n\n\n\n-  web挖掘研究的主要流派有：Web结构挖掘、Web使用挖掘、Web内容挖掘\n\n- 一般地说，KDD是一个多步骤的处理过程，一般分为**问题定义**、**数据抽取**、**数据预处理**、.**数据挖掘**以及**模式评估**等基本阶段。\n\n- 数据库中的知识发现处理过程模型有：阶梯处理过程模型，螺旋处理过程模型，以用户为中心的处理结构模型，联机KDD模型，支持多数据源多知识模式的KDD处理模型\n-  粗略地说，知识发现软件或工具的发展经历了独立的知识发现软件、横向的知识发现工具集和纵向的知识发现解决方案三个主要阶段，其中后面两种反映了目前知识发现软件的两个主要发展方向。\n\n-  决策树分类模型的建立通常分为两个步骤：决策树生成，决策树修剪。\n\n\n\n-  从使用的主要技术上看，可以把分类方法归结为四种类型：\n\n  - ​\t基于距离的分类方法\n\n  - ​\t决策树分类方法\n\n  - ​\t贝叶斯分类方法\n\n  - ​\t规则归纳方法\n\n-  关联规则挖掘问题可以划分成两个子问题：\n  1. 发现频繁项目集:通过用户给定Minsupport ，寻找所有频繁项目集或者最大频繁项目集。\n  2. 生成关联规则:通过用户给定Minconfidence ，在频繁项目集中，寻找关联规则。\n\n- 数据挖掘是相关学科充分发展的基础上被提出和发展的，主要的相关技术：\n  - 数据库等信息技术的发展\n  - 统计学深入应用\n  - 人工智能技术的研究和应用\n\n \n\n- 衡量关联规则挖掘结果的有效性，应该从多种综合角度来考虑：\n  - 准确性：挖掘出的规则必须反映数据的实际情况。\n  - 实用性：挖掘出的规则必须是简洁可用的。\n  - 新颖性：挖掘出的关联规则可以为用户提供新的有价值信息。\n\n-  约束的常见类型有：\n\n  ​\t单调性约束;\n\n  ​\t反单调性约束;\n\n  ​\t可转变的约束;\n\n  ​\t简洁性约束.\n\n- 根据规则中涉及到的层次，多层次关联规则可以分为：\n\n  ​\t同层关联规则：如果一个关联规则对应的项目是同一个粒度层次，那么它是同层关联规则。\n\n  ​\t层间关联规则：如果在不同的粒度层次上考虑问题，那么可能得到的是层间关联规\n\n \n\n-  按照聚类分析算法的主要思路，聚类方法可以被归纳为如下几种。\n\n  - 划分法：基于一定标准构建数据的划分。\n\n  - 属于该类的聚类方法有：k-means、k-modes、k-prototypes、k-medoids、PAM、CLARA、CLARANS等。\n  - 层次法：对给定数据对象集合进行层次的分解\n  - 密度法：基于数据对象的相连密度评价。\n  - 网格法：将数据空间划分成为有限个单元(Cell)的网格结构，基于网格结构进行聚类。\n  - 模型法：给每一个簇假定一个模型，然后去寻找能够很好的满足这个模型的数据集。\n\n- 类间距离的度量主要有：\n  - 最短距离法：定义两个类中最靠近的两个元素间的距离为类间距离。\n  - 最长距离法：定义两个类中最远的两个元素间的距离为类间距离。\n  - 中心法：定义两类的两个中心间的距离为类间距离。\n  - 类平均法：它计算两个类中任意两个元素间的距离，并且综合他们为类间距离：离差平方和。\n\n \n\n- 层次聚类方法具体可分为：\n  - 凝聚的层次聚类：一种自底向上的策略，首先将每个对象作为一个簇，然后合并这些原子簇为越来越大的簇，直到某个终结条件被满足。\n  - 分裂的层次聚类：采用自顶向下的策略，它首先将所有对象置于一个簇中，然后逐渐细分为越来越小的簇，直到达到了某个终结条件。\n  - 层次凝聚的代表是AGNES算法。层次分裂的代表是DIANA算法。\n\n-   文本挖掘(TD)的方式和目标是多种多样的，基本层次有：\n  - 关键词检索：最简单的方式，它和传统的搜索技术类似。\n  - 挖掘项目关联：聚焦在页面的信息(包括关键词)之间的关联信息挖掘上。\n  - 信息分类和聚类：利用数据挖掘的分类和聚类技术实现页面的分类，将页面在一个更到层次上进行抽象和整理。\n  - 自然语言处理：揭示自然语言处理技术中的语义，实现Web内容的更精确处理。 \n\n-  在web访问挖掘中常用的技术：\n  - 路径分析\n    - 路径分析最常用的应用是用于判定在一个Web站点中最频繁访问的路径，这样的知识对于一个电子商务网站或者信息安全评估是非常重要的。\n  - 关联规则发现\n    - 使用关联规则发现方法可以从Web访问事务集中，找到一般性的关联知识。\n  - 序列模式发现\n    - 在时间戳有序的事务集中，序列模式的发现就是指找到那些如“一些项跟随另一个项”这样的内部事务模式。\n  - 分类\n    - 发现分类规则可以给出识别一个特殊群体的公共属性的描述。这种描述可以用于分类新的项。\n  - 聚类\n    - 可以从Web Usage数据中聚集出具有相似特性的那些客户。在Web事务日志中，聚类顾客信息或数据项，就能够便于开发和执行未来的市场战略。\n\n\n\n- 根据功能和侧重点不同，数据挖掘语言可以分为三种类型：\n  - 数据挖掘查询语言：希望以一种像SQL这样的数据库查询语言完成数据挖掘的任务。\n  - 数据挖掘建模语言：对数据挖掘模型进行描述和定义的语言，设计一种标准的数据挖掘建模语言，使得数据挖掘系统在模型定义和描述方面有标准可以遵循。\n  - 通用数据挖掘语言：通用数据挖掘语言合并了上述两种语言的特点，既具有定义模型的功能，又能作为查询语言与数据挖掘系统通信，进行交互式挖掘。通用数据挖掘语言标准化是目前解决数据挖掘行业出现问题的颇具吸引力的研究方向。\n\n \n\n- 规则归纳有四种策略：减法、加法，先加后减、先减后加策略。\n  - 减法策略：以具体例子为出发点，对例子进行推广或泛化，推广即减除条件(属性值)或减除合取项(为了方便，我们不考虑增加析取项的推广)，使推广后的例子或规则不覆盖任何反例。\n  - 加法策略：起始假设规则的条件部分为空(永真规则)，如果该规则覆盖了反例，则不停地向规则增加条件或合取项，直到该规则不再覆盖反例。\n  - 先加后减策略：由于属性间存在相关性，因此可能某个条件的加入会导致前面加入的条件没什么作用，因此需要减除前面的条件。\n  - 先减后加策略：道理同先加后减，也是为了处理属性间的相关性。\n\n \n\n-  数据挖掘定义有广义和狭义之分。\n  - 从广义的观点，数据挖掘是从大型数据集(可能是不完全的、有噪声的、不确定性的、各种存储形式的)中，挖掘隐含在其中的、人们事先不知道的、对决策有用的知识的过程。\n  - 从这种狭义的观点上，我们可以定义数据挖掘是从特定形式的数据集中提炼知识的过程。\n\n \n\n- web挖掘的含义： \n  - 针对包括Web页面内容、页面之间的结构、用户访问信息、电子商务信息等在内的各种Web数据，应用数据挖掘方法以帮助人们从因特网中提取知识，为访问者、站点经营者以及包括电子商务在内的基于因特网的商务活动提供决策支持。\n\n\n\n- K-近邻分类算法(K Nearest Neighbors，简称KNN)的定义：通过计算每个训练数据到待分类元组的距离，取和待分类元组距离最近的K个训练数据，K个数据中哪个类别的训练数据占多数，则待分类元组就属于哪个类别。\n\n- K-means算法的性能分析：\n\n  主要优点：\n\n  - 是解决聚类问题的一种经典算法，简单、快速。\n  - 对处理大数据集，该算法是相对可伸缩和高效率的。\n  - 当结果簇是密集的，它的效果较好。\n\n  主要缺点\n\n  - 在簇的平均值被定义的情况下才能使用，可能不适用于某些应用。\n  - 必须事先给出k(要生成的簇的数目)，而且对初值敏感，对于不同的初始值，可能会导致不同结果。\n  - 不适合于发现非凸面形状的簇或者大小差别很大的簇。而且，它对于“躁声”和孤立点数据是敏感的。\n\n- ID3算法的性能分析：\n  - ID3算法的假设空间包含所有的决策树，它是关于现有属性的有限离散值函数的一个完整空间。所以ID3算法避免了搜索不完整假设空间的一个主要风险：假设空间可能不包含目标函数。\n  - ID3算法在搜索的每一步都使用当前的所有训练样例，大大降低了对个别训练样例错误的敏感性。因此，通过修改终止准则，可以容易地扩展到处理含有噪声的训练数据。\n  - ID3算法在搜索过程中不进行回溯。所以，它易受无回溯的爬山搜索中的常见风险影响：收敛到局部最优而不是全局最优。\n\n \n\n- Apriori算法有两个致命的性能瓶颈:\n\n  - 多次扫描事务数据库，需要很大的I/O负载：\n\n    对每次k循环，侯选集Ck中的每个元素都必须通过扫描数据库一次来验证其是否加入Lk。假如有一个频繁大项目集包含10个项的话，那么就至少需要扫描事务数据库10遍。\n\n  - 可能产生庞大的侯选集：\n\n    由Lk-1产生k-侯选集Ck是指数增长的，例如104个1-频繁项目集就有可能产生接近107个元素的2-侯选集。如此大的侯选集对时间和主存空间都是一种挑战。a基于数据分割的方法：基本原理是“在一个划分中的支持度小于最小支持度的k-项集不可能是全局频繁的”。\n\n\n\n- 改善Apriori算法适应性和效率的主要的改进方法有：\n  - 基于数据分割(Partition)的方法：基本原理是“在一个划分中的支持度小于最小支持度的k-项集不可能是全局频繁的”。\n  - 基于散列的方法：基本原理是“在一个hash桶内支持度小于最小支持度的k-项集不可能是全局频繁的”。\n  - 基于采样的方法：基本原理是“通过采样技术，评估被采样的子集中，并依次来估计k-项集的全局频度”。\n  - 其他：如，动态删除没有用的事务：“不包含任何Lk的事务对未来的扫描结果不会产生影响，因而可以删除”。\n\n \n\n-  面向Web的数据挖掘比面向数据库和数据仓库的数据挖掘要复杂得多：\n  - 异构数据源环境：Web网站上的信息是异构: 每个站点的信息和组织都不一样;存在大量的无结构的文本信息、复杂的多媒体信息;站点使用和安全性、私密性要求各异等等。\n  - 数据的是复杂性：有些是无结构的(如Web页)，通常都是用长的句子或短语来表达文档类信息;有些可能是半结构的(如Email，HTML页)。当然有些具有很好的结构(如电子表格)。揭开这些复合对象蕴涵的一般性描述特征成为数据挖掘的不可推卸的责任。\n  - 动态变化的应用环境：\n  - Web的信息是频繁变化的，像新闻、股票等信息是实时更新的。\n  - 这种高变化也体现在页面的动态链接和随机存取上。\n  - Web上的用户是难以预测的。\n  - Web上的数据环境是高噪音的。\n\n- 简述知识发现项目的过程化管理I-MIN过程模型。\n  - MIN过程模型把KDD过程分成IM1、IM2、…、IM6等步骤处理，在每个步骤里，集中讨论几个问题，并按一定的质量标准来控制项目的实施。\n  - IM1任务与目的：它是KDD项目的计划阶段，确定企业的挖掘目标，选择知识发现模式，编译知识发现模式得到的元数据;其目的是将企业的挖掘目标嵌入到对应的知识模式中。\n  - IM2任务与目的：它是KDD的预处理阶段，可以用IM2a、IM2b、IM2c等分别对应于数据清洗、数据选择和数据转换等阶段。其目的是生成高质量的目标数据。\n  - IM3任务与目的：它是KDD的挖掘准备阶段，数据挖掘工程师进行挖掘实验，反复测试和验证模型的有效性。其目的是通过实验和训练得到浓缩知识(Knowledge Concentrate)，为最终用户提供可使用的模型。\n  - IM4任务与目的：它是KDD的数据挖掘阶段，用户通过指定数据挖掘算法得到对应的知识。\n  - IM5任务与目的：它是KDD的知识表示阶段，按指定要求形成规格化的知识。\n  - IM6任务与目的：它是KDD的知识解释与使用阶段，其目的是根据用户要求直观地输出知识或集成到企业的知识库中。\n\n-  改善Apriori算法适应性和效率的主要的改进方法有：\n  - 基于数据分割(Partition)的方法：基本原理是“在一个划分中的支持度小于最小支持度的k-项集不可能是全局频繁的”。\n  - 基于散列(Hash)的方法：基本原理是“在一个hash桶内支持度小于最小支持度的k-项集不可能是全局频繁的”。\n  - 基于采样(Sampling)的方法：基本原理是“通过采样技术，评估被采样的子集中，并依次来估计k-项集的全局频度”。\n  - 其他：如，动态删除没有用的事务：“不包含任何Lk的事务对未来的扫描结果不会产生影响，因而可以删除”。\n\n \n\n- 数据分类的两个步骤是什么?\n  - 建立一个模型，描述预定的数据类集或概念集\n  - 数据元组也称作样本、实例或对象。\n  - 为建立模型而被分析的数据元组形成训练数据集。\n  - 训练数据集中的单个元组称作训练样本，由于提供了每个训练样本的类标号，因此也称作有指导的学习。\n  - 通过分析训练数据集来构造分类模型，可用分类规则、决策树或数学公式等形式提供。\n  - 使用模型进行分类\n  - 首先评估模型(分类法)的预测准确率。\n  - 如果认为模型的准确率可以接受，就可以用它对类标号未知的数据元组或对象进行分类。\n\n- web访问信息挖掘的特点：\n  - Web访问数据容量大、分布广、内涵丰富和形态多样\n  - 一个中等大小的网站每天可以记载几兆的用户访问信息。\n  - 广泛分布于世界各处。\n  - 访问信息形态多样。\n  - 访问信息具有丰富的内涵。\n  - Web访问数据包含决策可用的信息\n  - 每个用户的访问特点可以被用来识别该用户和网站访问的特性。\n  - 同一类用户的访问，代表同一类用户的个性。\n  - 一段时期的访问数据代表了群体用户的行为和群体用户的共性。\n  - Web访问信息数据是网站的设计者和访问者进行沟通的桥梁。\n  - Web访问信息数据是开展数据挖掘研究的良好的对象。\n  - Web访问信息挖掘对象的特点\n  - 访问事务的元素是Web页面，事务元素之间存在着丰富的结构信息。\n  - 访问事务的元素代表的是每个访问者的顺序关系，事务元素之间存在着丰富的顺序信息。\n  - 每个页面的内容可以被抽象出不同的概念，访问顺序和访问量部分决定概念。\n  - 用户对页面存在不同的访问时长，访问长代表了用户的访问兴趣。\n\n \n\n- web页面内文本信息的挖掘：\n  - 挖掘的目标是对页面进行摘要和分类。\n    - 页面摘要：对每一个页面应用传统的文本摘要方法可以得到相应的摘要信息。\n    - 页面分类：分类器输入的是一个Web页面集(训练集)，再根据页面文本信息内容进行监督学习，然后就可以把学成的分类器用于分类每一个新输入的页面。\n\n{在文本学习中常用的方法是TFIDF向量表示法，它是一种文档的词集(Bag-of-Words)表示法，所有的词从文档中抽取出来，而不考虑词间的次序和文本的结构。这种构造二维表的方法是：\n\n- 每一列为一个词，列集(特征集)为辞典中的所有有区分价值的词，所以整个列集可能有几十万列之多。\n- 每一行存储一个页面内词的信息，这时，该页面中的所有词对应到列集(特征集)上。列集中的每一个列(词)，如果在该页面中不出现，则其值为0;如果出现k次，那么其值就为k;页面中的词如果不出现在列集上，可以被放弃。这种方法可以表征出页面中词的频度。\n\n对中文页面来说，还需先分词然后再进行以上两步处理。\n\n这样构造的二维表表示的是Web页面集合的词的统计信息，最终就可以采用Naive Bayesian方法或k-Nearest Neighbor等方法进行分类挖掘。\n\n在挖掘之前，一般要先进行特征子集的选取，以降低维数。\n\n**转自：数据在线；**\n\n\n","tags":["机器学习"]},{"title":"栈应用","url":"/2018/10/21/栈应用/","content":"# 栈应用\n\n## 栈数据结构的定义\n\n```python\nclass Stack:\n     def __init__(self):\n         self.items = []\n\n     def isEmpty(self):\n         return self.items == []\n\n     def push(self, item):\n         self.items.append(item)\n\n     def pop(self):\n         return self.items.pop()\n\n     def peek(self):\n         return self.items[len(self.items)-1]\n\n     def size(self):\n         return len(self.items)\n```\n\n```python\ns=Stack()\n\nprint(s.isEmpty())\ns.push(4)\ns.push('dog')\nprint(s.peek())\ns.push(True)\nprint(s.size())\nprint(s.isEmpty())\ns.push(8.4)\nprint(s.pop())\nprint(s.pop())\n```\n\n```\nTrue\ndog\n3\nFalse\n8.4\nTrue\n```\n\n## 简单的符号匹配\n\n```python\ndef parChecker(symbolString):\n    s = Stack()\n    balanced = True\n    index = 0\n    while index < len(symbolString) and balanced:\n        symbol = symbolString[index]\n        if symbol == \"(\":\n            s.push(symbol)\n        else:\n            if s.isEmpty():\n                balanced = False\n            else:\n                s.pop()\n\n        index = index + 1\n\n    if balanced and s.isEmpty():\n        return True\n    else:\n        return False\n\nprint(parChecker('((()))'))\nprint(parChecker('(()'))\n```\n\n```\nTrue\nFalse\n```\n\n## 符号匹配\n\n```python\ndef parChecker(symbolString):\n    s = Stack()\n    balanced = True\n    index = 0\n    while index < len(symbolString) and balanced:\n        symbol = symbolString[index]\n        if symbol in \"([{\":\n            s.push(symbol)\n        else:\n            if s.isEmpty():\n                balanced = False\n            else:\n                top = s.pop()\n                if not matches(top,symbol):\n                       balanced = False\n        index = index + 1\n    if balanced and s.isEmpty():\n        return True\n    else:\n        return False\n\ndef matches(open,close):\n    opens = \"([{\"\n    closers = \")]}\"\n    return opens.index(open) == closers.index(close)\n\n\nprint(parChecker('{{([][])}()}'))\nprint(parChecker('[{()]'))\n```\n\n```\nTrue\nFalse\n```\n\n## 十进制转二进制\n\n```python\ndef divideBy2(decNumber):\n    remstack = Stack()\n\n    while decNumber > 0:\n        rem = decNumber % 2\n        remstack.push(rem)\n        decNumber = decNumber // 2\n\n    binString = \"\"\n    while not remstack.isEmpty():\n        binString = binString + str(remstack.pop())\n\n    return binString\n\nprint(divideBy2(42))\n```\n\n```\n101010\n```\n\n## 十进制转化为多进制\n\n```python\ndef baseConverter(decNumber,base):\n    digits = \"0123456789ABCDEF\"\n\n    remstack = Stack()\n\n    while decNumber > 0:\n        rem = decNumber % base\n        remstack.push(rem)\n        decNumber = decNumber // base\n\n    newString = \"\"\n    while not remstack.isEmpty():\n        newString = newString + digits[remstack.pop()]\n\n    return newString\n\nprint(baseConverter(25,2))\nprint(baseConverter(25,16))\n```\n\n```\n11001\n19\n```\n\n## 中缀转后缀通用法\n\n```python\ndef infixToPostfix(infixexpr):\n    prec = {}\n    prec[\"*\"] = 3\n    prec[\"/\"] = 3\n    prec[\"+\"] = 2\n    prec[\"-\"] = 2\n    prec[\"(\"] = 1\n    opStack = Stack()\n    postfixList = []\n    tokenList = infixexpr.split()\n\n    for token in tokenList:\n        if token in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" or token in \"0123456789\":\n            postfixList.append(token)\n        elif token == '(':\n            opStack.push(token)\n        elif token == ')':\n            topToken = opStack.pop()\n            while topToken != '(':\n                postfixList.append(topToken)\n                topToken = opStack.pop()\n        else:\n            while (not opStack.isEmpty()) and \\\n               (prec[opStack.peek()] >= prec[token]):\n                  postfixList.append(opStack.pop())\n            opStack.push(token)\n\n    while not opStack.isEmpty():\n        postfixList.append(opStack.pop())\n    return \" \".join(postfixList)\n\nprint(infixToPostfix(\"A * B + C * D\"))\nprint(infixToPostfix(\"( A + B ) * C - ( D - E ) * ( F + G )\"))\n```\n\n```\nA B * C D * +\nA B + C * D E - F G + * -\n```\n\n## 后缀表达式求值\n\n```python\ndef postfixEval(postfixExpr):\n    operandStack = Stack()\n    tokenList = postfixExpr.split()\n\n    for token in tokenList:\n        if token in \"0123456789\":\n            operandStack.push(int(token))\n        else:\n            operand2 = operandStack.pop()\n            operand1 = operandStack.pop()\n            result = doMath(token,operand1,operand2)\n            operandStack.push(result)\n    return operandStack.pop()\n\ndef doMath(op, op1, op2):\n    if op == \"*\":\n        return op1 * op2\n    elif op == \"/\":\n        return op1 / op2\n    elif op == \"+\":\n        return op1 + op2\n    else:\n        return op1 - op2\n\nprint(postfixEval('7 8 + 3 2 + /'))\n```\n\n```\n3.0\n```\n\n\n","tags":["算法"]},{"title":"计算函数运行效率","url":"/2018/10/21/计算函数运行效率/","content":"计算函数时间\n\n```python\n\n```\n\n要捕获我们的每个函数执行所需的时间，我们将使用 Python 的 timeit 模块。timeit 模块旨在允许 Python 开发人员通过在一致的环境中运行函数并使用尽可能相似的操作系统的时序机制来进行跨平台时序测量。\n要使用 timeit，你需要创建一个 Timer 对象，其参数是两个 Python 语句。第一个参数是一个你想要执行时间的 Python 语句; 第二个参数是一个将运行一次以设置测试的语句。然后 timeit 模块将计算执行语句所需的时间。默认情况下，timeit 将尝试运行语句一百万次。 当它完成时，它返回时间作为表示总秒数的浮点值。由于它执行语句一百万次，可以读取结果作为执行测试一次的微秒数。你还可以传递 timeit 一个参数名字为 number，允许你指定执行测试语句的次数。以下显示了运行我们的每个测试功能 1000 次需要多长时间。\n\n```python\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndef test1():\n    l = []\n    for i in range(1000):\n        l = l + [i]\n\ndef test2():\n    l = []\n    for i in range(1000):\n        l.append(i)\n\ndef test3():\n    l = [i for i in range(1000)]\n\ndef test4():\n    l = list(range(1000))\n```\n\n```python\nimport timeit\nfrom timeit import Timer\nt1 = Timer(\"test1()\", \"from __main__ import test1\")\nprint(\"concat \",t1.timeit(number=1000), \"milliseconds\")\nt2 = Timer(\"test2()\", \"from __main__ import test2\")\nprint(\"append \",t2.timeit(number=1000), \"milliseconds\")\nt3 = Timer(\"test3()\", \"from __main__ import test3\")\nprint(\"comprehension \",t3.timeit(number=1000), \"milliseconds\")\nt4 = Timer(\"test4()\", \"from __main__ import test4\")\nprint(\"list range \",t4.timeit(number=1000), \"milliseconds\")\n\n\n```\n\n```\nconcat  1.0196415490549953 milliseconds\nappend  0.08268737967887319 milliseconds\ncomprehension  0.03390247851893946 milliseconds\nlist range  0.013240015113296977 milliseconds\n```\n\n作为一种演示性能差异的方法，我们用 timeit 来做一个实验。我们的目标是验证从列表从末尾 pop 元素和从开始 pop 元b素的性能。同样，我们也想测量不同列表大小对这个时间的影响。我们期望看到的是，从列表末尾处弹出所需时间将保持不变，即使列表不断增长。而从列表开始处弹出元素时间将随列表增长而增加。\n\n\n\n```python\npopzero = Timer(\"x.pop(0)\",\n                \"from __main__ import x\")\npopend = Timer(\"x.pop()\",\n               \"from __main__ import x\")\ndata=[]\nprint(\"pop(0)   pop()\")\nfor i in range(1000000,100000001,1000000):\n    x = list(range(i))\n    pt = popend.timeit(number=1000)\n    x = list(range(i))\n    pz = popzero.timeit(number=1000)\n    print(\"%15.5f, %15.5f\" %(pz,pt))\n    one_data={}\n    one_data['time']=i\n    one_data['pop(0)']=pz\n    one_data['pop()']=pt\n    data.append(one_data)\n    \n```\n\n```\npop(0)   pop()\n        0.56428,         0.00026\n        1.46132,         0.00009\n        2.27953,         0.00007\n        3.11111,         0.00025\n        4.03941,         0.00007\n        5.01496,         0.00011\n        5.72766,         0.00008\n        6.41760,         0.00007\n        6.98101,         0.00016\n        7.69488,         0.00009\n        8.59127,         0.00008\n        9.39142,         0.00008\n       10.03456,         0.00007\n       10.85724,         0.00011\n       11.69828,         0.00008\n       12.37023,         0.00008\n       13.12709,         0.00010\n       13.89727,         0.00008\n       14.65594,         0.00007\n       15.49640,         0.00008\n       16.35027,         0.00008\n       17.00838,         0.00010\n       17.72517,         0.00008\n       19.29465,         0.00010\n       21.02476,         0.00008\n       21.60967,         0.00008\n       22.32767,         0.00008\n       22.62550,         0.00008\n       25.15527,         0.00009\n       24.14118,         0.00008\n       25.48768,         0.00009\n       24.84720,         0.00007\n       26.40644,         0.00008\n       26.62662,         0.00008\n       28.86800,         0.00008\n       27.79692,         0.00008\n\n```\n\n将比较列表和字典之间的 contains 操作的性能。在此过程中，我们将确认列表的 contains 操作符是 O(n)，字典的 contains 操作符是 O(1)。我们将在实验中列出一系列数字。然后随机选择数字，并检查数字是否在列表中。如果我们的性能表是正确的，列表越大，确定列表中是否包含任意一个数字应该花费的时间越长。\n\n```python\nimport timeit\nimport random\ndata=[]\nfor i in range(10000,1000001,20000):\n    t = timeit.Timer(\"random.randrange(%d) in x\"%i,\n                     \"from __main__ import random,x\")\n    x = list(range(i))\n    lst_time = t.timeit(number=1000)\n    x = {j:None for j in range(i)}\n    d_time = t.timeit(number=1000)\n    print(\"%d,%10.3f,%10.3f\" % (i, lst_time, d_time))\n    one_data={}\n    one_data['time']=i\n    one_data['lis_contains']=lst_time\n    one_data['dict_contains']=d_time\n    data.append(one_data)\ndata\n```\n\n```\n10000,     0.075,     0.001\n30000,     0.149,     0.001\n50000,     0.235,     0.001\n70000,     0.353,     0.001\n90000,     0.449,     0.001\n110000,     0.526,     0.001\n130000,     0.614,     0.001\n\n\n```\n\n\n\n\n\n```\n\n```\n\n\n\n```python\nimport pandas as pd \nrow_data=pd.DataFrame.from_dict(data=data)\nrow_data.head()\n```\n\n\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dict_contains</th>\n      <th>lis_contains</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000865</td>\n      <td>0.074577</td>\n      <td>10000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000848</td>\n      <td>0.149025</td>\n      <td>30000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000817</td>\n      <td>0.235177</td>\n      <td>50000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.001056</td>\n      <td>0.352916</td>\n      <td>70000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000893</td>\n      <td>0.448741</td>\n      <td>90000</td>\n    </tr>\n  </tbody>\n</table>\n\n\n\n```python\npd.pivot_table(data=row_data,index='time').plot()\n```\n\n\n\n```\n<matplotlib.axes._subplots.AxesSubplot at 0x1a28403b4a8>\n```\n\n\n\n![png](/image/output_10_1.png)\n\n\n","tags":["算法"]},{"title":"skill_in_python","url":"/2018/10/20/skill-in-python/","content":"# python编程技巧\n\n\n\n## 可变类型和不可变类型\n\nPython提供两种内置或用户定义的类型。可变类型允许内容的内部修改。典型的动态类型 包括列表与字典：列表都有可变方法，如 `list.append()` 和 `list.pop()`， 并且能就地修改。字典也是一样。不可变类型没有修改自身内容的方法。比如，赋值为整数 6的变量 x 并没有 \"自增\" 方法，如果需要计算 x + 1，必须创建另一个整数变量并给其命名。\n\n这种差异导致的一个后果就是，可变类型是不 '稳定 '的，因而不能作为字典的键使用。合理地 使用可变类型与不可变类型有助于阐明代码的意图。例如与列表相似的不可变类型是元组， 创建方式为 `(1, 2)`。元组是不可修改的，并能作为字典的键使用。\n\nPython 中一个可能会让初学者惊讶的特性是：字符串是不可变类型。这意味着当需要组合一个 字符串时，将每一部分放到一个可变列表里，使用字符串时再组合 ('join') 起来的做法更高效。 值得注意的是，使用列表推导的构造方式比在循环中调用 `append()` 来构造列表更好也更快。\n\n差\n\n```\n# 创建将0到19连接起来的字符串 (例 \"012..1819\")\nnums = \"\"\nfor n in range(20):\n    nums += str(n)   # 慢且低效\nprint nums\n```\n\n好\n\n```\n# 创建将0到19连接起来的字符串 (例 \"012..1819\")\nnums = []\nfor n in range(20):\n    nums.append(str(n))\nprint \"\".join(nums)  # 更高效\n```\n\n## 避免对不同类型的对象使用同一个变量名\n\n差\n\n```\na = 1\na = 'a string'\ndef a():\n    pass  # 实现代码\n```\n\n好\n\n```\ncount = 1\nmsg = 'a string'\ndef func():\n    pass  # 实现代码\n```\n\n更好\n\n```\n# 创建将0到19连接起来的字符串 (例 \"012..1819\")\nnums = [str(n) for n in range(20)]\nprint \"\".join(nums)\n```\n\n极佳\n\n```\n# 创建将0到19连接起来的字符串 (例 \"012..1819\")\nnums = map(str, range(20))\nprint \"\".join(nums)\n```\n\n\n\n最后关于字符串的说明的一点是，使用 `join()` 并不总是最好的选择。比如当用预先 确定数量的字符串创建一个新的字符串时，使用加法操作符确实更快，但在上文提到的情况 下或添加到已存在字符串的情况下，使用 `join()` 是更好\n\n```\nfoo = 'foo'\nbar = 'bar'\n\nfoobar = foo + bar  # 好的做法\nfoo += 'ooo'  # 不好的做法, 应该这么做:\nfoo = ''.join([foo, 'ooo'])\n```\n\n除了 [`str.join()`](https://docs.python.org/3/library/stdtypes.html#str.join) 和 `+`，您也可以使用 [%](https://docs.python.org/3/library/string.html#string-formatting) 格式运算符来连接确定数量的字符串，但 [**PEP 3101**](https://www.python.org/dev/peps/pep-3101) 建议使用 [`str.format()`](https://docs.python.org/3/library/stdtypes.html#str.format) 替代 `%` 操作符。\n\n```\nfoo = 'foo'\nbar = 'bar'\n\nfoobar = '%s%s' % (foo, bar) # 可行\nfoobar = '{0}{1}'.format(foo, bar) # 更好\nfoobar = '{foo}{bar}'.format(foo=foo, bar=bar) # 最好\n```\n\n使用简短的函数或方法能降低对不相关对象使用同一个名称的风险。即使是相关的不同 类型的对象，也更建议\n\n使用不同命名：\n\n```\nitems = 'a b c d'  # 首先指向字符串...\nitems = items.split(' ')  # ...变为列表\nitems = set(items)  # ...再变为集合\n```\n\n重复使用命名对效率并没有提升：赋值时无论如何都要创建新的对象。然而随着复杂度的 提升，赋值语句被其他代码包括 'if' 分支和循环分开，使得更难查明指定变量的类型。 在某些代码的做法中，例如函数编程，推荐的是从不重复对同一个变量命名赋值。Java 内的实现方式是使用 'final' 关键字。Python并没有 'final' 关键字而且这与它的哲学 相悖。尽管如此，避免给同一个变量命名重复赋值仍是是个好的做法，并且有助于掌握 可变与不可变类型的概念。\n\n## 单行描述单行代码\n\n每一行一个语句，尤其在复杂的逻辑表达式的时候，这样会清晰很容易阅读。\n\n差\n\n```\nprint \"one\";print \"two\"\nif x == 1;print \"one\"\n```\n\n好\n\n```\nprint \"one\"\nprint \"two\"\nif x == 1:\n\tprint \"one\"\n\t\n```\n\n\n\n## 技巧\n\n使用enumerate() 将列表中的每个项提供两个元素的元组，一个是下标，一个是值。\n\n\n\n```\nfor index , iteme in enumerate(sorted_list):\n\tprint(index,iteme)\n```\n\n交换变量\n\n```\na,b=b,a\n```\n\n## **访问字典元素**\n\n不要使用该dict.has_key()方法。相反使用语法或传递默认参数 比如x in dict ，dict.get(k,default_value)\n\n差\n\n```\nd = {“hello”: \"world\"}\nif d.has_key(\"hello\"):\n\tprint(d['hello'])\nelse:\n\tprint(\"fault values\")\n```\n\n好\n\n\n\n```\nd = {“hello”: \"world\"}\nprint(d.get(\"hello\"),\"fault vluse\")\n\n# or\nif \"hello\" in d:\n\tprint(d[“hello”])\n```\n\n\n","tags":["python"]},{"title":"shadowsocks_config","url":"/2018/10/20/shadowsocks-config/","content":"# 搭建shadowsocks server 和 client端\n\n## sever 端\n\n```\nsudo apt update\nsudo apt install shadowsocks\n```\n\n下载完成后，配置shadowsocks 文件，\n\n```\nvim /etc/shadowsocks/config.json\n```\n\n```\n{\n    \"server\":\"ethan2lee.online\", #you server ip\n    \"server_port\":8888,\n    \"local_address\": \"127.0.0.1\",\n    \"local_port\":1080,\n    \"password\":\"your_password\",\n    \"timeout\":300,\n    \"method\":\"aes-256-cfb\"\n}\n```\n\n一般来说，我们只需要更改“server”和“password”字段的值即可，根据自己实际情况配置。更改完后，保存。\n\n接下来就是启动server端了，先设置开机自启，再启动。\n\n```\n➜  ~ sudo systemctl enable  shadowsocks.service\n\nshadowsocks.service is not a native service, redirecting to systemd-sysv-install\nExecuting /lib/systemd/systemd-sysv-install enable shadowsocks\n```\n\n\n\n```\n➜  ~ sudo systemctl start   shadowsocks.service\n```\n\n\n\n查看服务状态：\n\n```\n➜  ~ sudo systemctl status  shadowsocks.service\n● shadowsocks.service - LSB: Fast tunnel proxy that helps you bypass firewalls\n   Loaded: loaded (/etc/init.d/shadowsocks; bad; vendor preset: enabled)\n   Active: active (exited) since Sat 2018-10-20 04:59:16 UTC; 4s ago\n​     Docs: man:systemd-sysv-generator(8)\n  Process: 634 ExecStart=/etc/init.d/shadowsocks start (code=exited, status=0/SUCCESS)\n​    Tasks: 0\n   Memory: 0B\n​      CPU: 0\n\nOct 20 04:59:16 ethan systemd[1]: Starting LSB: Fast tunnel proxy that helps you bypass firewalls...\nOct 20 04:59:16 ethan systemd[1]: Started LSB: Fast tunnel proxy that helps you bypass firewalls.\n```\n\n这样我们shadowsocks的server端就配置完成了。\n\n## client端\n\n在需要用shadowsocks的server服务的机子也安装shaodowsocks。\n\n```\nsudo apt update\nsudo apt install shadowsocks\n```\n\n\n\n\n\n同样需要配置一下shadowsocks的配置文件，一般跟server端的配置差不多，但是需要自行更改“local_port”的字段值。\n\n```\nvim /etc/shadowsocks/config.json\n```\n\n\n\n```\n{\n    \"server\":\"ethan2lee.online\", #you server ip\n    \"server_port\":8888,\n    \"local_address\": \"127.0.0.1\",\n    \"local_port\":1080, ### 在clien端可以自主配置的\n    \"password\":\"your_password\",\n    \"timeout\":300,\n    \"method\":\"aes-256-cfb\"\n}\n```\n\n配置完成之手进行保存。\n\n开启client 端，这样我们只要使用sock5协议通过代理端口1080访问网络，就可以完全使用server的网络环境了。\n\n```\n➜  ~ sslocal -c /etc/shadowsocks/config.json\nINFO: loading config from /etc/shadowsocks/config.json\nshadowsocks 2.1.0\n2018-10-20 05:10:08 INFO     starting local at 127.0.0.1:1080\n```\n\n打开之后，shadowsocks的client 端是运行在前台的。我们先按Ctr +c 终止进行，再输入以下命令，让它在后台运行：\n\n```\n➜  ~ nohup sslocal -c /etc/shadowsocks/config.json &\n[1] 9479\nnohup: ignoring input and appending output to 'nohup.out'\n```\n\n这样它就作为守护进程在运行着了。\n\n## proxychains\n\n安装完shadowsocks之后，如果想让终端命令也经常代理，那么我们就需要proxychains来帮忙了。\n\n安装proxychains\n\n```\nsudo apt install proxychains\n```\n\n修改proxychains 配置文件，直接找到最后一行。\n\n```\n➜  ~ vim /etc/proxychains.conf\n```\n\n将`socks4 127.0.0.1 9095`改为\n\n```\nsocks5 127.0.0.1 1080 \n```\n\n1080 为你刚刚配置shadowsocks client端的“local_host”字段的值，配置完成后保存。\n\n\n\n\n\n经过以上所有步骤，我们可以通过一下命令来查看我们是否配置正确了：\n\n\n\n```\nproxychains wget www.google.com\n```\n\n如果有一下输出，那么就证明你配置成功了：\n\n```\n➜  ~ proxychains wget www.google.com\nProxyChains-3.1 (http://proxychains.sf.net)\n--2018-10-20 05:28:01--  http://www.google.com/\nResolving www.google.com (www.google.com)... |DNS-request| www.google.com\n|S-chain|-<>-127.0.0.1:1080-<><>-4.2.2.2:53-<><>-OK\n|DNS-response| www.google.com is 74.125.20.103\n74.125.20.103\nConnecting to www.google.com (www.google.com)|74.125.20.103|:80... |S-chain|-<>-127.0.0.1:1080-<><>-74.125.20.103:80-<><>-OK\nconnected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: ‘index.html’\n\nindex.html                        [ <=>                                           ]  11.05K  --.-KB/s    in 0s\n\n2018-10-20 05:28:02 (63.6 MB/s) - ‘index.html’ saved [11319]\n\n```\n\n\n\n如果没有，就只能继续百度了。\n","tags":["vpn"]},{"title":"更换ubuntu18源","url":"/2018/10/20/更换ubuntu18源/","content":"# Ubuntu 18.04换国内源 \n\n\n\n更换源前，先对本来的源文件做好备份。\n\n```\nmv /etc/apt/sources.list  /etc/apt/sourceslist-save\n```\n\n备份完成后，我们就可再建立source.list\n\n```\nvim /etc/apt/sources.list \n```\n\n添加以下内容\n\n```\n##中科大源\n\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multivers\n```\n\n然后执行以下命令：\n\n```\nsudo apt update\nsudo apt upgrade\n```\n\n\n\n\n\n更改其他源：\n\n阿里源\n\n```\ndeb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\n```\n\n360源\n\n```\ndeb http://mirrors.163.com/ubuntu/ bionic main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiverse\n```\n\n清华源\n\n```\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\n```\n\n\n","tags":["linux"]},{"title":"统计量分析示例","url":"/2018/10/19/统计量分析示例/","content":"\n# \n\n# 描述性统计分析基础\n\n- 数据集描述与属性说明\n\n- ID 客户编号\n\n- Suc_flag   成功入网标识\n\n- ARPU   入网后ARPU\n\n- PromCnt12  12个月内的营销次数\n\n- PromCnt36  36个月内的营销次数\n\n- PromCntMsg12   12个月内发短信的次数\n\n- PromCntMsg36   36个月内发短信的次数\n\n- Class  客户重要性等级(根据前运营商消费情况)\n\n- Age    年龄\n\n- Gender 性别\n\n- HomeOwner  是否拥有住房\n\n- AvgARPU    当地平均ARPU\n\n- AvgHomeValue   当地房屋均价\n\n- AvgIncome  当地人均收入\n\n```\nimport os\nimport pandas as pd\nimport numpy as np\n\npd.set_option('display.max_columns', None)\n#os.chdir('Q:/data')\n#os.getcwd()\n```\n\n读取数据\n\n```\ncamp= pd.read_csv('teleco_camp.csv')\ncamp.head(10)\n```\n\n数据预处理\n\n```\ncamp.dtypes\n```\n\n```\nID                 int64\nSuc_flag        category\nARPU             float64\nPromCnt12        float64\nPromCnt36        float64\nPromCntMsg12     float64\nPromCntMsg36     float64\nClass           category\nAge              float64\nGender            object\nHomeOwner         object\nAvgARPU          float64\nAvgHomeValue     float64\nAvgIncome        float64\ndtype: object\n```\n\ncamp.describe(include='all')\n\n|        | ID            | Suc_flag | ARPU        | PromCnt12   | PromCnt36   | PromCntMsg12 | PromCntMsg36 | Class  | Age         | Gender | HomeOwner | AvgARPU     | AvgHomeValue  | AvgIncome     |\n| ------ | ------------- | -------- | ----------- | ----------- | ----------- | ------------ | ------------ | ------ | ----------- | ------ | --------- | ----------- | ------------- | ------------- |\n| count  | 9686.000000   | 9686.0   | 4843.000000 | 9686.000000 | 9686.000000 | 9686.000000  | 9686.000000  | 9686.0 | 7279.000000 | 9686   | 9686      | 9686.000000 | 9583.000000   | 7329.000000   |\n| unique | NaN           | 2.0      | NaN         | NaN         | NaN         | NaN          | NaN          | 4.0    | NaN         | 3      | 2         | NaN         | NaN           | NaN           |\n| top    | NaN           | 1.0      | NaN         | NaN         | NaN         | NaN          | NaN          | 2.0    | NaN         | F      | H         | NaN         | NaN           | NaN           |\n| freq   | NaN           | 4843.0   | NaN         | NaN         | NaN         | NaN          | NaN          | 3303.0 | NaN         | 5223   | 5377      | NaN         | NaN           | NaN           |\n| mean   | 97975.474086  | NaN      | 78.121722   | 3.447212    | 7.337059    | 1.178402     | 2.390935     | NaN    | 59.150845   | NaN    | NaN       | 52.905156   | 112179.202755 | 53513.457361  |\n| std    | 56550.171120  | NaN      | 62.225686   | 1.231890    | 1.952436    | 0.287226     | 0.914314     | NaN    | 16.516400   | NaN    | NaN       | 4.993775    | 98522.888583  | 19805.168339  |\n| min    | 12.000000     | NaN      | 5.000000    | 0.750000    | 1.000000    | 0.200000     | 0.400000     | NaN    | 0.000000    | NaN    | NaN       | 46.138968   | 7500.000000   | 2499.000000   |\n| 25%    | 48835.500000  | NaN      | 50.000000   | 2.900000    | 6.250000    | 1.000000     | 1.400000     | NaN    | 47.000000   | NaN    | NaN       | 49.760116   | 53200.000000  | 40389.000000  |\n| 50%    | 99106.000000  | NaN      | 65.000000   | 3.250000    | 7.750000    | 1.200000     | 2.600000     | NaN    | 60.000000   | NaN    | NaN       | 50.876672   | 77700.000000  | 48699.000000  |\n| 75%    | 148538.750000 | NaN      | 100.000000  | 3.650000    | 8.250000    | 1.400000     | 3.200000     | NaN    | 73.000000   | NaN    | NaN       | 54.452822   | 129350.000000 | 62385.000000  |\n| max    | 191779.000000 | NaN      | 1000.000000 | 15.150000   | 19.500000   | 3.600000     | 5.600000     | NaN    | 87.000000   | NaN    | NaN       | 99.444787   | 600000.000000 | 200001.000000 |\n\n\n\n\n\n\n# 描述性统计与探索型数据分析\n\n## 分类变量分析\n\n可以查看列原因元素的种类\n\n```\ncamp['Suc_flag'].groupby(camp['Suc_flag']).count()\n```\n\n```\nSuc_flag\n0    4843\n1    4843\nName: Suc_flag, dtype: int64\n```\n\n\n\n\n\n## 连续变量分析\n### 数据的集中趋势\n#### ARPU的均值与中位数\n```\n\nfs = camp['ARPU'] # 可以使用camp.ARPU \nprint('mean = %6.4f' %fs.mean())                     # 求fs的均值\nprint('median = %6.4f' %fs.median() )                # 求fs的中位数\nprint('quantiles\\n', fs.quantile([0.25, 0.5, 0.75])) # 求a的上下四分位数与中位数\n```\n\n```\nmad = 38.1896\nrange = 995.0000\nvar = 3872.0359\nstd = 62.2257\n```\n\n\n\n```\nget_ipython().run_line_magic('matplotlib', 'inline'）\n\nfs.plot(kind='hist')\n```\n\n```\n(array([2.510e+03, 1.978e+03, 2.160e+02, 9.800e+01, 4.000e+00, 7.000e+00,\n        0.000e+00, 2.500e+01, 0.000e+00, 0.000e+00, 0.000e+00, 1.000e+00,\n        0.000e+00, 0.000e+00, 4.000e+00]),\n array([   5.        ,   71.33333333,  137.66666667,  204.        ,\n         270.33333333,  336.66666667,  403.        ,  469.33333333,\n         535.66666667,  602.        ,  668.33333333,  734.66666667,\n         801.        ,  867.33333333,  933.66666667, 1000.        ]),\n <a list of 15 Patch objects>)\n```\n\n![](image/output_11_1.png)\n\n\n### 数据的离散程度\n\n```\nprint ('mad = %6.4f' %fs.mad())      # 求平均绝对偏差 mad = np.abs(fs - fs.mean()).mean()\nprint ('range = %6.4f' %(fs.max(skipna=True) - fs.min(skipna=True))) # 求极差\nprint ('var = %6.4f' %fs.var())   # 求方差\nprint ('std = %6.4f' %fs.std())   # 求标准差\n```\n\n\n\n\n\n### 数据的偏度与峰度\n```\nimport matplotlib.pyplot as plt\n\nplt.hist(fs.dropna(), bins=15)\n```\n\n![](image/output_15_1.png)\n\n```\nprint ('skewness = %6.4f' %fs.skew(skipna=True))\nprint ('kurtosis = %6.4f' %fs.kurt(skipna=True))\n```\n\n```\nskewness = 5.1695\nkurtosis = 52.8509\n```\n\n\n### apply\\map\\groupby及其它相关\n\n```\ndata = pd.DataFrame(data={'a':range(1,11), 'b':np.random.randn(10)})\ndata.T\n```\n\n|      | 0         | 1        | 2        | 3        | 4        | 5         | 6        | 7       | 8        | 9         |\n| ---- | --------- | -------- | -------- | -------- | -------- | --------- | -------- | ------- | -------- | --------- |\n| a    | 1.000000  | 2.000000 | 3.000000 | 4.000000 | 5.000000 | 6.000000  | 7.000000 | 8.0000  | 9.000000 | 10.000000 |\n| b    | -0.087919 | 0.903531 | 0.603965 | 0.203005 | 0.282077 | -1.420298 | 0.283303 | -0.0565 | 1.047595 | -0.787566 |\n\n```\ndata.apply(np.mean) # 等价于data.mean()，是其完整形式\n```\n\n```\na    5.500000\nb    0.097119\ndtype: float64\n```\n\n```\ndata.apply(lambda x: x.astype('str')).dtypes # DataFrame没有astype方法，只有Series有\n```\n","tags":["数据分析"]},{"title":"data_clearning","url":"/2018/10/17/data-clearning/","content":"# 数据清洗\n\n> 数据清洗， 是整个数据分析过程中不可缺少的一个环节，其结果质量直接关系到模型效果和最终结论。在实际操作中，数据清洗通常会占据分析过程的50%—80%的时间。\n\n## 去除重复数据\n\n在获取到的数据中，我们发现会有重复数据。我们使用Pandas 提供的方法 duplicated 和 drop_duplicates来去重。\n\n\n\n```\nsample =  pd.DataFrame({'id':[1,1,1,3,4,5],\n                        'name':['Bob','Bob','Mark','Miki','Sully','Rose'],\n                        'score':[99,99,87,77,77,np.nan],\n                        'group':[1,1,1,2,1,2]})\nsample\nOut[85]: \n   id   name  score  group\n0   1    Bob   99.0      1\n1   1    Bob   99.0      1\n2   1   Mark   87.0      1\n3   3   Miki   77.0      2\n4   4  Sully   77.0      1\n5   5   Rose    NaN      2\n\n```\n\n查找重复数据：\n\n```\nsample[sample.duplicated()]\nOut[86]: \n   id name  score  group\n1   1  Bob   99.0      1\n```\n\n需要去重时：\n\n```\nsample.drop_duplicates()\nOut[87]: \n   id   name  score  group\n0   1    Bob   99.0      1\n2   1   Mark   87.0      1\n3   3   Miki   77.0      2\n4   4  Sully   77.0      1\n5   5   Rose    NaN      2\n```\n\n按列去重时，需要加入列索引：\n\n```\nsample.drop_duplicates('id')\nOut[88]: \n   id   name  score  group\n0   1    Bob   99.0      1\n3   3   Miki   77.0      2\n4   4  Sully   77.0      1\n5   5   Rose    NaN      2\n```\n\n## 缺失值处理\n\n在数据挖掘中，面对数据中存在缺失值时，我们一般采取以下几种办法：\n\n1. 缺失值较多的特征处理\n\n当缺失值处于20%~80%，每个缺失值可以生成一个指示哑变量，参与后续的建模。\n\n2.缺失较少时\n\n首先需要根据业务理解处理缺失值，弄清楚缺失值产生的原因，是故意缺失还是随机缺失。可以依靠业务经验进行填补。连续变量可以使用均值或中位数进行填补。\n\n- 把 NaN 直接作为一个特征，假设0表示\n\n```\ndf.fillna(0) \n```\n\n\n\n- 用均值填充\n\n```\n# 将所有行用各自的均值填充 \ndf.fillna(df.mean())  \n# 将所有行用各自的均值填充 \ndf.fillna(df.mean()['collum_name])\n```\n\n\n\n- 用上下数据填充\n\n```\n# 用前一个数据替代NaN\ndf.fillnan(method=\"pad\")\n\n# 与pad相反，bfill表示用后一个数据代替NaN\ndf.fillna(method=)\n```\n\n\n\n- 用插值填充\n\n```\n# 插值法就是通过两点（x0,y0）,(x1,y1)估计中间点的值\ndf.interpolate() \n```\n\n\n\n\n\n- 查看数据值缺失情况，我们可有构造一个lambda 函数来查看缺失值。\n\n```\nsample =  pd.DataFrame({'id':[1,1,1,3,4,np.nan],\n                        'name':['Bob','Bob','Mark','Miki','Sully',np.nan],\n                        'score':[99,99,87,77,77,np.nan],\n                        'group':[1,1,1,2,1,np.nan]})\nsample\nsample.apply(lambda col:sum(col.isnull())/col.size)\nOut[91]: \nid       0.166667\nname     0.166667\nscore    0.166667\ngroup    0.166667\ndtype: float64\n```\n\n### 已指定值填补\n\n均值\n\n```\nsample.score.fillna(sample.score.mean())\nOut[93]: \n0    99.0\n1    99.0\n2    87.0\n3    77.0\n4    77.0\n5    87.8\nName: score, dtype: float64\n```\n\n中位数\n\n```\nsample.score.fillna(sample.score.median())\nOut[94]: \n0    99.0\n1    99.0\n2    87.0\n3    77.0\n4    77.0\n5    87.0\nName: score, dtype: float64\n```\n\n### 缺失值指示变量\n\nPanda DataFrame 对象可以直接调用方法isnull 产生缺失值指示变量，例如产生score变量的缺失值变量：\n\n```\n# 指示变量\nsample.score.isnull()\nOut[95]: \n0    False\n1    False\n2    False\n3    False\n4    False\n5     True\nName: score, dtype: bool\n```\n\n若想转化为数据0，1型指示变量，可以使用apply方法，int表示将该列替换为 int 类型：\n\n```\nsample.score.isnull().apply(int)\nOut[97]: \n0    0\n1    0\n2    0\n3    0\n4    0\n5    1\n```\n\n\n\n\n\n## 噪声处理\n\n噪声值是指数据中有一个或多个数据与其他数值相比差异性比较大，又称异常值，离群值。\n\n对于单变量，我们可以采用盖帽法，分箱法；\n\n对于多变量，我们可以采用就聚类。\n\n### 盖帽法\n\n盖帽法将连续变量均值上下三倍标准差范围外的记录替换为均值上下三倍标准差值。\n\n```\n\ndef cap(df,quantile=[0.01,0.99]):\n    \"\"\"\n    盖帽法处理异常值\n    :param df: pd.Series 列，连续变量\n    :param quantile: 指定盖帽法的上下分位数范围\n    :return: \n    \"\"\"\n    Q01 ,Q99 = df.quantile(quantile).values.tolist() # 生成分位数\n    # 替代异常值\n    if Q01 > df.min():\n        x = df.copy()\n        x.loc[x<Q01] = Q01\n    if Q99 < df.max():\n        x = df.copy()\n        x.loc[x > Q99] = Q99\n    return(x)\n```\n\n生成一组服从正态分布的随机数，sample.hsit 为直方图。\n\n```\nimport matplotlib.pyplot as plt\nsample = pd.DataFrame({'normall':np.random.randn(1000)})\nsample.hist(bins =50)\nplt.show()\n```\n\n![处理前](\\image\\before_handle.png)\n\n\n\n对 sample 数据所有列进行盖帽法转换，，下图可以看出盖帽后极端值频数的变化。\n\n```\nnew =sample.apply(cap,quantile=[0.001,0.99])\nnew.hist(bins=50)\nplt.show()\n```\n\n![](\\image\\after_handle.png)\n\n\n\n\n\n### 分箱法\n\n分箱法通过考察数据的\"近邻\"来光滑有序数据的值。有序值分布到一些桶或箱中。\n\n深分箱，即每个分箱中的样本量一致；\n\n等宽分箱，即每个分箱中的取值范围一致。直方图就是首先对数据进行了等宽分箱，再计算频数画图。\n\n分箱法可以将异常数据包含再箱子中，在进行建模的时候，不直接进行到模型中，因而可以达到处理异常值的目的。\n\n```\n# 生成10个标准正态分布的随机数\nsample = pd.DataFrame({\"normal\":np.random.randn(10)})\nsample\nOut[118]: \n     normal\n0 -0.028929\n1  0.327508\n2 -0.596384\n3 -2.036334\n4  1.452605\n5 -0.403936\n6  0.315138\n7  0.252127\n8 -0.775113\n9  0.171641\n```\n\n#### 等宽分箱\n\n现将sample 按照宽度分位5份，下限中，cut 函数自动选择小于列最小值的一个数值未下限，最大值为上限，等分为5份。\n\n```\npd.cut(sample.normal,5)\nOut[119]: \n0     (-0.641, 0.057]\n1      (0.057, 0.755]\n2     (-0.641, 0.057]\n3     (-2.04, -1.339]\n4      (0.755, 1.453]\n5     (-0.641, 0.057]\n6      (0.057, 0.755]\n7      (0.057, 0.755]\n8    (-1.339, -0.641]\n9      (0.057, 0.755]\nName: normal, dtype: category\nCategories (5, interval[float64]): [(-2.04, -1.339] < (-1.339, -0.641] < (-0.641, 0.057] <\n                                    (0.057, 0.755] < (0.755, 1.453]]\n\n```\n\n使用labels参数指定分箱后的各个水平的标签，\n\n```\npd.cut(sample.normal,bins=5,labels=[1,2,3,4,5])\nOut[120]: \n0    3\n1    4\n2    3\n3    1\n4    5\n5    3\n6    4\n7    4\n8    2\n9    4\nName: normal, dtype: category\nCategories (5, int64): [1 < 2 < 3 < 4 < 5]\n```\n\n### 等深分箱\n\n等深分箱中，各个箱的宽度可能不一，但频数是几乎相等，所以可以采用数据的分位数来分箱。现对sample数据进行等深度分二箱，首先要找到2箱的分位数：\n\n```\nsample.normal.quantile([0,0.5,1])\nOut[121]: \n0.0   -2.036334\n0.5    0.071356\n1.0    1.452605\nName: normal, dtype: float64\n```\n\n在bins参数中设定分位数区间，为将下边界包含，include_lowest= True，完成等深分箱：\n\n```\npd.cut(sample.normal,bins=sample.normal.quantile([0,0.5,1]),include_lowest=True)\nOut[124]: \n0    (-2.037, 0.0714]\n1     (0.0714, 1.453]\n2    (-2.037, 0.0714]\n3    (-2.037, 0.0714]\n4     (0.0714, 1.453]\n5    (-2.037, 0.0714]\n6     (0.0714, 1.453]\n7     (0.0714, 1.453]\n8    (-2.037, 0.0714]\n9     (0.0714, 1.453]\nName: normal, dtype: category\nCategories (2, interval[float64]): [(-2.037, 0.0714] < (0.0714, 1.453]]\n```\n\n可以对分组进行标签化。\n\n```\npd.cut(sample.normal,bins=sample.normal.quantile([0,0.5,1]),include_lowest=True,labels=['bad','good'])\nOut[125]: \n0     bad\n1    good\n2     bad\n3     bad\n4    good\n5     bad\n6    good\n7    good\n8     bad\n9    good\nName: normal, dtype: category\nCategories (2, object): [bad < good]\n\n```\n\n\n\n\n\n### 多变量异常值处理-聚类法\n\n通过快速聚类法将数据对象分组成为多个簇，在同一个簇中的对象具有较高的相似度，而不同簇之间的对象差别较大。聚类分析可以挖掘孤立点以发现噪声数据，因为噪声本身就是孤立点。\n","tags":["数据清洗"],"categories":["数据挖掘"]},{"title":"RFM模型分析用户行为","url":"/2018/10/17/RFM模型分析用户行为/","content":"# RFM模型分析用户行为\n\n根据美国数据库营销研究所Arthur Hughes的研究，客户数据库中有三个神奇的要素，这三个要素构成了数据分析最好的指标：最近一次消费(Recency)、消费频率(Frequency)、消费金额(Monetary)。\n\nRFM模型：\n\n- R(Recency)表示客户最近一次购买的时间有多远，对消费时间越近的客户，提供即时的商品或服务也最有可能有所反应。\n\n- F(Frequency)表示客户在最近一段时间内购买的次数，经常买的客户也是满意度最高的客户。\n\n- M  (Monetary)表示客户在最近一段时间内购买的金额，消费金额是最近消费的平均金额，是体现客户短期价值的中重要变量。如果预算不多，那么我们酒的将服务信息提供给收入贡献较高的那些人。\n\n一般原始数据为3个字段：客户ID、购买时间（日期格式）、购买金额，用数据挖掘软件处理，加权（考虑权重）得到RFM得分，进而可以进行客户细分，客户等级分类，Customer Level Value得分排序等，实现数据库营销！\n\n![](\\image\\RFM.png)\n\n\n\n（编号次序RFM,1代表高，0代表低）\n\n重要价值客户（111）：最近消费时间近、消费频次和消费金额都很高，必须是VIP啊！\n\n重要保持客户（011）：最近消费时间较远，但消费频次和金额都很高，说明这是个一段时间没来的忠实客户，我们需要主动和他保持联系。\n\n重要发展客户（101）：最近消费时间较近、消费金额高，但频次不高，忠诚度不高，很有潜力的用户，必须重点发展。\n\n重要挽留客户（001）：最近消费时间较远、消费频次不高，但消费金额高的用户，可能是将要流失或者已经要流失的用户，应当基于挽留措施。\n\nRFM模型的应用在于建立一个用户行为报告，这个报告会成为维系顾客的一个重要指标。\n\n\n\n现在我们以某淘宝店家做客户激活为案例，[RFM_TRAD_FLOW.csv](/data/RFM_TRAD_FLOW.csv) 为某段时间内客户消费记录\n\n```\nimport pandas as pd\ntrad_flow = pd.read_csv('data/RFM_TRAD_FLOW.csv', encoding='gbk')\ntrad_flow.head(10)\n```\n\n数据部分展示：\n\n| transID | cumid | time | amount | type_label | type |\n| ------- | ----- | ---- | ------ | ---------- | ---- |\n|9407|\t10001|\t14JUN09:17:58:34|\t199\t|正常|\tNormal|\n|9625\t|10001\t|16JUN09:15:09:13\t|369\t|正常\t|Normal|\n|11837\t|10001|\t01JUL09:14:50:36\t|369\t|正常|\tNormal|\n|26629\t|10001\t|14DEC09:18:05:32\t|359\t|正常\t|Normal|\n|30850|\t10001\t|12APR10:13:02:20|\t399|\t正常|\tNormal|\n|32007\t|10001|\t04MAY10:16:45:58|\t269\t|正常|\tNormal|\n|36637\t|10001\t|04JUN10:20:03:06|\t0\t|赠送|\tPresented|\n|43108\t|10001|\t06JUL10:16:56:40|\t381\t|正常|\tNormal|\n\n1. 计算F 反应顾客对打折的偏好程度\n\n通过计算F反应客户对打折产品的偏好\n\n```\nF=trad_flow.groupby(['cumid','type'])[['transID']].count()\nF.head()\n```\n\n建立数据透视表\n\n```\nF_trans=pd.pivot_table(F,index='cumid',columns='type',values='transID')\nF_trans.head()\n```\n\n对缺失的数据填补为零\n\n```\nF_trans['Special_offer']= F_trans['Special_offer'].fillna(0)\nF_trans.head()\n```\n\n计算兴趣用户比例\n\n```\nF_trans[\"interest\"]=F_trans['Special_offer']/(F_trans['Special_offer']+F_trans['Normal'])\nF_trans.head()\n```\n\n2. 计算M反应客户的价值信息\n\n通过计算M反应客户的价值信息\n\n```\nM=trad_flow.groupby(['cumid','type'])[['amount']].sum()\nM.head()\n```\n\n数据透视，缺失值补零，计算价值用户\n\n```\nM_trans=pd.pivot_table(M,index='cumid',columns='type',values='amount')\nM_trans['Special_offer']= M_trans['Special_offer'].fillna(0)\nM_trans['returned_goods']= M_trans['returned_goods'].fillna(0)\nM_trans[\"value\"]=M_trans['Normal']+M_trans['Special_offer']+M_trans['returned_goods']\nM_trans.head()\n```\n\n3. 通过计算R反应客户是否为沉默客户\n\n定义一个从文本转化为时间的函数\n\n```\nfrom datetime import datetime\nimport time\ndef to_time(t):\n    out_t=time.mktime(time.strptime(t, '%d%b%y:%H:%M:%S'))  \n    return out_t\t\t\n```\n\n将时间进行转化\n\n```\nrad_flow[\"time_new\"]= trad_flow.time.apply(to_time)\n```\n\n获取高频消费客户\n\n```\nR=trad_flow.groupby(['cumid'])[['time_new']].max()\nR.head()\n```\n\n4. 构建模型，筛选目标客户\n\n```\n# In[12]\nfrom sklearn import preprocessing\nthreshold = pd.qcut(F_trans['interest'], 2, retbins=True)[1][1]\nbinarizer = preprocessing.Binarizer(threshold=threshold)\ninterest_q = pd.DataFrame(binarizer.transform(F_trans['interest'].values.reshape(-1, 1)))\ninterest_q.index=F_trans.index\ninterest_q.columns=[\"interest\"]\n# In[12]\nthreshold = pd.qcut(M_trans['value'], 2, retbins=True)[1][1]\nbinarizer = preprocessing.Binarizer(threshold=threshold)\nvalue_q = pd.DataFrame(binarizer.transform(M_trans['value'].values.reshape(-1, 1)))\nvalue_q.index=M_trans.index\nvalue_q.columns=[\"value\"]\n# In[12]\nthreshold = pd.qcut(R[\"time_new\"], 2, retbins=True)[1][1]\nbinarizer = preprocessing.Binarizer(threshold=threshold)\ntime_new_q = pd.DataFrame(binarizer.transform(R[\"time_new\"].values.reshape(-1, 1)))\ntime_new_q.index=R.index\ntime_new_q.columns=[\"time\"]\n# In[12]\nanalysis=pd.concat([interest_q, value_q,time_new_q], axis=1)\n# In[12]\n#analysis['rank']=analysis.interest_q+analysis.interest_q\nanalysis = analysis[['interest','value','time']]\nanalysis.head()\n\nlabel = {\n    (0,0,0):'无兴趣-低价值-沉默',\n    (1,0,0):'有兴趣-低价值-沉默',\n    (1,0,1):'有兴趣-低价值-活跃',\n    (0,0,1):'无兴趣-低价值-活跃',\n    (0,1,0):'无兴趣-高价值-沉默',\n    (1,1,0):'有兴趣-高价值-沉默',\n    (1,1,1):'有兴趣-高价值-活跃',\n    (0,1,1):'无兴趣-高价值-活跃'\n}\nanalysis['label'] = analysis[['interest','value','time']].apply(lambda x: label[(x[0],x[1],x[2])], axis = 1)\nanalysis.head()\n```\n\n\n"},{"title":"markdown_picture","url":"/2018/10/16/markdown-picture/","content":"# 序列图\n\n``` sequence\ntitle: 序列图sequence(示例)\nparticipant A\nparticipant B\nparticipant C\n\nnote left of A: A左侧说明\nnote over B: 覆盖B的说明\nnote right of C: C右侧说明\n\nA->A:自己到自己\nA->B:实线实箭头\nA-->C:虚线实箭头\nB->>C:实线虚箭头\nB-->>A:虚线虚箭头\t\t\t\t\t\n```\n\n关键词:\n\n1. title, 定义该序列图的标题\n2. participant, 定义时序图中的对象\n3. note, 定义对时序图中的部分说明\n4. {actor}, 表示时序图中的具体对象（名称自定义）\n\n针对note的方位控制主要包含以下几种关键词：\n\n1. left of, 表示当前对象的左侧\n2. right of, 表示当前对象的右侧\n3. over, 表示覆盖在当前对象（们）的上面\n\n针对{actor}的箭头分为以下几种：\n\n1. -> 表示实线实箭头\n2. –> 表示虚线实箭头\n3. ->> 表示实线虚箭头\n4. –>> 表示虚线虚箭头\n\n# 流程图\n\n```flow\nst=>start: 开始\ne=>end: 结束\nop=>operation: 操作\nsub=>subroutine: 子程序\ncond=>condition: 是或者不是?\nio=>inputoutput: 输出\n\nst(right)->op->cond\ncond(yes)->io(right)->e\ncond(no)->sub(right)->op\n```\n\n- start,end, 表示程序的开始与结束\n- operation, 表示程序的处理块\n- subroutine, 表示子程序块\n- condition, 表示程序的条件判断\n- inputoutput, 表示程序的出入输出\n- right,left, 表示箭头在当前模块上的起点(默认箭头从下端开始)\n- yes,no, 表示condition判断的分支(其可以和right,left同时使用)\n\n模块定义(模块标识与模块名称可以任意定义名称,关键词不可随意取名)如下:\n\n```\n模块标识=>模块关键词: 模块名称\n```\n\n连接定义如下:\n\n```\n模块标识1->模块标识2\n模块标识1->模块标识2->模块标识3\n... ...\n```\n\n```flow\nst=>start: Start\ne=>end: End\nop1=>operation: My Operation\nop2=>operation: Stuff\nsub1=>subroutine: My Subroutine\ncond=>condition: Yes\nor No?\nc2=>condition: Good idea\nio=>inputoutput: catch something...\n\nst->op1(right)->cond\ncond(yes, right)->c2\ncond(no)->sub1(left)->op1\nc2(yes)->io->e\nc2(no)->op2->e\n```\n\n![Alt text](https://g.gravizo.com/svg?\n  digraph G {\n​    aize =\"4,4\";\n​    main [shape=box];\n​    main -> parse [weight=8];\n​    parse -> execute;\n​    main -> init [style=dotted];\n​    main -> cleanup;\n​    execute -> { make_string; printf}\n​    init -> make_string;\n​    edge [color=red];\n​    main -> printf [style=bold,label=\"100 times\"];\n​    make_string [label=\"make a string\"];\n​    node [shape=box,style=filled,color=\".7 .3 1.0\"];\n​    execute -> compare;\n  }\n)\n","tags":["markdown"]},{"title":"data_integration","url":"/2018/10/16/data-integration/","content":"# 数据整合\n\n## 行列操作\n\n```python\nimport pandas as pd\nimport numpy as np\nsample = pd.DataFrame(np.random.randn(4,5),columns=['a','b','c','d','e'])\nsample\n```\n\n```\nOut[2]: \n          a         b         c         d         e\n0 -0.776807  2.355071 -0.940921  0.164487 -1.025772\n1  0.596704  0.962625  1.848441 -1.122676 -0.359290\n2 -0.092755 -0.124250 -0.259899 -0.111997 -1.816197\n3  0.372941  0.297850 -0.409256  0.485376 -2.790929\n```\n\n### 选择单列\n\n```\nsample['a']\n```\n\n\n\n```\nOut[4]: \n0   -0.776807\n1    0.596704\n2   -0.092755\n3    0.372941\nName: a, dtype: float64\n```\n\n数据框的ix，iloc，ioc方法都可以选择行，列，iloc方法只能使用数值作为索引来选择行列，loc方法在选择时能使用字符串索引，ix方法则可以使用两种索引\n\n```\nsample.ix[:,'a']\n```\n\n```\nOut[5]: \n0   -0.776807\n1    0.596704\n2   -0.092755\n3    0.372941\nName: a, dtype: float64\n```\n\n或者单选列\n\n```\nsample[['a']]\nOut[6]: \n          a\n0 -0.776807\n1  0.596704\n2 -0.092755\n3  0.372941\n```\n\n\n\n### 选择多行多列\n```\nsample.ix[0:2,0:2]\n```\n\n```\nOut[7]: \n          a         b\n0 -0.776807  2.355071\n1  0.596704  0.962625\n2 -0.092755 -0.124250\n\n```\n\n```\nsample.iloc[0:2,0:2]\n\n```\n\n### 创建，删除列\n\n第一种方式\n\n```\nsample['new_col1']=sample['a']-sample['b']\nsample\n```\n\n```\nOut[10]: \n          a         b         c         d         e  new_col1\n0 -0.776807  2.355071 -0.940921  0.164487 -1.025772 -3.131877\n1  0.596704  0.962625  1.848441 -1.122676 -0.359290 -0.365921\n2 -0.092755 -0.124250 -0.259899 -0.111997 -1.816197  0.031495\n3  0.372941  0.297850 -0.409256  0.485376 -2.790929  0.075091\n```\n\n第二种方式\n\n```\nsample.assign(new_col2=sample['a']-sample['b'],new_col3=sample['a']+sample['b'])\n```\n\n```\nOut[11]: \n          a         b         c    ...     new_col1  new_col2  new_col3\n0 -0.776807  2.355071 -0.940921    ...    -3.131877 -3.131877  1.578264\n1  0.596704  0.962625  1.848441    ...    -0.365921 -0.365921  1.559329\n2 -0.092755 -0.124250 -0.259899    ...     0.031495  0.031495 -0.217004\n3  0.372941  0.297850 -0.409256    ...     0.075091  0.075091  0.670792\n[4 rows x 8 columns]\n```\n\n删除列，第一种方式\n\n```\nsample.drop('a',axis=1)\n```\n\n```\nOut[12]: \n          b         c         d         e  new_col1\n0  2.355071 -0.940921  0.164487 -1.025772 -3.131877\n1  0.962625  1.848441 -1.122676 -0.359290 -0.365921\n2 -0.124250 -0.259899 -0.111997 -1.816197  0.031495\n3  0.297850 -0.409256  0.485376 -2.790929  0.075091\n```\n\n第二种方式，\n\n```\n# In\nsample.drop(['a','b'],axis=1)\n```\n\n```\n      c         d         e  new_col1\n\n0 -0.940921  0.164487 -1.025772 -3.131877\n1  1.848441 -1.122676 -0.359290 -0.365921\n```\n\n## 条件查询\n\n### 生成示例数据\n\n```\n# In[]\nsample = pd.DataFrame({'name':['Bob','Lindy','Mark',\"Miki\",'Sully','Rose'],\n                       'score':[98,78,88,77,69,69],\n                       'group':[1,1,1,2,1,2]})\nsample\n```\n\n```\nOut[14]: \n    name  score  group\n0    Bob     98      1\n1  Lindy     78      1\n2   Mark     88      1\n3   Miki     77      2\n4  Sully     69      1\n5   Rose     69      2\n```\n\n### 单条件查询\n\n涉及单条件查询时，一般会使用比较运算符，产生布尔类型的索引可用于条件查询。\n\n```\nsample.score >66\n```\n\n\n\n```\nOut[15]: \n0    True\n1    True\n2    True\n3    True\n4    True\n5    True\n```\n\n再通过指定的索引进行条件查询，返回bool值为True的数据：\n\n```\nsample[sample.score >66]\n\nOut[16]: \n    name  score  group\n0    Bob     98      1\n1  Lindy     78      1\n2   Mark     88      1\n3   Miki     77      2\n4  Sully     69      1\n5   Rose     69      2\n```\n\n### 多条件查询\n```\nsample[(sample.score >66) & (sample.group==1)]\nOut[17]: \n    name  score  group\n0    Bob     98      1\n1  Lindy     78      1\n2   Mark     88      1\n4  Sully     69      1\n```\n\n### 使用 qurey\n\n```\nsample.query('score > 90')\nOut[20]: \n  name  score  group\n0  Bob     98      1\n```\n\n其他查询\n\n查询sample中70到80之间的记录，并且将边界包含进来（inclusive=True）\n\n```\n# In[]\nsample[sample['score'].between(70,80,inclusive=True)]\nOut[21]: \n    name  score  group\n1  Lindy     78      1\n3   Miki     77      2\n```\n\n对于字符串列来说，可以使用isin 方法进行查询：\n\n```\nsample[sample['name'].isin(['Bob','Lindy'])]\nOut[24]: \n    name  score  group\n0    Bob     98      1\n1  Lindy     78      1\n```\n\n使用正则表达式匹配进行查询，例如查询姓名以M开头的人的所有记录：\n\n```\nsample[sample['name'].str.contains('[M]+')]\nOut[26]: \n   name  score  group\n2  Mark     88      1\n3  Miki     77      2\n```\n\n## 横向连接\n\nPandas Data Frame 提供 merge 方法以完成各种表格的横向连接操作，这种连接操作跟SQL语句的连接操作类似。\n\n```\ndf1 =pd.DataFrame({'id':[1,2,3],\n                   'col1':['a','b','c']})\ndf2 = pd.DataFrame({'id':[4,3],\n                    'col2':['d','e']})\ndf1\nOut[29]: \n   id col1\n0   1    a\n1   2    b\n2   3    c\ndf2\nOut[30]: \n   id col2\n0   4    d\n1   3    e\n```\n\n内连接使用merge函数示例，根据公共字段保留两表的共有信息，`how = 'innner'`参数表示使用内连接，`on`表示两表的公共字段，若公共字段再两表名称不一致时，可以通过 `left_on`和`right_on`指定：\n\n```\ndf1.merge(df2,how='inner',on='id')\nOut[32]: \n   id col1 col2\n0   3    c    e\ndf1.merge(df2,how='inner',left_on='id',right_on='id')\nOut[33]: \n   id col1 col2\n0   3    c    e\n```\n\n### 外连接\n\n外连接包括左连接，全连接，右连接\n\n#### 左连接\n\n左连接通过公共字段，保留坐标的全部信息，右表在左表缺失的信息会以NaN补全：\n\n```\ndf1.merge(df2,how='left',on='id')\nOut[34]: \n   id col1 col2\n0   1    a  NaN\n1   2    b  NaN\n2   3    c    e\n```\n\n#### 右连接\n\n右连接与左连接相对，右连接通过公共字段，保留右表的全部信息，左表在右表缺失的信息会以 NaN 补全。\n\n```\ndf1.merge(df2,how='right',on='id')\nOut[35]: \n   id col1 col2\n0   3    c    e\n1   4  NaN    d\n```\n\n#### 全连接\n\n全连接通过公共字段，保留右表的全部信息，两表相互缺失的信息会以 NaN 补全。\n\n\n\n```\ndf1.merge(df2,how='outer',on='id')\nOut[36]: \n   id col1 col2\n0   1    a  NaN\n1   2    b  NaN\n2   3    c    e\n3   4  NaN    d\n```\n\n## 行索引连接\n\n`pd.concat`可以完成横向和纵向的合并，这通过 ’axis=‘ 来控制，当参数axis= 1时表示进行横向合并。\n\n```\ndf1 = pd.DataFrame({'id':[1,2,3],\n                    'col1':['a','b','c']},\n                   index=[1,2,3])\ndf2 =pd.DataFrame({'id':[1,2,3],\n                   'col2':['aa','bb','cc']},\n                  index=[1,3,2])\npd.concat([df1,df2],axis=1)\nOut[37]: \n   id col1  id col2\n1   1    a   1   aa\n2   2    b   3   cc\n3   3    c   2   bb\n\n```\n\n## 纵向合并\n\n当参数 axis = 0 时，表示纵向合并。ignore_index= True 表示忽略df1 和 df2 的原先的行索引，合并后重新排列索引。\n\n```\npd.concat([df1,df2],ignore_index=True,axis=0)\nOut[43]: \n  col1 col2  id\n0    a  NaN   1\n1    b  NaN   2\n2    c  NaN   3\n3  NaN   aa   1\n4  NaN   bb   2\n5  NaN   cc   3\n```\n\n去除重复行\n\n```\npd.concat([df1,df2],ignore_index=True,axis=0).drop_duplicates()\n\nOut[44]: \n  col1 col2  id\n0    a  NaN   1\n1    b  NaN   2\n2    c  NaN   3\n3  NaN   aa   1\n4  NaN   bb   2\n5  NaN   cc   3\n```\n\n\n\n\n\n## 排序\n\n按照学生成绩降序排列数据，第一个参数表示排序的依据，ascending = False 代表降序排列，na_position='last'表示缺失值数据排列在数据的最后位置。\n\n```\nsample = pd.DataFrame({'name':['Bob','Lindy','Mark',\"Miki\",'Sully','Rose'],\n                       'score':[98,78,88,77,69,np.nan],\n                       'group':[1,1,1,2,1,2]})\nsample\n###\nsample.sort_values('score',ascending= False,na_position='last')\nOut[46]: \n    name  score  group\n0    Bob   98.0      1\n2   Mark   88.0      1\n1  Lindy   78.0      1\n3   Miki   77.0      2\n4  Sully   69.0      1\n5   Rose    NaN      2\n```\n\n## 分组汇总\n\n数据准备\n\n```\nsample = pd.read_csv('./sample.csv',encoding='utf-8')\nsample\nOut[58]: \n   chinese   class  grade  math   name\n0        88      1      1  98.0    Bob\n1        78      1      1  78.0  Lindy\n2        68      1      1  78.0   Miki\n3        56      2      2  77.0   Mark\n4        77      1      2  77.0  Sully\n5        56      2      2   NaN   Rose\n\n```\n\n分组汇总操作中，会涉及分组变量，度量变量和汇总统计量。pandas 提供了 groupby 方法进行分组汇总。\n\n在sample数据中，grade为分组变量，math 为度量变量，现需要查询grade 为1，2中数学成绩最高。\n\n### 分组变量\n\n在进行分组汇总时，分组变量可以有多个。\n\n```\nsample.groupby(['grade','class'])['math'].max()\nOut[65]: \ngrade  class\n1      1        98.0\n2      1        77.0\n       2        77.0\nName: math, dtype: float64\n```\n\n\n\n### 汇总变量\n\n在进行分组汇总时，汇总变量也可以多个。\n\n```\nsample.groupby('grade',)['math','chinese'].mean()\nOut[75]: \n            math  chinese\ngrade                    \n1      84.666667       78\n2      77.000000       63\n```\n\n### 汇总统计量\n\n| 方法   | 解释   | 方法     | 解释         |\n| ------ | ------ | -------- | ------------ |\n| mean   | 均值   | mad      | 平均绝对偏差 |\n| max    | 最大值 | count    | 计数         |\n| min    | 最小值 | skew     | 偏度         |\n| median | 中位数 | quantile | 指定分位数   |\n| std    | 标准差 |          |              |\n\n以上统计量方法可以直接接 groupby 对象使用，agg方法提供了一次汇总多个统计量的方法，例如，汇总各个班级的数学成绩的均值，最大值，最小值。\n\n```\nsample.groupby('class')['math'].agg(['mean','min','max'])\nOut[78]: \n        mean   min   max\nclass                   \n1      82.75  77.0  98.0\n2      77.00  77.0  77.0\n```\n\n\n\n\n\n### 多重索引\n\n\n\n以年级，班级对学生的数学，语文成绩进行分组汇总，汇总统计量为均值。此时df中有两个行索引和两个列索引。\n\n```\ndf=sample.groupby(['class','grade'])['math','chinese'].agg(['mean','min','max'])\nOut[80]: \n                  math             chinese        \n                  mean   min   max    mean min max\nclass grade                                       \n1     1      84.666667  78.0  98.0      78  68  88\n      2      77.000000  77.0  77.0      77  77  77\n2     2      77.000000  77.0  77.0      56  56  56\n```\n\n查询各个年级、班级的数学成绩的最小值。\n\n```\ndf['math']['min']\nOut[84]: \nclass  grade\n1      1        78.0\n       2        77.0\n2      2        77.0\nName: min, dtype: float64\n```\n\n\n","tags":["数据整合"],"categories":["数据挖掘"]},{"title":"data_mining","url":"/2018/10/16/data-mining/","content":"[TOC]\n\n# Python 常用数据分析框架\n\n| 名称       |             解释             |\n| ---------- | :--------------------------: |\n| Numpy      |  数组，矩阵的存储，运算框架  |\n| Scipy      | 提供统计，线性代数等计算框架 |\n| Pandas     |  结构化数据的整合，处理框架  |\n| Statsmodel |    常见的统计分析框架模型    |\n| Matplotlib |        数据可视化框架        |\n\n\n\n# python 基础数据类型\n\n| 名称    | 解释   | 示例        |\n| ------- | ------ | ----------- |\n| str     | 字符串 | 'a', '2'    |\n| float   | 浮点数 | 1.23， 3.45 |\n| int     | 整数   | 3，5        |\n| bool    | 布尔   | Ture, False |\n| complex | 复数   | 1+2j, 2 +0j |\n\n\n\n# Python 数据格式转换\n\n| 数据类型 | 转换函数  |\n| -------- | --------- |\n| Str      | str()     |\n| Float    | float()   |\n| Int      | Int()     |\n| Bool     | bool()    |\n| Complex  | complex() |\n\n# 读取数据\n\n```\nimport pandas as pd\ncsv =  pd.read_csv('filename')\n```\n\n# 制图步骤\n\n在进行描述性图展示时，制图分为以下四步：\n\n1. 整理原始数据：对初始数据进行预处理和清洗，已达到制图得要求。\n2. 明确表达的信息： 根据初始可用数据，明确分析要表达的信息。\n3. 确定比较的类型： 明确要表达的信息中对目标比较的类型。\n4. 选择图表类型： 选择合适的图表类型，进行绘制并展示。\n\n```flow\n\na=>operation: 数据\nb=>operation: 信息\nc=>operation: 相对关系\nd=>operation: 图形\n\na(right)->b(right)->c(right)->d\n```\n\n\n","tags":["数据分析"]},{"title":"UML","url":"/2018/10/16/UML/","content":"\n\n\n# 制图步骤\n\n在进行描述性图展示时，制图分为以下四步：\n\n1. 整理原始数据：对初始数据进行预处理和清洗，已达到制图得要求。\n2. 明确表达的信息： 根据初始可用数据，明确分析要表达的信息。\n3. 确定比较的类型： 明确要表达的信息中对目标比较的类型。\n4. 选择图表类型： 选择合适的图表类型，进行绘制并展示。\n\n``` flow\n\na=>operation: 数据\nb=>operation: 信息\nc=>operation: 相对关系\nd=>operation: 图形\n\na(right)->b(right)->c(right)->d\n```\n\n\n\n\n(refer link)[http://www.gravizo.com/#howto]\n","tags":["markdown"]},{"title":"es-python-client","url":"/2018/10/12/es-python-client/","content":"# python 操作 Elasticsearch\n\n## 安装 Elasticsearch 模块\n`pip install elasticsearch`\n\n## 添加数据\n``` \nfrom elasticsearch import Elasticsearch\n\n# 默认host为localhost,port为9200.但也可以指定host与port\nes = Elasticsearch([{'host': '192.168.10.21', 'port': 9200}])\ndoc = {\n     'author': 'kimchy',\n    'text': 'Elasticsearch: cool. bonsai cool.',\n     'timestamp': localtime(),\n      }\nres = es.index(index=\"test-index\", doc_type='tweet', id=1, body=doc)\nprint(res)\n```\n如果创建成功会返回以下结果\n``` \n{'_index': 'test-index', '_type': 'tweet', '_id': '1', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 2, 'failed': 0}, '_seq_no': 0, '_primary_term': 1}\n\n```\n## 创建索引\n```\nes.indices.create(index='irisaa') \n```\n## 删除索引\n``` \nes.indices.create(index='irisaa')\n```\n## 查看集群状态\n```\nes.cluster.health(wait_for_status='yellow', request_timeout=1)\n```\n## 查询数据\n### 按照 id 来查询数据\n``` \nres = es.get(index=\"test-index\", doc_type='tweet', id=1)\nprint(res['_source'])\n```\n操作成功后返回如下结果：\n```\n{'author': 'kimchy', 'text': 'Elasticsearch: cool. bonsai cool.', 'timestamp': [2018, 10, 12, 14, 9, 44, 4, 285, 0]}\n\n```\n### 按照DSL语句查询\n```   \nres = es.search(index=\"test-index\", body={\"query\": {\"match_all\": {}}})\nprint(res)\n```\n操作成功后结果，返回如下结果：\n```\n{'took': 2, 'timed_out': False, '_shards': {'total': 5, 'successful': 5, 'skipped': 0, 'failed': 0}, 'hits': {'total': 1, 'max_score': 1.0, 'hits': [{'_index': 'test-index', '_type': 'tweet', '_id': '1', '_score': 1.0, '_source': {'author': 'kimchy', 'text': 'Elasticsearch: cool. bonsai cool.', 'timestamp': [2018, 10, 12, 14, 9, 44, 4, 285, 0]}}]}}\n```\n\n## 更新数据\n```\nes = Elasticsearch()\ndoc = {\n     'author': 'kimchy',\n    'text': 'ok.',\n     'timestamp': localtime(),\n      }\nresult = es.update(index=\"test-index\", doc_type='tweet', id=1, body=doc)\nprint(result) \n```\n\n\n## 删除数据\n```angular2html\n\nes.delete_by_query(index='twtter',doc_type='_doc',body={\n   \"query\": {\n     \"match\": {\n       \"message\": \"some message\"\n     }\n   }\n })\n```\n\n## 批量化导入es\n```\nfrom elasticsearch import helpers\ndef gendata(index, type, jsons):\n    for json in jsons:\n        yield {\n            \"_index\": index,\n            \"_type\": type,\n            \"_source\": json,\n        }\n        \nhelpers.bulk(es, gendata(index='index-test', jsons))        \n```\n","tags":["ELK"]},{"title":"ELK_docker-compose.md","url":"/2018/10/12/ELK-docker-compose-md/","content":"# elasticsearch 集群部署\n我们使用dockers-compose实现单机多节点的部署\n\n## 安装docker\n在测试或开发环境中 Docker 官方为了简化安装流程，提供了一套便捷的安装脚本，Ubuntu 系统上可以使用这套脚本安装：\n``` \n$ curl -fsSL get.docker.com -o get-docker.sh\n$ sudo sh get-docker.sh --mirror Aliyun\n```\n执行这个命令后，脚本就会自动的将一切准备工作做好，并且把 Docker CE 的 Edge 版本安装在系统中。\n### 启动 Docker CE\n \n``` \n$ sudo systemctl enable docker\n$ sudo systemctl start docker\n```\n\n### 建立 docker 用户组\n默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组。\n\n`$ sudo groupadd docker\n`\n\n将当前用户 加入 ``docker`` 组：\n\n`$ sudo usermod -aG docker $USER`\n\n### 测试dockers 是否安装正确\n```\n$ docker run hello-world\n\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\nca4f61b1923c: Pull complete\nDigest: sha256:be0cd392e45be79ffeffa6b05338b98ebb16c87b255f48e297ec7f98e123905c\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://cloud.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/engine/userguide/\n```\n\n## 安装docker-compopse\nCompose 项目是 Docker 官方的开源项目，负责实现对 Docker 容器集群的快速编排。\n它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）。\n\nCompose 中有两个重要的概念：\n\n- 服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。\n\n- 项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。\n\nCompose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。\n\nCompose 项目由 Python 编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。因此，只要所操作的平台支持 Docker API，就可以在其上利用 Compose 来进行编排管理。\n\n### PIP 安装 dockerpose\n``$ sudo pip install -U docker-compose\n``\n\n### 编写 docker-compose.yml\n编写 docker-compose.yml 文件,输入以下内容：\n```\nversion: '2.2'\nservices:\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.0\n    container_name: elasticsearch\n    environment:\n      - cluster.name=docker-cluster\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n        - esdata1:/usr/share/elasticsearch/data\n    volumes:\n      - /home/ethan/EKL/config:/usr/share/elasticsearch/config\n    ports:\n      - 9200:9200\n      - 9300:9300\n    networks:\n      - esnet\n\n  elasticsearch2:\n    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.0\n    container_name: elasticsearch2\n    environment:\n      - cluster.name=docker-cluster\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n      - \"discovery.zen.ping.unicast.hosts=elasticsearch\"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n      - esdata2:/usr/share/elasticsearch/data\n    networks:\n      - esnet\n\n  elasticsearch3:\n    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.0\n    container_name: elasticsearch3\n    environment:\n      - cluster.name=docker-cluster\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n      - \"discovery.zen.ping.unicast.hosts=elasticsearch\"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n      - esdata3:/usr/share/elasticsearch/data\n    networks:\n      - esnet\n  kibana:\n    image: docker.elastic.co/kibana/kibana:6.4.0\n    container_name: kibana\n    environment:\n      SERVER_NAME: kibana\n      ELASTICSEARCH_URL: http://elasticsearch:9200\n    ports:\n      - 5601:5601\n    networks:\n      - esnet\n\nvolumes:\n  esdata1:\n    driver: local\n  esdata2:\n    driver: local\n  esdata3:\n    driver: local\n\nnetworks:\n  esnet:\n\n```\n## 运行 eslasticsearch-kibana\n```\ndocker-compose up\n\n```\n","tags":["elasticsearch"]},{"title":"Elk初探","url":"/2018/10/10/Elk_platfrom/","content":"> 启动elaticsearch需要java环境，请自己谷歌搭建哈\n\n# elaticsearch\n下载tar包\n```powershell\nwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.4.1.tar.gz\n\n```\n\n解压\n```\ntar -zxvf elasticsearch-6.4.1.tar.gz\n```\n进入elaticsearch 可执行文件目录\n```angular2html\ncd elasticsearch-6.4.0/bin\n```\n启动elaticsearch\n```angular2html\n./elaticseaerch\n```\n如果没有抱任何错误，正常来说应该是可以通过restful API来访问的。\n````\ncurl -X GET \"localhost:9200/_cat/health?v\"\n\n````\n对应的返回结果\n```\nepoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\n1538014388 10:13:08  elasticsearch green           1         1      0   0    0    0        0             0                  -                100.0%\n```\n由此我们可以认为elaticsearch已经启动成功了。\n\n# kibana\n下载tar包\n```angular2html\nhttps://artifacts.elastic.co/downloads/kibana/kibana-6.4.1-linux-x86_64.tar.gz\n```\n解压tar包\n```\ntar -zxvf kibana-6.4.1-linux-x86_64.tar.gz\n```\n\n更改kibana配置文件\n```\nvim  /kibana-6.4.1-linux-x86_64/config/kibana.yml\n```\n添加或解注释以下内容\n```\n server.port: 5601\n server.host: \"localhost\"\n elasticsearch.url: \"http://localhost:9200\"\n\n```\n进入kibana可执行文件目录\n```\ncd kibana-6.4.1-linux-x86_64/bin/\n```\n启动kibana\n\n`./kibana`\n\n配置成功后，用浏览器访问`http://ip:5601`可以看到以下页面\n![](/image/kibana.png)\n\n\n# elaticsearc 基本操作\n## 检查集群状态\n```\ncurl -X GET \"localhost:9200/_cat/health?v\"\n\n```\n获取结果如下显示\n```\nepoch      timestamp cluster     status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\n1538019385 11:36:25  data-mining green           1         1      1   1    0    0        0             0                  -                100.0%\n\n```\n## 列出所有索引\n``` \ncurl -X GET \"localhost:9200/_cat/indices?v\"\n```\n如下图所示，我们只有一个索引\n```\nhealth status index   uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   .kibana eFUOj6qJTQCt4xQdvbv8KA   1   0          1            0        4kb            4kb\n\n```\n\n## 创建一个自定义的索引\n```angular2html\n curl -X PUT \"localhost:9200/customer?pretty\"\n```\n对插入的索引返回结果\n```\n\n{\n  \"acknowledged\" : true,\n  \"shards_acknowledged\" : true,\n  \"index\" : \"customer\"\n}\n\n```\n\n再次查看当前索引\n````angular2html\n curl -X GET \"localhost:9200/_cat/indices?v\"\n````\n可以观察到多了一个名为‘customer’的新索引\n```\nhealth status index    uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   .kibana  eFUOj6qJTQCt4xQdvbv8KA   1   0          1            0        4kb            4kb\nyellow open   customer k50FrrMLScGvwzOvfiF0fg   5   1          0            0       401b           401b\n```\n## 插入一个文档\n```angular2html\ncurl -X PUT \"localhost:9200/customer/_doc/1?pretty\" -H 'Content-Type: application/json' -d'\n{\n  \"name\": \"John Doe\"\n}\n'\n```\n如果操作正常，那么插入成功后会返回以下结果\n``` \n{\n  \"_index\" : \"customer\",\n  \"_type\" : \"_doc\",\n  \"_id\" : \"1\",\n  \"_version\" : 1,\n  \"result\" : \"created\",\n  \"_shards\" : {\n    \"total\" : 2,\n    \"successful\" : 1,\n    \"failed\" : 0\n  },\n  \"_seq_no\" : 0,\n  \"_primary_term\" : 1\n}\n```\n## 按照_id查询索引内的文档\n```\ncurl -X GET \"localhost:9200/customer/_doc/1?pretty\"\n```\n查看返回结果，\"_index\"为当前查询的索引，“_type”为查询的文档类型，“_id”为文档所在的id，\n“_source”为我们要查询的内容\n\n```\n\n{\n  \"_index\" : \"customer\",\n  \"_type\" : \"_doc\",\n  \"_id\" : \"1\",\n  \"_version\" : 1,\n  \"found\" : true,\n  \"_source\" : {\n    \"name\" : \"John Doe\"\n  }\n}\n```\n# elastcisear-head\n为了更方便的地查询es中的数据，我推荐使用es插件elasticsearch-head来快速检索es中的数据。\n\n传送门：[elastcsearch-head](https://github.com/mobz/elasticsearch-head)\n\n如果你使用的版本是在elasticsearch 5之后，还需要对elasticsearch 进行配置\n```\ncat >> elasticsearch-6.4.0/config/elasticsearch.yml << EOF\nhttp.cors.enabled: true\nhttp.cors.allow-origin: \"*\"\nEOF\n```\n\n\n## 安装 \n由于elasticsearch-head 需要nodejs，所以我们需要先安装 nodejs 以及 npm\n## nodejs 安装\n```\n$ sudo apt install nodejs\n$ sudo apt instlal npm\n```\nnodejs 安装完后，我们就可以把elasticsearch-head 下载，进行配置了。\n```\ngit clone git://github.com/mobz/elasticsearch-head.git\ncd elasticsearch-head\nnpm install\nnpm run start\n```\n操作成功后的输出显示\n```\n $ npm run start\n\n> elasticsearch-head@0.0.0 start /home/ethan/ekl/elasticsearch-head\n> grunt server\n\n(node:22696) ExperimentalWarning: The http2 module is an experimental API.\nRunning \"connect:server\" (connect) task\nWaiting forever...\nStarted connect web server on http://localhost:9100\n```\n![](/image/eshead.png)\n\n这样我们最简单的数据搜素平台就搭建成功了。\n","tags":["Elk"]},{"title":"Ethan Lee","url":"/2018/08/18/mygirl/","content":"# say something to mygirl\n\n\n<blockquote class=\"blockquote-center\">我知道到遇见你不容易,错过了会很可惜,我不希望余生都是回忆,我希望余生都是你,我爱你</blockquote>\n\n\n![](http://img02.tooopen.com/images/20160509/tooopen_sy_161967094653.jpg)\n\n[link](https://ethan2lee.github.io)\n","tags":["paper"]}]