[{"title":"spark-learn-note","url":"/2019/08/26/spark-learn-note/","content":"\n\n\n先读些系列的文章，对spark有个基本的了解\n\n[1. spark 简介](https://juejin.im/entry/57316d021532bc0065140876)\n\n[2. spark 基本概念解析](http://litaotao.github.io/spark-questions-concepts)\n\n[3. spark 编程模式](http://litaotao.github.io/spark-programming-model?s=inner)\n\n[4. spark 之 RDD](http://litaotao.github.io/spark-what-is-rdd?s=inner)\n\n[5. 这些年，你不能错过的 spark 学习资源](http://litaotao.github.io/spark-resouces-blogs-paper?s=inner)\n\n[6. 深入研究 spark 运行原理之 job, stage, task](http://litaotao.github.io/deep-into-spark-exection-model?s=inner)\n\n[7. 使用 Spark DataFrame 进行大数据分析](http://litaotao.github.io/spark-dataframe-introduction?s=inner)\n\n[8. 实战案例 ｜ Spark 在金融领域的应用 ｜ 日内走势预测](http://litaotao.github.io/spark-in-finance-and-investing?s=inner)\n\n[9. 搭建 IPython + Notebook + Spark 开发环境](http://litaotao.github.io/ipython-notebook-spark)\n\n[10. spark 应用程序性能优化｜12 个优化方法](http://litaotao.github.io/boost-spark-application-performance?s=inner)\n\n[11. spark 机器学习](http://litaotao.github.io/spark-mlib-machine-learning?s=inner)\n\n[RDD：基于内存的集群计算容错抽象](http://shiyanjun.cn/archives/744.html)\n\n书籍\n\n百度云盘：\n【spark 快速大数据分析】链接: https://pan.baidu.com/s/1D7GMbGeKEVg7edJwujehGA 提取码: he2d \n\n# 服务器分布\nmaster ip: 192.168.10.170  \nuser: spark  \npasswd:s1  \nspark_root_dir:/home/spark/spark-2.4.0-bin-hadoop2.7  \n\nslaver ip: 192.168.10.171,   192.168.10.172  \nuser: spark  \npasswd:s1  \nspark_root_dir:/home/spark/spark-2.4.0-bin-hadoop2.7 \n\n# 安装\n[download](https://spark.apache.org/downloads.html)\n\n1 下载特定版本安装包，并解压\n```\ntar -zxvf spark-2.4.0-bin-hadoop2.7.tgz\n```\n\n2 配置 JAVA_HOME\n\n```\nexport JAVA_HOME=/home/spark/jdk1.8.0_191\nexport PATH=$PATH:$JAVA_HOME/bin\n```\n\n3 配置python api\n```\ncd spark_home_dir/python\nsudo python3 setup.py install \n```\n4 测试配置成功\n```\nspark@spark1:~/spark-2.4.0-bin-hadoop2.7$ ./bin/spark-submit examples/src/main/python/pi.py\n\n```\n若配置成功会输出以下内容\n```\n2018-12-12 02:53:31 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2018-12-12 02:53:31 INFO  SparkContext:54 - Running Spark version 2.4.0\n2018-12-12 02:53:31 INFO  SparkContext:54 - Submitted application: PythonPi\n2018-12-12 02:53:31 INFO  SecurityManager:54 - Changing view acls to: spark\n2018-12-12 02:53:31 INFO  SecurityManager:54 - Changing modify acls to: spark\n2018-12-12 02:53:31 INFO  SecurityManager:54 - Changing view acls groups to:\n2018-12-12 02:53:31 INFO  SecurityManager:54 - Changing modify acls groups to:\n2018-12-12 02:53:31 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()\n2018-12-12 02:53:31 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 34371.\n2018-12-12 02:53:31 INFO  SparkEnv:54 - Registering MapOutputTracker\n2018-12-12 02:53:31 INFO  SparkEnv:54 - Registering BlockManagerMaster\n2018-12-12 02:53:31 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n2018-12-12 02:53:31 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\n2018-12-12 02:53:31 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-0f28f494-80f9-4d57-880f-ff3ab72fc569\n2018-12-12 02:53:31 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB\n2018-12-12 02:53:31 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\n2018-12-12 02:53:32 INFO  log:192 - Logging initialized @1724ms\n2018-12-12 02:53:32 INFO  Server:351 - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n2018-12-12 02:53:32 INFO  Server:419 - Started @1791ms\n2018-12-12 02:53:32 INFO  AbstractConnector:278 - Started ServerConnector@8aee1e3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n2018-12-12 02:53:32 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4f06548c{/jobs,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@291e161b{/jobs/json,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@fca97e6{/jobs/job,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4b23942e{/jobs/job/json,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1254fd9f{/stages,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@49fa3d94{/stages/json,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3c3aa98a{/stages/stage,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@46ecf8e5{/stages/stage/json,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2f736f4b{/stages/pool,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3acf3392{/stages/pool/json,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7721ef93{/storage,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@56ab1a42{/storage/json,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@62163eeb{/storage/rdd,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2106c298{/storage/rdd/json,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5923ec5c{/environment,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@57d49430{/environment/json,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4e60947{/executors,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@13e31941{/executors/json,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@55e4566d{/executors/threadDump,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@26e23df5{/executors/threadDump/json,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@127aa45f{/static,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@60a5aa78{/,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2b225b78{/api,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@62fead0{/jobs/job/kill,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@62f13110{/stages/stage/kill,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://spark1:4040\n2018-12-12 02:53:32 INFO  Executor:54 - Starting executor ID driver on host localhost\n2018-12-12 02:53:32 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42589.\n2018-12-12 02:53:32 INFO  NettyBlockTransferService:54 - Server created on spark1:42589\n2018-12-12 02:53:32 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n2018-12-12 02:53:32 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, spark1, 42589, None)\n2018-12-12 02:53:32 INFO  BlockManagerMasterEndpoint:54 - Registering block manager spark1:42589 with 366.3 MB RAM, BlockManagerId(driver, spark1, 42589, None)\n2018-12-12 02:53:32 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, spark1, 42589, None)\n2018-12-12 02:53:32 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, spark1, 42589, None)\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@526f5cb1{/metrics/json,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/spark-2.4.0-bin-hadoop2.7/spark-warehouse').\n2018-12-12 02:53:32 INFO  SharedState:54 - Warehouse path is 'file:/home/spark/spark-2.4.0-bin-hadoop2.7/spark-warehouse'.\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@61e254a2{/SQL,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4c890f2{/SQL/json,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@460c2a3f{/SQL/execution,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7462cbcf{/SQL/execution/json,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@116110ac{/static/sql,null,AVAILABLE,@Spark}\n2018-12-12 02:53:32 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint\n2018-12-12 02:53:33 INFO  SparkContext:54 - Starting job: reduce at /home/spark/spark-2.4.0-bin-hadoop2.7/examples/src/main/python/pi.py:44\n2018-12-12 02:53:33 INFO  DAGScheduler:54 - Got job 0 (reduce at /home/spark/spark-2.4.0-bin-hadoop2.7/examples/src/main/python/pi.py:44) with 2 output partitions\n2018-12-12 02:53:33 INFO  DAGScheduler:54 - Final stage: ResultStage 0 (reduce at /home/spark/spark-2.4.0-bin-hadoop2.7/examples/src/main/python/pi.py:44)\n2018-12-12 02:53:33 INFO  DAGScheduler:54 - Parents of final stage: List()\n2018-12-12 02:53:33 INFO  DAGScheduler:54 - Missing parents: List()\n2018-12-12 02:53:33 INFO  DAGScheduler:54 - Submitting ResultStage 0 (PythonRDD[1] at reduce at /home/spark/spark-2.4.0-bin-hadoop2.7/examples/src/main/python/pi.py:44), which has no missing parents\n2018-12-12 02:53:33 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 6.2 KB, free 366.3 MB)\n2018-12-12 02:53:33 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.2 KB, free 366.3 MB)\n2018-12-12 02:53:33 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on spark1:42589 (size: 4.2 KB, free: 366.3 MB)\n2018-12-12 02:53:33 INFO  SparkContext:54 - Created broadcast 0 from broadcast at DAGScheduler.scala:1161\n2018-12-12 02:53:33 INFO  DAGScheduler:54 - Submitting 2 missing tasks from ResultStage 0 (PythonRDD[1] at reduce at /home/spark/spark-2.4.0-bin-hadoop2.7/examples/src/main/python/pi.py:44) (first 15 tasks are for partitions Vector(0, 1))\n2018-12-12 02:53:33 INFO  TaskSchedulerImpl:54 - Adding task set 0.0 with 2 tasks\n2018-12-12 02:53:33 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7852 bytes)\n2018-12-12 02:53:33 INFO  TaskSetManager:54 - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7852 bytes)\n2018-12-12 02:53:33 INFO  Executor:54 - Running task 1.0 in stage 0.0 (TID 1)\n2018-12-12 02:53:33 INFO  Executor:54 - Running task 0.0 in stage 0.0 (TID 0)\n2018-12-12 02:53:33 INFO  PythonRunner:54 - Times: total = 378, boot = 246, init = 48, finish = 84\n2018-12-12 02:53:33 INFO  PythonRunner:54 - Times: total = 382, boot = 242, init = 52, finish = 88\n2018-12-12 02:53:33 INFO  Executor:54 - Finished task 1.0 in stage 0.0 (TID 1). 1421 bytes result sent to driver\n2018-12-12 02:53:33 INFO  Executor:54 - Finished task 0.0 in stage 0.0 (TID 0). 1421 bytes result sent to driver\n2018-12-12 02:53:33 INFO  TaskSetManager:54 - Finished task 1.0 in stage 0.0 (TID 1) in 426 ms on localhost (executor driver) (1/2)\n2018-12-12 02:53:33 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 446 ms on localhost (executor driver) (2/2)\n2018-12-12 02:53:33 INFO  TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool\n2018-12-12 02:53:33 INFO  PythonAccumulatorV2:54 - Connected to AccumulatorServer at host: 127.0.0.1 port: 41033\n2018-12-12 02:53:33 INFO  DAGScheduler:54 - ResultStage 0 (reduce at /home/spark/spark-2.4.0-bin-hadoop2.7/examples/src/main/python/pi.py:44) finished in 0.536 s\n2018-12-12 02:53:33 INFO  DAGScheduler:54 - Job 0 finished: reduce at /home/spark/spark-2.4.0-bin-hadoop2.7/examples/src/main/python/pi.py:44, took 0.576000 s\nPi is roughly 3.150560\n2018-12-12 02:53:33 INFO  AbstractConnector:318 - Stopped Spark@8aee1e3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n2018-12-12 02:53:33 INFO  SparkUI:54 - Stopped Spark web UI at http://spark1:4040\n2018-12-12 02:53:33 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!\n2018-12-12 02:53:33 INFO  MemoryStore:54 - MemoryStore cleared\n2018-12-12 02:53:33 INFO  BlockManager:54 - BlockManager stopped\n2018-12-12 02:53:33 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped\n2018-12-12 02:53:33 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!\n2018-12-12 02:53:33 INFO  SparkContext:54 - Successfully stopped SparkContext\n2018-12-12 02:53:34 INFO  ShutdownHookManager:54 - Shutdown hook called\n2018-12-12 02:53:34 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-c09679ff-e0e0-4e24-839b-74de1930969a\n2018-12-12 02:53:34 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-c42919fe-751c-4223-9f83-530cbd978945\n2018-12-12 02:53:34 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-c09679ff-e0e0-4e24-839b-74de1930969a/pyspark-876b59fd-18f9-4284-8131-b09ac5642632\n```\n\n# 快速入门\n[spark 2.2.x中文文档](https://spark-reference-doc-cn.readthedocs.io/zh_CN/latest/programming-guide/quick-start.html)\n\n\n# 编程指南\n[link](https://aiyanbo.gitbooks.io/spark-programming-guide-zh-cn/content/quick-start/using-spark-shell.html)\n\n\n# 环境变量参考\nexport JAVA_HOME=/home/spark/jdk1.8.0_191\n\nexport PATH=$PATH:$JAVA_HOME/bin\n\nexport PYSPARK_PYTHON=python3\n\nexport SPARK_HOME=/home/spark/spark-2.4.0-bin-hadoop2.7\n\nexport PYSPARK_SUBMIT_ARGS=--master spark://192.168.10.170:7077 --deploy-mode client\n\n\n\n\n或者\n\n\nexport JAVA_HOME=/home/spark/jdk1.8.0_191\nexport PATH=$PATH:$JAVA_HOME/bin\n\nexport SPARK_HOME=/home/spark/spark-2.4.0-bin-hadoop2.7\nexport PATH=$PATH:$SPARK_HOME/bin\n\nexport PYSPARK_PYTHON=python3\nexport PYSPARK_DRIVER_PYTHON=jupyter\nexport PYSPARK_DRIVER_PYTHON_OPTS=\"notebook  --ip=192.168.10.170\"\nexport MASTER=\"spark://192.168.10.170:7077\"\n\nexport PYSPARK_SUBMIT_ARGS=\"--master spark://192.168.10.170:7077 --deploy-mode client\"\nexport HADOOP_HOME=/home/spark/hadoop-2.7.7\nexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n\nexport JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native/\n\nexport SPARK__VERSION=2.4.0\nexport SCALA_VERSION=2.11.8\n\n\n\n# 自建worker\nspark-class org.apache.spark.deploy.worker.Worker spark://spark1:7077\n\n# python spark 基础学习\n\n\n在 Spark 目录下运行以下命令可以启动 Spark Shell：\n\n```\n./bin/pyspark\n```\n\n或者如果在你当前环境已经使用 pip 安装了 PySpark，你也可以直接使用以下命令:\n```\npyspark\n```\n运行输出\n\n```\nspark@spark1:~/spark-2.4.0-bin-hadoop2.7$ pyspark\nPython 3.6.7 (default, Oct 22 2018, 11:32:17)\n[GCC 8.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n2018-12-12 03:32:28 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.0\n      /_/\n\nUsing Python version 3.6.7 (default, Oct 22 2018 11:32:17)\nSparkSession available as 'spark'.\n>>>\n```\n\nSpark 最主要的抽象概念就是一个叫做 Dataset 的分布式数据集。\nDataset 可以从 Hadoop InputFormats(例如 HDFS 文件)创建或者由其他 Dataset 转换而来。由于 Python 语言的动态性, 我们不需要 Dataset 是强类型的。\n因此 Python 中所有的 Dataset 都是 Dataset[Row], 并且为了和 Pandas 以及 R 中的 data frame 概念保持一致, 我们称其为 DataFrame。\n下面我们利用 Spark 源码目录下 README 文件中的文本来新建一个 DataFrame:\n\n```\n>>> textFile = spark.read.text(\"README.md\")\n```\n\n计算文本的行数,以及获取第一行\n\n\n```\n>>> textFile.count()   # Number of rows in this DataFrame\n105\n\n>>> textFile.first()  # First row in this DataFrame\nRow(value=u'# Apache Spark')\n```\n\n现在我们将该 DataFrame 转换成一个新的 DataFrame。\n\n我们调用 filter 这个 transformation 算子返回一个只包含原始文件数据项子集的新 DataFrame。\n\n```\n>>> linesWithSpark = textFile.filter(textFile.value.contains(\"Spark\"))\n```\n我们可以将 transformation 算子和 action 算子连在一起:\n```\n>>> textFile.filter(textFile.value.contains(\"Spark\")).count()  # How many lines contain \"Spark\"?\n20\n````\n\nSpark 可以很容易地实现 MapReduce 流程：\n\n```\nfrom pyspark.sql.functions import *\ntextFile.select(size(split(textFile.value, \"\\s+\")).name(\"numWords\")).agg(max(col(\"numWords\"))).collect()\n```\n首先，使用 map 算子将每一行映射为一个整数值并给其取别名 “numWords”, 创建了一个新的 DataFrame。\n\n然后在该 DataFrame 上调用 agg 算子找出最大的单词计数。select 和 agg 的参数都是 Column , \n\n我们可以使用 df.colName 从 DataFrame 上获取一列，也可以引入 pyspark.sql.functions, 它提供了很多方便的函数用来从旧的 Column 构建新的 Column。\n\n\n因 Hadoop 而广为流行的 MapReduce 是一种通用的数据流模式。Spark 可以很容易地实现 MapReduce 流程：\n\n```\n>>> wordCounts = textFile.select(explode(split(textFile.value, \"\\s+\")).alias(\"word\")).groupBy(\"word\").count()\n\n```\n\n这里我们在 select 函数中使用 explode 函数将一个行的 Dataset 转换成了一个单词的 Dataset, \n然后组合 groupBy 和 count 算子来计算文件中每个单词出现的次数，生成一个包含 “word” 和 “count” 这 2 列的 DataFrame。\n为了在 shell 中收集到单词计数, 我们可以调用 collect 算子:\n\n```\n>>> wordCounts.collect()\n[Row(word='online', count=1), Row(word='graphs', count=1), Row(word='[\"Parallel', count=1), Row(word='[\"Building', count=1), Row(word='thread', count=1), Row(word='documentation', count=3), Row(word='command,', count=2), Row(word='abbreviated', count=1), Row(word='overview', count=1), Row(word='rich', count=1), Row(word='set', count=2), Row(word='-DskipTests', count=1), Row(word='name', count=1), Row(word='page](http://spark.apache.org/documentation.html).', count=1), Row(word='[\"Specifying', count=1), Row(word='stream', count=1), Row(word='run:', count=1), Row(word='not', count=1), Row(word='programs', count=2), Row(word='tests', count=2), Row(word='./dev/run-tests', count=1), Row(word='will', count=1), Row(word='[run', count=1), Row(word='particular', count=2), Row(word='option', count=1), Row(word='Alternatively,', count=1), Row(word='by', count=1), Row(word='must', count=1), Row(word='using', count=5), Row(word='you', count=4), Row(word='MLlib', count=1), Row(word='DataFrames,', count=1), Row(word='variable', count=1), Row(word='Note', count=1), Row(word='core', count=1), Row(word='more', count=1), Row(word='protocols', count=1), Row(word='guidance', count=2), Row(word='shell:', count=2), Row(word='can', count=7), Row(word='site,', count=1), Row(word='systems.', count=1), Row(word='Maven', count=1), Row(word='[building', count=1), Row(word='configure', count=1), Row(word='for', count=12), Row(word='README', count=1), Row(word='Interactive', count=2), Row(word='how', count=3), Row(word='[Configuration', count=1), Row(word='Hive', count=2), Row(word='system', count=1), Row(word='provides', count=1), Row(word='Hadoop-supported', count=1), Row(word='pre-built', count=1), Row(word='[\"Useful', count=1), Row(word='directory.', count=1), Row(word='Example', count=1), Row(word='example', count=3), Row(word='Kubernetes', count=1), Row(word='one', count=3), Row(word='MASTER', count=1), Row(word='in', count=6), Row(word='library', count=1), Row(word='Spark.', count=1), Row(word='contains', count=1), Row(word='Configuration', count=1), Row(word='programming', count=1), Row(word='with', count=4), Row(word='contributing', count=1), Row(word='downloaded', count=1), Row(word='1000).count()', count=1), Row(word='comes', count=1), Row(word='machine', count=1), Row(word='Tools\"](http://spark.apache.org/developer-tools.html).', count=1), Row(word='building', count=2), Row(word='params', count=1), Row(word='Guide](http://spark.apache.org/docs/latest/configuration.html)', count=1), Row(word='given.', count=1), Row(word='be', count=2), Row(word='same', count=1), Row(word='integration', count=1), Row(word='than', count=1), Row(word='Programs', count=1), Row(word='locally', count=2), Row(word='using:', count=1), Row(word='fast', count=1), Row(word='[Apache', count=1), Row(word='your', count=1), Row(word='optimized', count=1), Row(word='Developer', count=1), Row(word='R,', count=1), Row(word='should', count=2), Row(word='graph', count=1), Row(word='package', count=1), Row(word='-T', count=1), Row(word='[project', count=1), Row(word='project', count=1), Row(word='`examples`', count=2), Row(word='resource-managers/kubernetes/integration-tests/README.md', count=1), Row(word='versions', count=1), Row(word='Spark](#building-spark).', count=1), Row(word='general', count=3), Row(word='other', count=1), Row(word='learning,', count=1), Row(word='when', count=1), Row(word='submit', count=1), Row(word='Apache', count=1), Row(word='1000:', count=2), Row(word='detailed', count=2), Row(word='About', count=1), Row(word='is', count=7), Row(word='on', count=7), Row(word='scala>', count=1), Row(word='print', count=1), Row(word='use', count=3), Row(word='different', count=1), Row(word='following', count=2), Row(word='SparkPi', count=2), Row(word='refer', count=2), Row(word='./bin/run-example', count=2), Row(word='data', count=1), Row(word='Tests', count=1), Row(word='Versions', count=1), Row(word='Data.', count=1), Row(word='processing.', count=1), Row(word='its', count=1), Row(word='basic', count=1), Row(word='latest', count=1), Row(word='only', count=1), Row(word='<class>', count=1), Row(word='have', count=1), Row(word='runs.', count=1), Row(word='You', count=4), Row(word='tips,', count=1), Row(word='project.', count=1), Row(word='developing', count=1), Row(word='YARN,', count=1), Row(word='It', count=2), Row(word='\"local\"', count=1), Row(word='processing,', count=1), Row(word='built', count=1), Row(word='Pi', count=1), Row(word='thread,', count=1), Row(word='A', count=1), Row(word='APIs', count=1), Row(word='Scala,', count=1), Row(word='file', count=1), Row(word='computation', count=1), Row(word='Once', count=1), Row(word='find', count=1), Row(word='the', count=24), Row(word='To', count=2), Row(word='sc.parallelize(1', count=1), Row(word='uses', count=1), Row(word='Version', count=1), Row(word='N', count=1), Row(word='programs,', count=1), Row(word='\"yarn\"', count=1), Row(word='see', count=4), Row(word='./bin/pyspark', count=1), Row(word='return', count=2), Row(word='computing', count=1), Row(word='Java,', count=1), Row(word='from', count=1), Row(word='Because', count=1), Row(word='cluster', count=2), Row(word='Streaming', count=1), Row(word='More', count=1), Row(word='analysis.', count=1), Row(word='Maven](http://maven.apache.org/).', count=1), Row(word='cluster.', count=1), Row(word='Running', count=1), Row(word='Please', count=4), Row(word='talk', count=1), Row(word='distributions.', count=1), Row(word='guide,', count=1), Row(word='tests](http://spark.apache.org/developer-tools.html#individual-tests).', count=1), Row(word='There', count=1), Row(word='\"local[N]\"', count=1), Row(word='Try', count=1), Row(word='and', count=10), Row(word='do', count=2), Row(word='Scala', count=2), Row(word='class', count=2), Row(word='build', count=4), Row(word='3\"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).', count=1), Row(word='setup', count=1), Row(word='need', count=1), Row(word='spark://', count=1), Row(word='Hadoop,', count=2), Row(word='Thriftserver', count=1), Row(word='are', count=1), Row(word='requires', count=1), Row(word='package.', count=1), Row(word='Enabling', count=1), Row(word='clean', count=1), Row(word='sc.parallelize(range(1000)).count()', count=1), Row(word='high-level', count=1), Row(word='SQL', count=2), Row(word='against', count=1), Row(word='of', count=5), Row(word='through', count=1), Row(word='review', count=1), Row(word='package.)', count=1), Row(word='Python,', count=2), Row(word='easiest', count=1), Row(word='no', count=1), Row(word='Testing', count=1), Row(word='several', count=1), Row(word='help', count=1), Row(word='The', count=1), Row(word='sample', count=1), Row(word='MASTER=spark://host:7077', count=1), Row(word='Big', count=1), Row(word='examples', count=2), Row(word='an', count=4), Row(word='#', count=1), Row(word='Online', count=1), Row(word='test,', count=1), Row(word='including', count=4), Row(word='usage', count=1), Row(word='Python', count=2), Row(word='at', count=2), Row(word='development', count=1), Row(word='Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', count=1), Row(word='IDE,', count=1), Row(word='way', count=1), Row(word='Contributing', count=1), Row(word='get', count=1), Row(word='that', count=2), Row(word='##', count=9), Row(word='For', count=3), Row(word='prefer', count=1), Row(word='This', count=2), Row(word='build/mvn', count=1), Row(word='builds', count=1), Row(word='running', count=1), Row(word='web', count=1), Row(word='run', count=7), Row(word='locally.', count=1), Row(word='Spark', count=16), Row(word='URL,', count=1), Row(word='a', count=9), Row(word='higher-level', count=1), Row(word='tools', count=1), Row(word='if', count=4), Row(word='available', count=1), Row(word='', count=48), Row(word='Documentation', count=1), Row(word='this', count=1), Row(word='(You', count=1), Row(word='>>>', count=1), Row(word='information', count=1), Row(word='info', count=1), Row(word='<http://spark.apache.org/>', count=1), Row(word='Shell', count=2), Row(word='environment', count=1), Row(word='built,', count=1), Row(word='module,', count=1), Row(word='them,', count=1), Row(word='`./bin/run-example', count=1), Row(word='instance:', count=1), Row(word='first', count=1), Row(word='[Contribution', count=1), Row(word='guide](http://spark.apache.org/contributing.html)', count=1), Row(word='documentation,', count=1), Row(word='[params]`.', count=1), Row(word='mesos://', count=1), Row(word='engine', count=1), Row(word='GraphX', count=1), Row(word='Maven,', count=1), Row(word='example:', count=1), Row(word='HDFS', count=1), Row(word='YARN\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)', count=1), Row(word='or', count=3), Row(word='to', count=17), Row(word='Hadoop', count=3), Row(word='individual', count=1), Row(word='also', count=5), Row(word='changed', count=1), Row(word='started', count=1), Row(word='./bin/spark-shell', count=1), Row(word='threads.', count=1), Row(word='supports', count=2), Row(word='storage', count=1), Row(word='version', count=1), Row(word='instructions.', count=1), Row(word='Building', count=1), Row(word='start', count=1), Row(word='Many', count=1), Row(word='which', count=2), Row(word='And', count=1), Row(word='distribution', count=1)]\n\n```\n\n# 搭建集群\n[link](https://spark.apache.org/docs/latest/spark-standalone.html)\n\n\n在spark1,spark2,spark3的spark_home_dir下，\n```\nvim /home/spark/spark-2.4.0-bin-hadoop2.7/conf/spark-env.sh\n\n```\n添加以下内容\n```\nexport SPARK_MASTER_IP=master\nexport SPARK_WORKER_CORES=4\nexport SPARK_WORKER_MEMORY=512m\nexport SPARK_WORKER_INSTANCES=4\n```\nSPARK_MASTER_IP ：主机地址，现为192.168.10.170\nSPARK_WORKER_CORES: 每个worker 分配的cpu内核数目，\nSPARK_WORKER_MEMORY ：每个worker 分配的最大内存\nSPARK_WORKER_INSTANCES： 每个spark启动worker的数目\n\n\n在spark1的的spark_home_dir下，启动主机\n```\n./sbin/start-master.sh\n```\n\n在spark2的的spark_home_dir下，启动从机的worker\n```\n./sbin/start-slave.sh spark://spark1:7077\n```\n\n在spark3的的spark_home_dir下，启动从机的worker\n```\n./sbin/start-slave.sh spark://spark1:7077\n```\n\n启动成功后，用浏览器访问 http://spark1:8080 ，查看spark-standalone集群。\n\n备注：<master-spark-URL> 的文字格式应为  \"spark://master_ip:7077\"\n\n# spark machine learning api\n[link](https://spark.apache.org/docs/latest/mllib-guide.html)\n\n# 脚本启动spark-hadoop集群\n\n```\n### cluster_setup.sh\n###  location:   spakr1:/home/spark/spark-2.4.0-bin-hadoop2.7/cluster_setup.sh\n#! /bin/bash\necho \"set up spark cluster and hadoop cluster\"\n\n/home/spark/spark-2.4.0-bin-hadoop2.7/sbin/start-master.sh\nssh spark@slave1 \"/home/spark/spark-2.4.0-bin-hadoop2.7/sbin/start-slave.sh spark://spark1:7077\"\nssh spark@slave2 \"/home/spark/spark-2.4.0-bin-hadoop2.7/sbin/start-slave.sh spark://spark1:7077\"\n/home/spark/spark-2.4.0-bin-hadoop2.7/sbin/start-master.sh spark://spark1:7077\n\n/home/spark/hadoop-2.7.7/sbin/start-all.sh\nssh spark@slave1 \"/home/spark/hadoop-2.7.7/sbin/start-all.sh\"\nssh spark@slave2 \"/home/spark/hadoop-2.7.7/sbin/start-all.sh\"\n\n```\n\n# 配置jupyter运行spark\n```\nPYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS=\"notebook  --ip=192.168.10.170\"  pyspark\n```\n不过我们可以把这两个加入环境变量中去,在~/.bashrc加入以下内容即可\n\n```\nexport PYSPARK_DRIVER_PYTHON=jupyter \nexport PYSPARK_DRIVER_PYTHON_OPTS=\"notebook  --ip=192.168.10.170\" \n```\n更新环境变量\n```\nsource  ~/.bashrc\n```\n以后直接在shell中直接输入pyspark即可。\n\n# 在pycharm调用pyspark\n\n需要在Run/Debug Configurations设置环境变量\n```\nSPARK_HOME=/home/spark/spark-2.4.0-bin-hadoop2.7\nPYSPARK_DRIVER_PYTHON=\nPYSPARK_PYTHON=/usr/bin/python3\nMASTER=spark://192.168.10.170:7077\n```\n\n# Transformation操作\n\n|Transformation\t|Transformation\tMeaning|\n|-------|------|\n|map(func)|利用函数func处理原DStream的每个元素，返回一个新的DStream|\n|flatMap(func)|\t与map相似，但是每个输入项可用被映射为0个或者多个输出项|\n|filter(func)\t|返回一个新的DStream，它仅仅包含源DStream中满足函数func的项|\n|repartition(numPartitions)\t|通过创建更多或者更少的partition改变这个DStream的并行级别(level of parallelism)|\n|union(otherStream)|\t返回一个新的DStream,它包含源DStream和otherStream的联合元素|\n|count()\t|通过计算源DStream中每个RDD的元素数量，返回一个包含单元素(single-element)RDDs的新DStream|\n|reduce(func)|\t利用函数func聚集源DStream中每个RDD的元素，返回一个包含单元素(single-element)RDDs的新DStream。函数应该是相关联的，以使计算可以并行化|\n|countByValue()\t|这个算子应用于元素类型为K的DStream上，返回一个（K,long）对的新DStream，每个键的值是在原DStream的每个RDD中的频率。|\n|reduceByKey(func, [numTasks])\t|当在一个由(K,V)对组成的DStream上调用这个算子，返回一个新的由(K,V)对组成的DStream，每一个key的值均由给定的reduce函数聚集起来。注意：在默认情况下，这个算子利用了Spark默认的并发任务数去分组。你可以用numTasks参数设置不同的任务数|\n|join(otherStream, [numTasks])\t|当应用于两个DStream（一个包含（K,V）对,一个包含(K,W)对），返回一个包含(K, (V, W))对的新DStream|\n|cogroup(otherStream, [numTasks])\t|当应用于两个DStream（一个包含（K,V）对,一个包含(K,W)对），返回一个包含(K, Seq[V], Seq[W])的元组|\n|transform(func)\t|通过对源DStream的每个RDD应用RDD-to-RDD函数，创建一个新的DStream。这个可以在DStream中的任何RDD操作中使用|\n|updateStateByKey(func)\t|利用给定的函数更新DStream的状态，返回一个新\"state\"的DStream。Meaning|\n\n\n# spark 基本函数操作\n\n使用parallelize对array对象并行化\n\n```\nspark@spark1:~/spark-2.4.0-bin-hadoop2.7/test_app_build$ spark-shell\n18/12/13 01:41:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSpark context Web UI available at http://spark1:4040\nSpark context available as 'sc' (master = local[*], app id = local-1544665284869).\nSpark session available as 'spark'.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.0\n      /_/\n\nUsing Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_191)\nType in expressions to have them evaluated.\nType :help for more information.\nscala> val lines = sc.parallelize(Array(\"hello\",\"spark\",\"hello\",\"world\",\"!\"))\nlines: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:24\n```\n\n使用foreache和println内置函数对lines的元素进行打印输出\n\n```\nscala> lines.foreach(println)\nhello\nworld\nspark\n!\nhello\n\n```\n\n是用map内置函数对array对象元素进行更改化为key-value类型\n```\nscala> val lines2 = lines.map(world => (world,1))\nlines2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[1] at map at <console>:25\n\nscala> lines2.foreach(println)\n(hello,1)\n(spark,1)\n(hello,1)\n(world,1)\n(!,1)\n```\n\n使用filter内置函数对array对象进行条件筛选。\n```\nscala> val lines3 = lines.filter(word => word.contains(\"hello\"))\n\nscala> lines3.foreach(println)\nhello\nhello\n```\n\n## 集合操作\n进入spark-shell并读取文件\n```\nspark@spark1:~/spark-2.4.0-bin-hadoop2.7$ spark-shell\n18/12/13 02:55:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n18/12/13 02:56:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\nSpark context Web UI available at http://spark1:4041\nSpark context available as 'sc' (master = local[*], app id = local-1544669763113).\nSpark session available as 'spark'.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.0\n      /_/\n\nUsing Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_191)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> val inputs= sc.textFile(\"data.txt\")\ninputs: org.apache.spark.rdd.RDD[String] = data.txt MapPartitionsRDD[1] at textFile at <console>:24\n\nscala> inputs.foreach(println)\nhello I am hear ,\nhello word !\nhello spark ,\nhello ethan .\n```\n\nflatMap内置函数对每一行进行操作\n```\nscala> val lines = input\ninput_file_name   inputs\n\nscala> val lines = inputs.flatMap(line => line.split(\" \"))\nlines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at <console>:25\n\nscala> lines.foreach(println)\nhello\nword\n!\nhello\nspark\n,\nhello\nethan\n.\nhello\nI\nam\nhear\n,\n```\n\n初始化两个rdd集合，以便后续做集合运算\n```\nscala> var rdd1 = sc.parallelize(Array(\"coffe\",\"coffe\",\"panda\",\"monkey\",\"tea\"))\nrdd1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[3] at parallelize at <console>:24\n\nscala> rdd1.foreach(println)\npanda\ncoffe\ncoffe\nmonkey\ntea\n\nscala> val rdd2  = sc.parallelize(Array(\"coffe\",\"monkey\",\"kitty\"))\nrdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[4] at parallelize at <console>:24\n\nscala> rdd2.foreach(println)\ncoffe\nmonkey\nkitty\n\n```\n对rdd1 元素去重操作\n```\nscala> var rdd_distinct = rdd1.distinct()\nrdd_distinct: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[7] at distinct at <console>:25\n\nscala> rdd_distinct.foreach(println)\n\ntea\nmonkey\npanda\ncoffe\n```\n求两集合的交集\n\n```\nscala> val rdd_union = rdd1.union(rdd2)\nrdd_union: org.apache.spark.rdd.RDD[String] = UnionRDD[8] at union at <console>:27\n\nscala> rdd_union.foreach(println)\ncoffe\npanda\ncoffe\nmonkey\ntea\nmonkey\ncoffe\nkitty\n```\n\n求两集合的交集\n```\nscala> val rdd_inter= rdd1.intersection(rdd2)\nrdd_inter: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[14] at intersection at <console>:27\n\nscala> rdd_inter\nres9: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[14] at intersection at <console>:27\n\nscala> rdd_inter.foreach(println)\nmonkey\ncoffe\n```\n\n求集合的子集\n\n\n```\nscala> val rdd_sub=rdd1.subtract(rdd2)\nrdd_sub: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[18] at subtract at <console>:27\n\nscala> rdd_sub.foreach(println)\npanda\ntea\n```\n\n## key values 值操作\n\n\n读取文件，形成key-values数据\n```\nscala> val rdd=sc.textFile(\"data.txt\")\nrdd: org.apache.spark.rdd.RDD[String] = data.txt MapPartitionsRDD[1] at textFile at <console>:24\n\nscala> rdd.foreach(println)\nhello I am hear ,\nhello word !\nhello spark ,\nhello ethan .\n\nscala> val rdd2 = rdd.map(line => (line.split(\" \")(0),line))\nrdd2: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[2] at map at <console>:25\n\n\nscala> rdd2.foreach(println)\n(hello,hello I am hear ,)\n(hello,hello word !)\n(hello,hello spark ,)\n(hello,hello ethan .)\n```\nreduceByKey\n\n```\nscala> val rdd3 = sc.parallelize(Array((1,2),(3,4),(3,6)))\nrdd3: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[3] at parallelize at <console>:24\n\n\nscala> rdd3.foreach(println)\n(3,4)\n(1,2)\n(3,6)\n\n\nscala> val rdd4 =rdd3.reduceByKey((x,y)=>x+y)\nrdd4: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[4] at reduceByKey at <console>:25\n\nscala> rdd4.foreach(println)\n(1,2)\n(3,10)\n```\ngroupByKey\n```\n\nscala> val rdd5=rdd3.groupByKey()\nrdd5: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[6] at groupByKey at <console>:25\n\nscala> rdd5.foreach(println)\n(3,CompactBuffer(4, 6))\n(1,CompactBuffer(2))\n```\n查看rdds key值\n```\nscala> val rdd6=rdd3.keys\nrdd6: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[7] at keys at <console>:25\n\nscala> rdd6.foreach(println)\n1\n3\n3\n```\nsortByKey\n\n```\nscala> val rdd7=rdd3.sortByKey()\nrdd7: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[10] at sortByKey at <console>:25\n\nscala> rdd7.foreach\nforeach   foreachAsync   foreachPartition   foreachPartitionAsync\n\nscala> rdd7.foreach(println)\n(1,2)\n(3,4)\n(3,6)\n```\ncombineByKey()：(creatCombiner,mergeValue,mergeCombiners,partitioner)\n遍历partiton中的元素，元素的key,要么之前见过的，要么不是。\n如果是新元素，适应我们提供的creatCombiner()函数，\n如果是这个partition中已经存在的Key,就会使用mergeValue()函数，\n举例，求分数平均值\n\n```\nscala> val scores = sc.parallelize(Array((\"jake\",80.0),(\"jake\",90.0),(\"jake\",85.0),(\"mike\",85.0),(\"mike\",92.0),(\"mike\",90.0)))\nscores: org.apache.spark.rdd.RDD[(String, Double)] = ParallelCollectionRDD[11] at parallelize at <console>:24\n\nscala> scores.for\nforeach   foreachAsync   foreachPartition   foreachPartitionAsync   formatted\n\nscala> scores.foreach\nforeach   foreachAsync   foreachPartition   foreachPartitionAsync\n\nscala> scores.foreach(println)\n(jake,90.0)\n(jake,80.0)\n(jake,85.0)\n(mike,85.0)\n(mike,90.0)\n(mike,92.0)\n\n\nscala> val score2=scores.combineByKey(score => (1,score),(c1:(Int,Double),newScore)=>(c1._1+1,c1._2+newScore),(c1:(Int,Double),c2:(Int,Double))=>(c1._1+c2._1,c1._2+c2._2))\nscore2: org.apache.spark.rdd.RDD[(String, (Int, Double))] = ShuffledRDD[12] at combineByKey at <console>:25\n\nscala> score2.foreach(println)\n(jake,(3,255.0))\n```\n\n\n\n\n配置spark JDBC\n\n## 下载  mysql-connector jar\nhttps://dev.mysql.com/downloads/file/?id=480090\n\n\n为了让 Spark 能用上 MySQL 服务器，我们需要驱动程序 Connector/J for MySQL. 下载这个压缩文件解压后拷贝 mysql-connector-java-5.1.39-bin.jar 到 spark 目录，然后在 conf/spark-defaults.conf 中添加类路径，如下：\n\nspark.driver.extraClassPath = /usr/local/spark/mysql-connector-java-5.1.47-bin.jar\nspark.executor.extraClassPath = /usr/local/spark/mysql-connector-java-5.1.47-bin.jar\n\n# Spark SQL\n\n## 创建SparkSession\n```\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Python Spark SQL basic example\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n```\n## 创建 DataFrames\n```\n# spark is an existing SparkSession\ndf = spark.read.json(\"examples/src/main/resources/people.json\")\n# Displays the content of the DataFrame to stdout\ndf.show()\n# +----+-------+\n# | age|   name|\n# +----+-------+\n# |null|Michael|\n# |  30|   Andy|\n# |  19| Justin|\n# +----+-------+\n```\n\n## DataSet 操作\n\n```\n# spark, df are from the previous example\n# Print the schema in a tree format\ndf.printSchema()\n# root\n# |-- age: long (nullable = true)\n# |-- name: string (nullable = true)\n\n# Select only the \"name\" column\ndf.select(\"name\").show()\n# +-------+\n# |   name|\n# +-------+\n# |Michael|\n# |   Andy|\n# | Justin|\n# +-------+\n\n# Select everybody, but increment the age by 1\ndf.select(df['name'], df['age'] + 1).show()\n# +-------+---------+\n# |   name|(age + 1)|\n# +-------+---------+\n# |Michael|     null|\n# |   Andy|       31|\n# | Justin|       20|\n# +-------+---------+\n\n# Select people older than 21\ndf.filter(df['age'] > 21).show()\n# +---+----+\n# |age|name|\n# +---+----+\n# | 30|Andy|\n# +---+----+\n\n# Count people by age\ndf.groupBy(\"age\").count().show()\n# +----+-----+\n# | age|count|\n# +----+-----+\n# |  19|    1|\n# |null|    1|\n# |  30|    1|\n# +----+-----+\n```\n\n## SQL 查询编程\n\n```\n// Register the DataFrame as a SQL temporary view\ndf.createOrReplaceTempView(\"people\")\n\nval sqlDF = spark.sql(\"SELECT * FROM people\")\nsqlDF.show()\n// +----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+\n```\n\n## Global Temporary View\n\n```\n# Register the DataFrame as a global temporary view\ndf.createGlobalTempView(\"people\")\n\n# Global temporary view is tied to a system preserved database `global_temp`\nspark.sql(\"SELECT * FROM global_temp.people\").show()\n# +----+-------+\n# | age|   name|\n# +----+-------+\n# |null|Michael|\n# |  30|   Andy|\n# |  19| Justin|\n# +----+-------+\n\n# Global temporary view is cross-session\nspark.newSession().sql(\"SELECT * FROM global_temp.people\").show()\n# +----+-------+\n# | age|   name|\n# +----+-------+\n# |null|Michael|\n# |  30|   Andy|\n# |  19| Justin|\n# +----+-------+\n```\n\n## JDBC  连接创建\n\n```\njdbcDF = spark.read.format(\"jdbc\").option(\"url\",\"jdbc:mysql://192.168.10.170:3306\").option(\"dbtable\",\"HKIA_v1.flight_schedule\").option(\"user\", \"hkia_v1\").option(\"password\", \"Asdf168!!\").load()\n\njdbcDF.printSchema()\njdbcDF.show()\n```\n\n\n\n\n\n\n```python\njdbcDF = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:mysql://192.168.10.170:3306\").option(\"dbtable\", \"HKIA_v1.flight_schedule\").option(\"user\", \"hkia_v1\").option(\"password\", \"Asdf168!!\").load()\n```\n\n\n```python\njdbcDF.show()\n```\n\n\n    ---------------------------------------------------------------------------\n\n    Py4JJavaError                             Traceback (most recent call last)\n\n    <ipython-input-2-d4912bf3e00e> in <module>()\n    ----> 1 jdbcDF.show()\n    \n\n    /home/spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/dataframe.py in show(self, n, truncate, vertical)\n        376         \"\"\"\n        377         if isinstance(truncate, bool) and truncate:\n    --> 378             print(self._jdf.showString(n, 20, vertical))\n        379         else:\n        380             print(self._jdf.showString(n, int(truncate), vertical))\n\n\n    /home/spark/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in __call__(self, *args)\n       1255         answer = self.gateway_client.send_command(command)\n       1256         return_value = get_return_value(\n    -> 1257             answer, self.gateway_client, self.target_id, self.name)\n       1258 \n       1259         for temp_arg in temp_args:\n\n\n    /home/spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py in deco(*a, **kw)\n         61     def deco(*a, **kw):\n         62         try:\n    ---> 63             return f(*a, **kw)\n         64         except py4j.protocol.Py4JJavaError as e:\n         65             s = e.java_exception.toString()\n\n\n    /home/spark/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)\n        326                 raise Py4JJavaError(\n        327                     \"An error occurred while calling {0}{1}{2}.\\n\".\n    --> 328                     format(target_id, \".\", name), value)\n        329             else:\n        330                 raise Py4JError(\n\n\n    Py4JJavaError: An error occurred while calling o38.showString.\n    : org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: KILLED\n    \tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n    \tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n    \tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n    \tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n    \tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n    \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n    \tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n    \tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n    \tat scala.Option.foreach(Option.scala:257)\n    \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n    \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n    \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n    \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n    \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n    \tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n    \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n    \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n    \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n    \tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n    \tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n    \tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)\n    \tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n    \tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n    \tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n    \tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n    \tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n    \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n    \tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n    \tat org.apache.spark.sql.Dataset.head(Dataset.scala:2545)\n    \tat org.apache.spark.sql.Dataset.take(Dataset.scala:2759)\n    \tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\n    \tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\n    \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    \tat java.lang.reflect.Method.invoke(Method.java:498)\n    \tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n    \tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n    \tat py4j.Gateway.invoke(Gateway.java:282)\n    \tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    \tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n    \tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n    \tat java.lang.Thread.run(Thread.java:748)\n\n\n\n","tags":["saprk"],"categories":["大数据处理"]},{"title":"自然语言处理-三大特征处理器.md","url":"/2019/08/21/自然语言处理-三大特征处理器-md/","content":"nlp的输入往往是一句话，或者一篇文章，所以它有以下几个特点：\n- 输入是个一维线性序列。\n- 单次或者句子的相对位置关系很重要，两个单次位置互换可能导致完全不同的意思。\n\n对于特征抽取器来说，能够具备长距离特征捕获能力是解决NLP任务的关键点。\n\n一个特征提取器是否匹配问题领域的特点，有时候决定了它的成败，而很多模型改进的方向，其实就是改造得使它更匹配领域问题的特性。\n\n自然语言处理四大分类任务：\n- 序列标注： 分词，词性标注,tag,命名实体识别,语义角色标注\n- 分类任务： 文本分类，情感计算\n- 句子关系判断 ： Etailment ,  QA ,自然语言推理，语义改写\n- 生成式任务： 机器翻译 , 文本摘要，写诗造句，看图说话\n\n从模型角度来看，特征抽取器的能力决定了是否能接到解决以上这些任务。深度学习最大的优点是端到端，相比以前研发人员需要考虑设计抽取哪些特征，而到深度学习时代，这些细节都不需要考虑了，只需要吧原始输入扔给好的特征提取器，它自己会吧有用的特征提取出来。\n![](../image/rnn.png)\n \n RNN模型结构参数如上图，核心是每个输入对应隐层节点，而隐层节点之间形成了线性序列,信息由前向后再隐层之间逐步向后传递。但原始的RNN也存在问题，它采取线性序列结构不断从前往后收集输入信息，但这种线性序列结构在反向传播的时候存在优化困难问题，因为反向传播路径太长，容易导致严重的`梯度消失`或`梯度爆炸`问题。\n 为了解决这个问题，后来引入了LSTM和GRU模型，通过增加中间状态信息直接向后传播，以此缓解梯度消失问题，获得了很好的效果，于是很快LSTM和GRU成为RNN的标准模型。\n 经过不断优化，后来NLP又从图像领域借鉴并引入了attention机制,叠加网络层吧层深做深，以及引入Encoder-Decoder 框架，这些技术进展极大拓展了RNN的能力以及应用效果。\n \n RNN面临的两个问题：\n 1. 新的模型性能要比rnn模型效果更佳，比如经过特殊改造过的CNN模型，以及Transformer.\n 2. RNN本身的序列依赖结构对于大规模并行计算来说相当不友好，不具备高效并行计算能力。\n \n RNN并行能力差的原因：\n RNN在T时刻隐层状态的计算，依赖两个输入，一个是T时刻的句子输入单词，另一个输入是隐含层St还依赖 T-1时刻隐层状态S(t-1)的输出，\n 这是最能体现RNN本质特征的一点，RNN的历史信息是通过这个信息传输渠道往后传输的。这种循环依赖会限制rnn 在并行计算方面的能力。\n \n CNN模型\n ![](../image/cnn.png)\n \n   一般而言，输入的字或词用word Embedding 的方式表达，这样本来一维的文本信息输入就转化为二维的输入结构，假设，输入X包含n个字符，而每个字符的 word Embeding 的长度为d,那么输入就是d*n的二维向量。\n 卷积层本质上是一个特征提取层，可以设定超参数F来指定卷积层包含多少个卷积核（Filter).\n\n   对于某个Filter来说，可以想象有一个d*k大小的移动窗口从输入矩阵的第一个字开始不断往后移动，其中k是Filter指定的窗口大小，d是Word Embedding长度。对于某个时刻的窗口，通过神经网络的非线性变换，将这个窗口内的输入值转换为某个特征值，随着窗口不断往后移动，这个Filter对应的特征值不断产生，形成这个Filter的特征向量。这就是卷积核抽取特征的过程。卷积层内每个Filter都如此操作，就形成了不同的特征序列。Pooling 层则对Filter的特征进行降维操作，形成最终的特征。一般在Pooling层之后连接全联接层神经网络，形成最后的分类过程。\n CNN捕获到的是什么特征呢？从上述怀旧版本CNN卷积层的运作机制你大概看出来了，关键在于卷积核覆盖的那个滑动窗口，CNN能捕获到的特征基本都体现在这个滑动窗口里了。\n    大小为k的滑动窗口轻轻的穿过句子的一个个单词，荡起阵阵涟漪，那么它捕获了什么?其实它捕获到的是单词的k-gram片段信息，这些k-gram片段就是CNN捕获到的特征，k的大小决定了能捕获多远距离的特征。\n \n ![](../image/single_cnn.png)\n  \n 对于远距离特征，单层CNN是无法捕获到的，如果滑动窗口k最大为2，而如果有个远距离特征距离是5，那么无论上多少个卷积核，都无法覆盖到长度为5的距离的输入，所以它是无法捕获长距离特征的。\n \n ![](../image/long_distance_cnn.png)\n\n  那么怎样才能捕获到长距离的特征呢？有两种典型的改进方法：一种是假设我们仍然用单个卷积层，滑动窗口大小k假设为3，就是只接收三个输入单词，但是我们想捕获距离为5的特征，怎么做才行？显然，如果卷积核窗口仍然覆盖连续区域，这肯定是完不成任务的。提示一下：你玩过跳一跳是吧？能采取类似策略吗？对，你可以跳着覆盖呀，是吧？这就是Dilated 卷积的基本思想，确实也是一种解决方法。\n  \n  ![](../image/deep_cnn.png) \n  \n  第二种方法是把深度做起来。第一层卷积层，假设滑动窗口大小k是3，如果再往上叠一层卷积层，假设滑动窗口大小也是3，但是第二层窗口覆盖的是第一层窗口的输出特征，所以它其实能覆盖输入的距离达到了5。如果继续往上叠加卷积层，可以继续增大卷积核覆盖输入的长度。\n  \n  \n  RNN因为是线性序列结构，所以很自然它天然就会把位置信息编码进去；那么，CNN是否能够保留原始输入的相对位置信息呢？我们前面说过对于NLP问题来说，位置信息是很有用的。其实CNN的卷积核是能保留特征之间的相对位置的，道理很简单，滑动窗口从左到右滑动，捕获到的特征也是如此顺序排列，所以它在结构上已经记录了相对位置信息了\n  \n  但是如果卷积层后面立即接上Pooling层的话，Max Pooling的操作逻辑是：从一个卷积核获得的特征向量里只选中并保留最强的那一个特征，所以到了Pooling层，位置信息就被扔掉了，这在NLP里其实是有信息损失的。所以在NLP领域里，目前CNN的一个发展趋势是抛弃Pooling层，靠全卷积层来叠加网络深度，这背后是有原因的\n  \n  \n  目前主流NLP的cnn：\n  \n  ![](../image/main_flow_cnn.png)\n  \n  上图展示了在NLP领域能够施展身手的摩登CNN的主体结构，通常由1-D卷积层来叠加深度，使用Skip Connection来辅助优化，也可以引入Dilated CNN等手段。\n  比如ConvS2S主体就是上图所示结构，Encoder包含 15个卷积层，卷积核kernel size=3，覆盖输入长度为25。当然对于ConvS2S来说，卷积核里引入GLU门控非线性函数也有重要帮助，限于篇幅，这里不展开说了，GLU貌似是NLP里CNN模型必备的构件，值得掌握。\n  \n  除此外，简单谈一下CNN的位置编码问题和并行计算能力问题。上面说了，CNN的卷积层其实是保留了相对位置信息的，只要你在设计模型的时候别手贱，中间层不要随手瞎插入Pooling层，问题就不大，不专门在输入部分对position进行编码也行。但是也可以类似ConvS2S那样，专门在输入部分给每个单词增加一个position embedding，将单词的position embedding和词向量embedding叠加起来形成单词输入，这样也可以，也是常规做法。\n  \n   至于CNN的并行计算能力，那是非常强的，这其实很好理解。我们考虑单层卷积层，首先对于某个卷积核来说，每个滑动窗口位置之间没有依赖关系，所以完全可以并行计算；另外，不同的卷积核之间也没什么相互影响，所以也可以并行计算。CNN的并行度是非常自由也非常高的，这是CNN的一个非常好的优点。\n   \n   Transformer 模型：\n   \n   Transformer是谷歌在17年做机器翻译任务的“Attention is all you need”的论文中提出的，引起了相当大的反响。 每一位从事NLP研发的同仁都应该透彻搞明白Transformer，它的重要性毫无疑问。\n\n\n\n\n\n","tags":["nlp"]},{"title":"neo4j 学习笔记","url":"/2019/08/16/neo4j_note/","content":"[TOC]: # \"目录 \"\n\n# Neo4j的特点\nSQL一样容易查询语言的Neo4j CQL\n\n它遵循属性图数据模型\n\n它通过使用Apache Lucence支持索引\n\n它支持UNIQUE约束\n\n它包含一个UI执行CQL指令：Neo4j的数据浏览器\n\n它支持完整的ACID（原子性，一致性，隔离性和持久性）规则\n\n它采用原生图形库与本地GPE（图形处理引擎）\n\n它支持查询的数据导出到JSON和XLS格式\n\n它提供了REST API由如Java，春天，斯卡拉等任何编程语言进行访问\n\n它提供了Java Script支持以任何UI MVC框架如Node JS进行访问。\n\n它支持两种类型的Java API：Cypher支架API和本地Java API来开发Java应用程序。\n\n# Neo4j的优势\n这是很容易以表示连接数据。\n\n这是很容易和更快的检索更多的数据/遍历/导航。\n\n它表示半结构化数据变得非常容易。\n\nNeo4j的CQL查询语言命令在人性化的可读格式，非常简单易学。\n\n它使用简单，功能强大的数据模型。\n\n它不需要复杂的连接来获取连接/相关的数据，因为它是非常方便地检索它的相邻节点或关系的细节没有加入或索引。\n\n\n# [创建节点](http://neo4j.com.cn/public/cypher/neo4j_cql_create_node.html)\nNeo4j的CQL“Create”命令使用不带属性来创建节点。 它只是创造没有任何数据的节点。\n\nCREATE命令语法\nCREATE (<node-name>:<label-name>)\n语法说明\n\n语法元素\t描述\n创建\t这是一个Neo4j的定制列表命令。\n<节点名称>\t这是我们要创建一个节点名称。\n<标签名称>\t这是一个节点的标签名称\n要记住的事情 -\n\nNeo4j的数据库服务器使用该<节点名称>以存储Database.As一个Neo4j的DBA或开发该节点的详细信息，我们不能用它来访问节点的详细信息。\n\nNeo4j的数据库服务器创建一个<标签名称>作为别名到内部节点name.As一个Neo4j的DBA或开发人员，我们应该利用这个标签名称访问节点的详细信息。\n\n## 创建一个简单的“雇员”节点\n```\nCREATE (emp:Employee)\n\n```\n\n\n## 创建一个简单的“部门”节点\n````\nCREATE (dept:Dept)\n````\n\n\n## Neo4j的CQL创建节点具有属性\nNeo4j的CQL“Create”命令用于创建具有属性节点。 它创建了一些属性（键 - 值对）来存储数据的节点。\n\nCREATE命令语法：\nCREATE (\n   <node-name>:<label-name>\n   { \t\n      <Property1-name>:<Property1-Value>\n      ........\n      <Propertyn-name>:<Propertyn-Value>\n   }\n)\n语法说明\n\n语法元素\t描述\n<节点名称>\t这是我们要创建一个节点名称。\n<标签名称>\t这是一个节点的标签名称\n<Property1名> ... <propertyN中名称>\t属性是键 - 值对。 定义了将要分配给创建节点的属性的名称\n<Property1值> ... <propertyN中值>\t属性是键 - 值对。 定义了将要分配给一个创建节点的属性的值\n\n## 创建一个部门节点的一些属性（DEPTNO，DNAME，位置）\n````\nCREATE (dept:Dept { deptno:10,dname:\"Accounting\",location:\"Hyderabad\" })\n````\n在这里，属性名称是DEPTNO，DNAME，位置\n\n属性值是10，“会计学”，“海得拉巴”\n\n正如我们所讨论的，属性的名称 - 值对。\n\n属性= DEPTNO：10\n\n由于DEPTNO是一个整数属性，所以我们没有使用单引号或双引号来界定它的价值10。\n\n由于DNAME和位置都是String类型的属性，所以我们使用了单引号或双引号来界定它的价值10。\n\n## 创建具有某些属性（ID，姓名，SAL，DEPTNO）劳动者节点\n\n````\nCREATE (emp:Employee{id:123,name:\"Lokesh\",sal:35000,deptno:10})\n````\n这里是EMP节点名称\n\n员工是部门节点的标签名称\n新增1标签，创建1个节点，设置4个属性，返回0行\n\n此命令创建一个节点“EMP”有4个属性（“ID”，“姓名”，“萨尔”，“DEPTNO”），并分配一个标签“雇员”。\n\n\n# match命令\n\nNeo4j的CQL MATCH命令用于 -\n\n要获取有关数据库节点和属性数据\n要获得有关节点，从数据库中的关系和属性数据\n\n\n## MATCH命令语法：\nMATCH \n(\n   <node-name>:<label-name>\n)\n语法说明\n\n语法元素\t描述\n<节点名称>\t这是我们要创建一个节点名称。\n<标签名称>\t这是一个节点的标签名称\n要记住的事情 -\n\nNeo4j的数据库服务器使用该<节点名称>以存储Database.As一个Neo4j的DBA或开发该节点的详细信息，我们不能用它来访问节点的详细信息。\n\nNeo4j的数据库服务器创建一个<标签名称>作为别名到内部节点name.As一个Neo4j的DBA或开发人员，我们应该利用这个标签名称访问节点的详细信息。\n\n注-我们不能用match命令只身从数据库中检索数据。 如果我们只使用它，那么我们将InvalidSyntax错误。\n\n# return子句\nNeo4j的CQL RETURN子句用于 -\n\n要检索节点的一些特性\n要检索节点的所有属性\n要检索节点和关联关系的一些性质\n要检索节点和关联关系的所有属性\n\n## 返回的命令语法：\nRETURN \n   <node-name>.<property1-name>,\n   ........\n   <node-name>.<propertyn-name>\n语法说明\n\n语法元素\t描述\n<节点名称>\t这是我们要创建一个节点名称。\n<Property1名> ... <propertyN中名称>\t属性是键 - 值对。 <属性名称>定义了将要分配给创建节点的属性的名称\n\n\n#  MATCH及return\n在Neo4j的CQL，我们不能用MATCH或RETURN单独命令，所以我们要结合这两个命令来检索数据库中的数据。\n\nNeo4j的CQL MATCH + RETURN命令用于 -\n\n要检索节点的一些特性\n要检索节点的所有属性\n要检索节点和关联关系的一些性质\n要检索节点和关联关系的所有属性\n\n\n## 匹配返回命令语法：\nMATCH Command\nRETURN Command\n语法说明\n\n语法元素\t描述\nmatch命令\t这是Neo4j的CQL MATCH命令。\n返回指令\t这是Neo4j的CQL RETURN命令。\nMATCH命令语法：\nMATCH \n(\n   <node-name>:<label-name>\n)\n语法说明\n\n语法元素\t描述\n<节点名称>\t这是我们要创建一个节点名称。\n<标签名称>\t这是一个节点的标签名称\n要点 -\n\nNeo4j的数据库服务器使用该<节点名称>以存储Database.As一个Neo4j的DBA或开发该节点的详细信息，我们不能用它来访问节点的详细信息。\n\nNeo4j的数据库服务器创建一个<标签名称>作为别名到内部节点name.As一个Neo4j的DBA或开发人员，我们应该利用这个标签名称访问节点的详细信息。\n\n## 返回的命令语法：\nRETURN \n   <node-name>.<property1-name>,\n   ...\n   <node-name>.<propertyn-name>\n语法说明\n\n语法元素\t描述\n<节点名称>\t这是我们要创建一个节点名称。\n<Property1名> ... <propertyN中名称>\t属性是键 - 值对。 定义了将要分配给创建节点的属性的名称\n\n\n检索一些属性（DEPTNO，DNAME）从数据库部节点的数据。\n\n````\nMATCH (dept: Dept)\nRETURN dept.deptno,dept.dname\n````\n\ndept是一个节点名\nDept是部节点标签名称\ndeptno是部门节点的属性名称\ndname是部门节点的属性名称\n\n\n\n检索所有属性（DEPTNO，DNAME，位置）从数据库部节点的数据\n\n````\nMATCH (dept: Dept)\nRETURN dept.deptno,dept.dname,dept.location\n````\n\ndept是一个节点名\nDept是部节点标签名称\nDEPTNO是部门节点的属性名称\nDNAME是部门节点的属性名称\nlocation是部门节点的属性名称\n\n如何在没有指定其属性从数据库中检索部节点的数据\n\n````\nMATCH (dept: Dept)\nRETURN dept\n````\n\n#  CREATE + MATCH + RETURN\n创建两个节点与两个节点之间的属性和关系。\n\n我们将创建两个节点：客户和信用卡式人际关系。\n\n- 客户节点包含：ID，姓名，出生日期属性\n\n- 信用卡式节点包括：身份证，号码，CVV，性能EXPIREDATE\n\n- 客户信用卡式的关系：DO_SHOPPING_WITH\n\n- 要的信用卡客户关系：ASSOCIATED_WITH\n\n\n## 步骤：\n\n1. 创建客户节点\n2. 创建信用卡式节点\n3. 观察先前创建的两个节点：客户和信用卡式\n4. 创建客户与信用卡式节点之间的关系\n5. 查看新创建的关系的细节\n6. 详细查看每个节点和关系属性\n\n### 创建客户节点\n\n````\nCREATE (e:Customer{id:\"1001\",name:\"Abc\",dob:\"01/10/1982\"})\n````\n- e 是一个节点名\n- Custoomer 是一个节点的标签名称\n- id,name，dob 是客户节点的属性名称\n\n### 创建信用卡式节点\n''''\nCREATE (c:CreditCard{id:\"5001\",number:\"1234567890\",cvv:\"888\",expiredate:\"20/17\"})\n\n''''\n- c是一个节点名\n\n- CreditCard是一个节点的标签名称\n\n- 身份证，号码，CVV和EXPIREDATE是信用卡式节点的属性名称\n\n\n### 观察节点\n\n我们已经创建了两个节点：客户和信用卡式,我们需要通过使用带有RETURN子句的Neo4j CQL MATCH命令来查看这两个节点的详细信息\n````\nMATCH (e:Customer)\nRETURN e.id,e.name,e.dob\n````\n### 查看信用卡式节点详细信息\n````\nMATCH (c:CreditCard)\nRETURN c.id,c.number,c.cvv,c.expiredate\n\n````\n\n# 基本关系\nNeo4j的图形数据库如下属性图模型来存储和管理数据。\n\n按物业图模型，关系应该是方向性的。 否则，Neo4j的将抛出一个错误消息。\n\n基于方向性，Neo4j的关系被分为两种主要类型。\n\n单向关系\n双向关系\n我们可以用Neo4j的CQL create命令创建在以下情况下两个节点之间的关系。 这些方案都适用于两个单向及双向关系。\n\n创建两个现有节点之间没有特性的关系\n\n创建两个现有节点之间，属性关系\n\n创建两个新的节点之间没有特性的关系\n\n创建两个新的节点之间，属性关系\n\n用WHERE子句2退出节点之间创建/关系没有属性\n\n我们要创造客户和信用卡式如下之间的关系\n\n\n\n![](../image/neo4j_create_relationship_example1.png) \n\n关系是一种“传出的关系”向“信用卡式的”节点和相同的关系是“传入的关系”的“客户”节点。\n\n我们创建了“信用卡式”和“客户”之间的节点两个关系：一是由“信用卡式”的“客户”。 另一名来自“客户”向“信用卡式”。 这意味着它是双向的关系。\n\n![](../image/neo4j_create_relationship_example2.png) \n- 创建无属性的关系随着现有节点\n\n- 创建关系具有属性随着现有节点\n\n- 无属性有了新节点创建关系\n\n- 创建关系具有属性有了新节点\n\n- 检索相关节点的详细信息\n\n\n# 创建节点标签\n标签是一个名称或标识节点或在Neo4j的数据库的关系。\n\n我们可以说此标签名称为“关系类型”的关系。\n\n我们可以使用定制列表create命令创建一个标签节点或关系与多个标签节点。 这意味着Neo4j的支持两个节点之间只有单一的关系类型。\n\n我们可以在这两种UI模式和网格模式CQL数据浏览器中观察此节点的或关系的标签名称。 同时，我们也参照执行其命令CQL。\n\n到目前为止，我们只创建一个标签，一个节点或有关系，但是我们并没有讨论关于它的语法了。\nNeo4j的CQL CREATE命令用于\n\n- 要创建一个单独的标签节点\n\n- 要创建多个标签节点\n\n- 要创建一个单独的标签有关系\n\n如何在本章中创建一个节点一个或多个标签。 我们将讨论如何创建一个单一的标签，以在下一章节的关系。\n\n## 单个标签到节点\n句法\nCREATE (<node-name>:<label-name>)\n\n\n|S.No. |\t语法元素 |\t描述 |  \n|---|---|---|\n|1。\t|创建\t|这是一个Neo4j的CQL关键字。|\n|2。\t|<节点名称>\t|它是一个节点的名称。|\n|3。\t|<标签名称>\t|这是一个节点的标签名称。|\n\n> 注意\n- 我们应该用冒号（:)运营商的节点名称和标签名称分开。\n\n- Neo4j的数据库服务器使用此名称来存储这个节点细节Database.As一个Neo4j的DBA或开发人员，我们不能用它来访问节点的详细信息\n\n- Neo4j的数据库服务器创建一个标签名称作为别名到内部节点name.As一个Neo4j的DBA或开发人员，我们应该利用这个标签名称访问节点的详细信息。\n\n例\n\n```\nCREATE (google1:GooglePlusProfile)\n```\n我们可以观察到一个标签和一个节点在Neo4j的数据库中创建。\n\n## 多个标签节点\n句法\n\nCREATE (<node-name>:<label-name1>:<label-name2>.....:<label-namen>)\n\n\n|  S.No.|语法元素|描述|\n|---|---|---|\n| 1。\t|创建\t|这是一个Neo4j的CQL关键字。 |\n|2。\t|<节点名称>\t|它是一个节点的名称。|\n|3。\t|<标签名1>，<标签名2>\t|它是一个节点的标签名称的列表。|\n\n>注意 \n- 我们应该用冒号（:)运营商的节点名称和标签名称分开。\n- 我们应该用冒号（:)运营商一个标签名称分开到另一个标签名称。\n\n创建多个标签名称为“电影”节点。\n\n影院，影片，电影，图片：由我们的客户端提供多标签名称\n\n## 单个标签有关系\n句法\n\nCREATE (<node1-name>:<label1-name>)-\n\t[(<relationship-name>:<relationship-label-name>)]\n\t->(<node2-name>:<label2-name>)\n\t\n\t语法说明\n|S.No.\t|语法元素|\t描述|\n|---|---|---|\n|1。\t|创建\t|这是一个Neo4j的CQL关键字。|\n|2。\t|<节点1名>\t|这是一个从节点的名称。|\n|3。|\t<节点2名>\t|这是A到节点的名称。|\n|4。\t|<LABEL1名称>\t|这是一个从节点的标签名称。|\n|5。|\t<LABEL1名称>\t|这是一个到节点的标签名称。|\n|6。|\t<关系名称>\t|它是一个关系的一个名字。|\n|7。|\t<相关标签名称>\t|这是一个关系的标签名称。|\n\n>  注意\n>  - 我们应该用冒号（:)运营商的节点名称和标签名称分开。\n >  - 我们应该用冒号（:)运营商的关系，名称和关系标签名称分开。\n  > - 我们应该用冒号（:)运营商一个标签名称分开到另一个标签名称。\n   >- Neo4j的数据库服务器使用此名称来存储这个节点细节Database.As一个Neo4j的DBA或开发人员，我们不能用它来访问节点的详细信息\n   >- Neo4j的数据库服务器创建一个标签名称作为别名到内部节点name.As一个Neo4j的DBA或开发人员，我们应该利用这个标签名称访问节点的详细信息。\n\n### 如何创建一个标签，一个关系\n```\nCREATE (p1:Profile1)-[r1:LIKES]->(p2:Profile2)\n```\n- p1和Profile1的都是“从节点”的节点名称和节点标签名称\n- P2和Profile2的都是的“节点”节点名称和节点标签名称\n- R1是有关系的名字,喜欢是有关系的标签名称\n\n# WHERE子句\n简单的WHERE子句语法\nWHERE <condition>\n复杂的WHERE子句语法\nWHERE <condition> <boolean-operator> <condition>\n我们可以通过使用布尔运算符将多个条件对同样的命令。 详情请参阅在Neo4j的CQL可用的布尔运算符下一节。\n\n\n<条件>的语法：\n<property-name> <comparison-operator> <value>\n语法说明\n\n|S.No.|\t语法元素|\t描述|\n|---|---|---|\n|1。\t|哪里\t|这是一个Neo4j的CQL关键字。|\n|2。\t|<属性名称>\t|这是一个节点或关系的属性名。|\n|3。\t|<比较运算符>\t|这是Neo4j的CQL比较operators.Please的一个是指在Neo4j的CQL可用比较运营商下一节。|\n|4。\t|<值>\t|这就像一些文字，字符串文字等文本值|\n\n何使用WHERE CQL在match命令子句检索基于员工的名字雇员的详细信息。\n\n````\nMATCH (emp:Employee)\nRETURN emp.empid,emp.name,emp.salary,emp.deptno\n\n````\n\n```\nMATCH (emp:Employee) \nWHERE emp.name = 'Abc'\nRETURN emp\n```\n\n\n```\nMATCH (emp:Employee) \nWHERE emp.name = 'Abc' OR emp.name = 'Xyz'\nRETURN emp\n\n```\n\n\n\n## 创建WHERE子句的关系\n\n在Neo4j的CQL，我们可以创建不同的方法拖车节点之间的关系。\n\n- 创建两个现有的节点之间的关系\n\n- 一次创建它们之间的两个节点和关系\n\n- 创建WHERE子句两个已存在节点之间的关系\n\n我们已经讨论了在前面章节前两种方法。 现在，我们将在本章中的“创建具有WHERE子句两个已存在节点之间的关系”的讨论\n\n句法\nMATCH (<node1-label-name>:<node1-name>),(<node2-label-name>:<node2-name>) \nWHERE <condition>\nCREATE (<node1-label-name>)-[<relationship-label-name>:<relationship-name>\n       {<relationship-properties>}]->(<node2-label-name>) \n       \n       \n       \n|S.No.\t|语法元素\t|描述|\n|---|---|---|\n|1。\t|MATCH，WHERE，CREATE\t|他们是Neo4j的CQL关键字。|\n|2。|\t<节点1标签名称>\t|它是用于创建关系的节点有一个标签名称。|\n|3。\t|<节点1名>\t它是用于创建关系一个节点一个名称。|\n|4。\t|<节点2标签名称>\t它是用于创建关系的节点有一个标签名称。|\n|5。\t|<节点2名>\t它是用于创建关系一个节点一个名称。|\n|6。\t|<条件>\t这是一个Neo4j的CQL WHERE子句条件。 它可以是简单的或复杂的。|\n|7。\t|<相关标签名称>|\t这是新创建的节点之一，节点两者之间关系的标签名称。|\n|8。\t|<关系名称>\t这是新创建的节点之一，节点两者之间关系的一个名字。|\n|9。\t|<关系的属性>\t|这是新创建的节点之一，节点两者之间关系的属性列表（键 - 值对）。|\n\n\n```\nMATCH (cust:Customer),(c:CreditCard) \nWHERE cust.id = \"1001\" AND c.id= \"5001\" \nCREATE (cust)-[r:DO_SHOPPING_WITH{shopdate:\"12/12/2014\",price:55000}]->(c) \nRETURN r\n```\n\n\n# 删除\n\n DELETE子句用于\n\n要删除一个节点。\n要删除一个节点和相关节点和关系。\n\n删除节点 -\n通过使用此命令，我们可以从数据库中永久删除节点及其相关属性。\n\n删除节点子句子语法\n|||\n\nDELETE <node-name-list>\n\n|S.No.\t|语法元素|\t描述|\n|----|----|----|\n|1。\t|删除\t|这是一个Neo4j的CQL关键字。|\n|2。|\t<节点名称列表>|\t它是节点名，它会从数据库中删除的列表。|\n\n\n\n\n## 删除节点和关系子句语法\n\nDELETE <node1-name>,<node2-name>,<relationship-name>\n\n\n|S.No.|\t语法元素|\t描述|\n|----|---|----|\n|1。|\t删除\t|这是一个Neo4j的CQL关键字。|\n|2。|\t<节点1名>\t|它是用于创建关系<关系名称>一端节点名称。|\n|3。\t|<节点2名>\t|它是用于创建关系<关系名称>另一个节点名称。|\n|4。|\t<关系名称>\t|这是一个关系名，这是<节点1-name>和<节点2名>之间产生。|\n\n\n从数据库中永久删除节点及其联营节点和关系。\n\n```\nMATCH (cc:CreditCard)-[r]-(c:Customer)RETURN r \n```\n\n\n观察为客户一个节点，对于信用卡式的一个节点和它们之间的关系可用。\n```\nMATCH (cc: CreditCard)-[rel]-(c:Customer) \nDELETE cc,c,rel\n```\n观察到两个节点及其相关联的10关系成功删除。\n```\nMATCH (cc:CreditCard)-[r]-(c:Customer) RETURN r\n```\n现在检查DELETE操作是否成功还是没有这样做,发现数据库中没有任何记录。\n\n\n# remove\n基于我们客户的需求，我们需要从现有节点或关系添加或删除属性,SET 子句中新的属性添加到现有节点或关系。\nremove 子句来删除节点或关系的现有属性。\n\n REMOVE命令用于\n\n- 要删除一个节点或关系标签\n- 要删除一个节点或关系的性质\n\ndelete 和 remove命令之间的相似性\n- 两个命令都不能单独使用。\n- 这两个命令都应该match命令使用。\n\n删除节点/关系属性\n我们可以使用相同的语法来删除一个节点或从数据库中永久关系的性质的属性或列表。\n\n删除属性子句语法\nREMOVE <property-name-list>\n\n|S.No.\t|语法元素|\t描述|\n|---|----|----|\n|1。\t|去掉\t|这是一个Neo4j的CQL关键字。|\n|2。\t|<属性名称列表>\t|它是属性的列表，以从节点或永久性的关系将其删除。|\n\n\n属性名称列表 《语法》\n<node-name>.<property1-name>,\n<node-name>.<property2-name>, \n.... \n<node-name>.<propertyn-name> \n\n语法说明\n\n|S.No.|\t语法元素\t|描述|\n|---|---|---|\n|1。|\t<节点名称>\t|它是一个节点的名称。|\n|2。|\t<属性名称>\t|它是一个节点的属性名。|\n\n> 我们应用逗号（，）云算法分隔标签名称列表。\n\n我们应用(.) 运算符分割节点名称和标签名称。\n\n\n创建一个节点，并从该节点从数据库中永久删除属性。\n\n```\nCREATE (book:Book {id:122,title:\"Neo4j Tutorial\",pages:340,price:250}) \n```\n也可以这么写\n\n```\nCREATE TABLE BOOK(\n\tid number,\n\ttitle varchar2(20),\n\tpages number,\n\tprice number\n);\nINSERT INTO BOOK VALUES (122,'Neo4j Tutorial',340,251);\n```\n\n\n\n删除 price属性\n\n```\nMATCH (book { id:122 })\nREMOVE book.price\nRETURN book\n```\n\n\n\n# set语法\n提供SET子句来执行下列操作。\n- 新属性添加到现有的节点或关系\n- 添加或更新属性值\n\nSET子句语法\nSET  <property-name-list>\n\n|S.No. |\t语法元素 | 描述 |\n|---|---|---|\n|1。\t|组\t|这是一个Neo4j的CQL关键字。|\n|2。|\t<属性名称列表>\t|这是属性执行添加或更新操作满足我们的要求清单。|\n\n\n<属性名称列表>语法：\n<node-label-name>.<property1-name>,\n<node-label-name>.<property2-name>, \n.... \n<node-label-name>.<propertyn-name> \n语法说明\n\n\n|S.No.\t|语法元素|\t描述|\n|---|---|---|\n|1。\t|<节点标签名称>\t|这是一个节点的标签名称。|\n|2。|\t<属性名称>\t|它是一个节点的属性名。|\n\n\n何将一个新的属性添加到现有的借记卡节点。\n\n```\nCREATE (dc:DebitCard{I'd:11,name:'ABC11XYZ',valid_from : 0211,vaild_to:0221})\n```\n\n查找这个节点\n```\nMATCH (dc:DebitCard)\nRETURN dc\n```\n\n\"借记卡”节点都有4个属性。 现在，我们要添加新属性“atm_pin”这个节点。\n\n```\nMATCH (dc:DebitCard)\nSET dc.atm_pin = 3456\nRETURN dc\n```\n\n\n# 排序\n默认情况下，按升序订单行。 如果我们要在他们降序排序，我们需要用DESC。\n\nORDER BY子句语法\nORDER BY  <property-name-list>  [DESC]\t\n|S.No.|\t语法元素|\t描述|\n|1。|\tORDER BY|\t这是一个Neo4j的CQL关键字。|\n|2。|\t<属性名称列表>\t|它是在分拣使用的属性的列表。|\n|3。|\tDESC\t|它是用于指定降序order.It是可选的一个Neo4j的CQL关键字。|\n\n<属性名称列表>语法：\n<node-label-name>.<property1-name>,\n<node-label-name>.<property2-name>, \n.... \n<node-label-name>.<propertyn-name> \n语法说明\n\n|S.No.|\t语法元素|\t描述|\n|1。|\t<节点标签名称>|\t这是一个节点的标签名称。|\n|2。|\t<属性名称>\t|它是一个节点的属性名。|\n\n\n通过员工姓名使用排序结果升序排列。\n\n```\nMATCH (emp:Employee)\nRETURN emp.empid,emp.name,emp.salary,emp.deptno\n```\n观察数据\n```\nMATCH (emp:Employee)\nRETURN emp.empid,emp.name,emp.salary,emp.deptno\nORDER BY emp.name\n```\n排序观察\n\n```\nMATCH (emp:Employee)\nRETURN emp.empid,emp.name,emp.salary,emp.deptno\nORDER BY emp.name DESC\n```\n\n\n# LIMIT and SKIP\n\n它修剪从CQL查询结果集底部的结果。\n\n如果我们要修剪掉CQL查询结果集顶部的结果，那么我们就应该去CQL SKIP子句。\n\nLIMIT子句语法\nLIMIT <number>\n语法说明\n\n|S.No.\t|语法元素\t|描述|\n|---|---|---|\n|1。|\tlimit\t|这是一个Neo4j的CQL关键字。|\n|2。|\tnumber\t|它是一个跨值。|\n\n\n```\nMATCH (emp:Employee) \nRETURN emp\n```\n\n我们仅取两条数据\n\n```\nMATCH (emp:Employee) \nRETURN emp\nLIMIT 2\n```\n\nSKIP子句语法：\nSKIP <number>\n语法说明\n\n|S.No.\t|语法元素|\t描述|\n|---|---|---|\n|1。|skip|\t这是一个Neo4j的CQL关键字。|\n|2。|\tnumber\t|它是一个跨值。|\n\n\n```\nMATCH (emp:Employee) \nRETURN emp\nSKIP 2\n```\n# MERGE\n MERGE 使用命令\n 1. 要创建节点，关系和属性\n 2. 为了从数据库中检索数据\n MERGE 命令是CREATE 命令和MATH命令的组合。\n MERGE = CREATE + MATCH \n MERGE 图中给定模式命令搜，如果存在则返回结果\n 如果他不在图中，则创建新的节点/关系并返回结果。\n \n \n MERGE语法\nMERGE (<node-name>:<label-name>\n{\n   <Property1-name>:<Pro<rty1-Value>\n   .....\n   <Propertyn-name>:<Propertyn-Value>\n})\n\n语法说明\n\n|S.No.|\t语法元素|\t描述|\n|---|---|---|\n|1。\t|MERGE\t|这是一个Neo4j的CQL关键字。|\n|2。|\tProperty1-name\t|它是一个节点或关系的名称。|\n|3。\t|label-name|\t这是一个节点或关系的标签名称。|\n|4。\t|<PROPERTY_NAME>\t|这是一个节点或关系的属性名。|\n|5。\t|<PROPERTY_VALUE>\t|这是一个节点或关系的属性值。|\n|6。\t|：\t|使用冒号（:)操作符来分隔属性名称和一个节点或关系的价值。|\n\nCREATE执行上述所有操作，MATCH和RETURN命令来创建Google+个人资料。\n\n\n```\nCREATE (gp1:GoogleProfile1 {Id: 201401, Name:\"Apple\"})\n```\n先创建一个数据点\n\n```\nCREATE (gp1:GoogleProfile1 {Id: 201401, Name:\"Apple\"})\n```\n再创建一个数据点\n\n\n```\nMATCH  (gp1:GoogleProfile1) \nRETURN gp1.Id,gp1.Name\n```\n\n获取所有个人资料节点详细信息和观察结果。\n\n\n从结果来看，它显示2行有重复的值。\n\nCQL CREATE命令不检查是否该节点可用与否，它只是在数据库中创建新的节点。\n\n通过观察这些结果，我们可以说，CREATE命令总是将新节点添加到数据库。\n\n\n现在我们用MERGE执行同一套以上操作，并返回命令创建Google+个人资料。\n\n```\nMERGE (gp2:GoogleProfile2{ Id: 201402,Name:\"Nokia\"})\n```\n第一次创建\n```\nMERGE (gp2:GoogleProfile2{ Id: 201402,Name:\"Nokia\"})\n```\n再次创建\n\n```\nMATCH  (gp1:GoogleProfile1) \nRETURN gp1.Id,gp1.Name\n```\n获取所有个人资料节点详细信息和观察结果\n\n\n\n# 定向关系\n在 Neo4j中，两个节点之间的关系是有方向的。他们要么是单向要么是双向。\n由于 Neo4j的如下属性图的数据模型，他应该只支持方向的关系。如果我们试图简历一个没有任何方向的关系，那么Neo4j的DB服务器应该会抛出一个错误。\n在本节中，我们将提供一个例子来证明这一点。\n\nCREATE (<node1-details>)-[<relationship-details>]->(<node2-details>)\n\n如果我们观察上面的语法，它用一个箭头标志：（） - []→（）。 它表示从左侧节点到右侧节点的方向。\n\n如果我们试图用同样的语法，而不箭头状（） - [] - （），这意味着没有方向的关系。 然后，Neo4j的DB服务器应该抛出一个错误信息\n\n证明一切Neo4j的关系是有方向的。\n","tags":["数据库"]},{"title":"Beatiful_soup_note","url":"/2019/05/29/Beatiful_soup_note/","content":"\n# Beatifu_Soup 漂亮的汤\n\nBeautiful Soup提供一些简单的、Python式的函数来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。\n\nBeautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为UTF-8编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时你仅仅需要说明一下原始编码方式就可以了。\n\nBeautiful Soup已成为和lxml、html6lib一样出色的Python解释器，为用户灵活地提供不同的解析策略或强劲的速度。\n\n## 解析器\nBeautiful Soup在解析时实际上依赖解析器，它除了支持Python标准库中的HTML解析器外，还支持一些第三方解析器（比如lxml）\n\n|解析器|使用方法|优势|劣势|\n|----|----|-----|-----|\n|Python标准库|BeautifulSoup(markup, \"html.parser\")|Python的内置标准库、</n>执行速度适中、文档容错能力强|Python 2.7.3及</r>Python 3.2.2之前的版本文档容错能力差|\n|lxml HTML解析器|BeautifulSoup(markup, \"lxml\")|速度快、文档容错能力强|需要安装C语言库|\n|lxml XML解析器|BeautifulSoup(markup, \"xml\")|速度快、唯一支持XML的解析器|需要安装C语言库|\n|html5lib|BeautifulSoup(markup, \"html5lib\")|最好的容错性、以浏览器的方式解析文档、生成HTML5格式的文档|速度慢、不依赖外部扩展|\n\n\n```python\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup('<p>Hello</p>', 'lxml')\nprint(soup.p.string)\n```\n\n    Hello\n\n\n```\nHello\n```\n\n\n```python\nhtml = \"\"\"\n<html><head><title>The Dormouse's story</title></head>\n<body>\n<p class=\"title\" name=\"dromouse\"><b>The Dormouse's story</b></p>\n<p class=\"story\">Once upon a time there were three little sisters; and their names were\n<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"><!-- Elsie --></a>,\n<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\nand they lived at the bottom of a well.</p>\n<p class=\"story\">...</p>\n\"\"\"\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint(soup.prettify())\nprint(soup.title.string)\n```\n\n```python\n<html>\n <head>\n  <title>\n   The Dormouse's story\n  </title>\n </head>\n <body>\n  <p class=\"title\" name=\"dromouse\">\n   <b>\n    The Dormouse's story\n   </b>\n  </p>\n  <p class=\"story\">\n   Once upon a time there were three little sisters; and their names were\n   <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n    <!-- Elsie -->\n   </a>\n   ,\n   <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n    Lacie\n   </a>\n   and\n   <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\n    Tillie\n   </a>\n   ;\nand they lived at the bottom of a well.\n  </p>\n  <p class=\"story\">\n   ...\n  </p>\n </body>\n</html>\nThe Dormouse's story\n```\n\n\n这里首先声明变量html，它是一个HTML字符串。但是需要注意的是，它并不是一个完整的HTML字符串，因为body和html节点都没有闭合。接着，我们将它当作第一个参数传给BeautifulSoup对象，该对象的第二个参数为解析器的类型（这里使用lxml），此时就完成了BeaufulSoup对象的初始化。然后，将这个对象赋值给soup变量。\n\n接下来，就可以调用soup的各个方法和属性解析这串HTML代码了。\n\n首先，调用prettify()方法。这个方法可以把要解析的字符串以标准的缩进格式输出。这里需要注意的是，输出结果里面包含body和html节点，也就是说对于不标准的HTML字符串BeautifulSoup，可以自动更正格式。这一步不是由prettify()方法做的，而是在初始化BeautifulSoup时就完成了。\n\n然后调用soup.title.string，这实际上是输出HTML中title节点的文本内容。所以，soup.title可以选出HTML中的title节点，再调用string属性就可以得到里面的文本了，所以我们可以通过简单调用几个属性完成文本提取，这是不是非常方便？\n\n## 节点选择器\n直接调用节点的名称就可以选择节点元素，再调用string属性就可以得到节点内的文本了，这种选择方式速度非常快。如果单个节点结构层次非常清晰，可以选用这种方式来解析。\n\n### 选择元素\n\n打印输出title节点的选择结果，输出结果正是title节点加里面的文字内容\n\n\n```python\nsoup.title\n```\n\n\n\n\n```python\n<title>The Dormouse's story</title>\n```\n\n\n\n它的类型，是bs4.element.Tag类型，这是Beautiful Soup中一个重要的数据结构。经过选择器选择后，选择结果都是这种Tag类型。Tag具有一些属性，比如string属性，调用该属性，可以得到节点的文本内容，所以接下来的输出结果正是节点的文本内容。\n\n\n```python\ntype(soup.title)\n```\n\n\n\n\n```python\nbs4.element.Tag\n```\n\n\n\n\n```python\nsoup.title.string\n```\n\n\n\n\n```python\n\"The Dormouse's story\"\n```\n\n\n\n选择了head节点，结果也是节点加其内部的所有内容。\n\n\n```python\nsoup.head\n```\n\n\n\n\n```python\n<head><title>The Dormouse's story</title></head>\n```\n\n\n\n### 提取信息\n\n### 获取名称\n\n可以利用name属性获取节点的名称。这里还是以上面的文本为例，选取title节点，然后调用name属性就可以得到节点名称：\n\n\n```python\nsoup.title.name\n```\n\n\n\n\n```python\n'title'\n```\n\n\n\n### 获取属性\n每个节点可能有多个属性，比如id和class等，选择这个节点元素后，可以调用attrs获取所有属性：\n\n\n```python\nsoup.p.attrs\n```\n\n\n\n\n```python\n{'class': ['title'], 'name': 'dromouse'}\n```\n\n\n\n可以看到，attrs的返回结果是字典形式，它把选择的节点的所有属性和属性值组合成一个字典。接下来，如果要获取name属性，就相当于从字典中获取某个键值，只需要用中括号加属性名就可以了。比如，要获取name属性，就可以通过attrs['name']来得到。\n\n\n```python\nsoup.p.attrs['name']\n```\n\n\n\n\n```python\n'dromouse'\n```\n\n\n\n其实这样有点烦琐，还有一种更简单的获取方式：可以不用写attrs，直接在节点元素后面加中括号，传入属性名就可以获取属性值了。样例如下：\n\n\n```python\nprint(soup.p['name'])\nprint(soup.p['class'])\n```\n\n```python\ndromouse\n['title']\n```\n\n\n这里需要注意的是，有的返回结果是字符串，有的返回结果是字符串组成的列表。比如，name属性的值是唯一的，返回的结果就是单个字符串。而对于class，一个节点元素可能有多个class，所以返回的是列表。在实际处理过程中，我们要注意判断类型。\n\n### 获取内容\n\n可以利用string属性获取节点元素包含的文本内容，比如要获取第一个p节点的文本：\n\n\n```python\nsoup.p.string\n```\n\n\n\n\n```python\n\"The Dormouse's story\"\n```\n\n\n\n在上面的例子中，我们知道每一个返回结果都是bs4.element.Tag类型，它同样可以继续调用节点进行下一步的选择。比如，我们获取了head节点元素，我们可以继续调用head来选取其内部的head节点元素：\n\n\n```python\nhtml = \"\"\"\n<html><head><title>The Dormouse's story</title></head>\n<body>\n\"\"\"\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint(soup.head.title)\nprint(type(soup.head.title))\nprint(soup.head.title.string)\n```\n\n```python\n<title>The Dormouse's story</title>\n<class 'bs4.element.Tag'>\nThe Dormouse's story\n```\n\n\n第一行结果是调用head之后再次调用title而选择的title节点元素。然后打印输出了它的类型，可以看到，它仍然是bs4.element.Tag类型。也就是说，我们在Tag类型的基础上再次选择得到的依然还是Tag类型，每次返回的结果都相同，所以这样就可以做嵌套选择了。\n\n最后，输出它的string属性，也就是节点里的文本内容。\n\n### 关联选择\n在做选择的时候，有时候不能做到一步就选到想要的节点元素，需要先选中某一个节点元素，然后以它为基准再选择它的子节点、父节点、兄弟节点等，这里就来介绍如何选择这些节点元素。\n\n#### 子节点和子孙节点\n\n选取节点元素之后，如果想要获取它的直接子节点，可以调用contents属性，示例如下：\n\n\n```python\nfrom bs4 import BeautifulSoup\nhtml = \"\"\"\n<html>\n    <head>\n        <title>The Dormouse's story</title>\n    </head>\n    <body>\n        <p class=\"story\">\n            Once upon a time there were three little sisters; and their names were\n            <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">\n                <span>Elsie</span>\n            </a>\n            <a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> \n            and\n            <a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>\n            and they lived at the bottom of a well.\n        </p>\n        <p class=\"story\">...</p>\n\"\"\"\nsoup = BeautifulSoup(html, 'lxml')\nprint(soup.p.children)\nfor i, child in enumerate(soup.p.children):\n    print(i, child)\n```\n\n```python\n<list_iterator object at 0x0000019337C7F3C8>\n0 \n            Once upon a time there were three little sisters; and their names were\n            \n1 <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n<span>Elsie</span>\n</a>\n2 \n\n3 <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\n4  \n            and\n            \n5 <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n6 \n            and they lived at the bottom of a well.\n```\n\n\n​    \n\n还是同样的HTML文本，这里调用了children属性来选择，返回结果是生成器类型。接下来，我们用for循环输出相应的内容。\n\n如果要得到所有的子孙节点的话，可以调用descendants属性：\n\n\n```python\nfor i, child in enumerate(soup.p.descendants):\n    print(i, child)\n```\n\n```python\n0 \n            Once upon a time there were three little sisters; and their names were\n            \n1 <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n<span>Elsie</span>\n</a>\n2 \n\n3 <span>Elsie</span>\n4 Elsie\n5 \n\n6 \n\n7 <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\n8 Lacie\n9  \n            and\n            \n10 <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n11 Tillie\n12 \n            and they lived at the bottom of a well.\n```\n\n\n​    \n\n此时返回结果还是生成器。遍历输出一下可以看到，这次的输出结果就包含了span节点。descendants会递归查询所有子节点，得到所有的子孙节点。\n\n### 父节点和祖先节点\n\n\n\n```python\nhtml = \"\"\"\n<html>\n    <head>\n        <title>The Dormouse's story</title>\n    </head>\n    <body>\n        <p class=\"story\">\n            Once upon a time there were three little sisters; and their names were\n            <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">\n                <span>Elsie</span>\n            </a>\n        </p>\n        <p class=\"story\">...</p>\n\"\"\"\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint(soup.a.parent)\n```\n\n```python\n<p class=\"story\">\n            Once upon a time there were three little sisters; and their names were\n            <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n<span>Elsie</span>\n</a>\n</p>\n```\n\n\n这里我们选择的是第一个a节点的父节点元素。很明显，它的父节点是p节点，输出结果便是p节点及其内部的内容。\n\n需要注意的是，这里输出的仅仅是a节点的直接父节点，而没有再向外寻找父节点的祖先节点。如果想获取所有的祖先节点，可以调用parents属性：\n\n\n```python\nhtml = \"\"\"\n<html>\n    <body>\n        <p class=\"story\">\n            <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">\n                <span>Elsie</span>\n            </a>\n        </p>\n\"\"\"\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint(type(soup.a.parents))\nprint(list(enumerate(soup.a.parents)))\n```\n\n```python\n<class 'generator'>\n[(0, <p class=\"story\">\n<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n<span>Elsie</span>\n</a>\n</p>), (1, <body>\n<p class=\"story\">\n<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n<span>Elsie</span>\n</a>\n</p>\n</body>), (2, <html>\n<body>\n<p class=\"story\">\n<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n<span>Elsie</span>\n</a>\n</p>\n</body></html>), (3, <html>\n<body>\n<p class=\"story\">\n<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n<span>Elsie</span>\n</a>\n</p>\n</body></html>)]\n```\n\n\n可以发现，返回结果是生成器类型。这里用列表输出了它的索引和内容，而列表中的元素就是a节点的祖先节点。\n\n### 兄弟节点\n\n\n```python\nhtml = \"\"\"\n<html>\n    <body>\n        <p class=\"story\">\n            Once upon a time there were three little sisters; and their names were\n            <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">\n                <span>Elsie</span>\n            </a>\n            Hello\n            <a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> \n            and\n            <a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>\n            and they lived at the bottom of a well.\n        </p>\n\"\"\"\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint('Next Sibling', soup.a.next_sibling)\nprint('Prev Sibling', soup.a.previous_sibling)\nprint('Next Siblings', list(enumerate(soup.a.next_siblings)))\nprint('Prev Siblings', list(enumerate(soup.a.previous_siblings)))\n```\n\n```python\nNext Sibling \n            Hello\n            \nPrev Sibling \n            Once upon a time there were three little sisters; and their names were\n            \nNext Siblings [(0, '\\n            Hello\\n            '), (1, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>), (2, ' \\n            and\\n            '), (3, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>), (4, '\\n            and they lived at the bottom of a well.\\n        ')]\nPrev Siblings [(0, '\\n            Once upon a time there were three little sisters; and their names were\\n            ')]\n```\n\n\n### 提取信息\n前面讲解了关联元素节点的选择方法，如果想要获取它们的一些信息，比如文本、属性等，也用同样的方法，示例如下：\n\n\n```python\nhtml = \"\"\"\n<html>\n    <body>\n        <p class=\"story\">\n            Once upon a time there were three little sisters; and their names were\n            <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Bob</a><a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> \n        </p>\n\"\"\"\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint('Next Sibling:')\nprint(type(soup.a.next_sibling))\nprint(soup.a.next_sibling)\nprint(soup.a.next_sibling.string)\nprint('Parent:')\nprint(type(soup.a.parents))\nprint(list(soup.a.parents)[0])\nprint(list(soup.a.parents)[0].attrs['class'])\n```\n\n```python\nNext Sibling:\n<class 'bs4.element.Tag'>\n<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\nLacie\nParent:\n<class 'generator'>\n<p class=\"story\">\n            Once upon a time there were three little sisters; and their names were\n            <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Bob</a><a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\n</p>\n['story']\n```\n\n\n如果返回结果是单个节点，那么可以直接调用string、attrs等属性获得其文本和属性；如果返回结果是多个节点的生成器，则可以转为列表后取出某个元素，然后再调用string、attrs等属性获取其对应节点的文本和属性。\n\n## 方法选择器\n\n前面所讲的选择方法都是通过属性来选择的，这种方法非常快，但是如果进行比较复杂的选择的话，它就比较烦琐，不够灵活了。幸好，Beautiful Soup还为我们提供了一些查询方法，比如find_all()和find()等，调用它们，然后传入相应的参数，就可以灵活查询了。\n\n`find_all()` \n\n`find_all`，顾名思义，就是查询所有符合条件的元素。给它传入一些属性或文本，就可以得到符合条件的元素，它的功能十分强大。\n\n它的API如下：\n```python\nfind_all(name , attrs , recursive , text , **kwargs)\n```\n(1) name\n我们可以根据节点名来查询元素，示例如下：\n\n\n```python\nhtml='''\n<div class=\"panel\">\n    <div class=\"panel-heading\">\n        <h4>Hello</h4>\n    </div>\n    <div class=\"panel-body\">\n        <ul class=\"list\" id=\"list-1\">\n            <li class=\"element\">Foo</li>\n            <li class=\"element\">Bar</li>\n            <li class=\"element\">Jay</li>\n        </ul>\n        <ul class=\"list list-small\" id=\"list-2\">\n            <li class=\"element\">Foo</li>\n            <li class=\"element\">Bar</li>\n        </ul>\n    </div>\n</div>\n'''\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint(soup.find_all(name='ul'))\nprint(type(soup.find_all(name='ul')[0]))\n```\n\n```python\n[<ul class=\"list\" id=\"list-1\">\n<li class=\"element\">Foo</li>\n<li class=\"element\">Bar</li>\n<li class=\"element\">Jay</li>\n</ul>, <ul class=\"list list-small\" id=\"list-2\">\n<li class=\"element\">Foo</li>\n<li class=\"element\">Bar</li>\n</ul>]\n<class 'bs4.element.Tag'>\n```\n\n\n这里我们调用了`find_all()`方法，传入`name`参数，其参数值为`ul`。也就是说，我们想要查询所有`ul`节点，返回结果是列表类型，长度为2，每个元素依然都是`bs4.element.Tag`类型。\n\n因为都是`Tag`类型，所以依然可以进行嵌套查询。还是同样的文本，这里查询出所有`ul`节点后，再继续查询其内部的`li`节点：\n\n\n```python\nfor ul in soup.find_all(name='ul'):\n    print(ul.find_all(name='li'))\n    for li in ul.find_all(name='li'):\n        print(li.string)\n```\n\n```python\n[<li class=\"element\">Foo</li>, <li class=\"element\">Bar</li>, <li class=\"element\">Jay</li>]\nFoo\nBar\nJay\n[<li class=\"element\">Foo</li>, <li class=\"element\">Bar</li>]\nFoo\nBar\n```\n\n(2)attrs\n除了根据节点名查询，我们也可以传入一些属性来查询，示例如下：\n\n\n```python\nhtml='''\n<div class=\"panel\">\n    <div class=\"panel-heading\">\n        <h4>Hello</h4>\n    </div>\n    <div class=\"panel-body\">\n        <ul class=\"list\" id=\"list-1\" name=\"elements\">\n            <li class=\"element\">Foo</li>\n            <li class=\"element\">Bar</li>\n            <li class=\"element\">Jay</li>\n        </ul>\n        <ul class=\"list list-small\" id=\"list-2\">\n            <li class=\"element\">Foo</li>\n            <li class=\"element\">Bar</li>\n        </ul>\n    </div>\n</div>\n'''\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint(soup.find_all(attrs={'id': 'list-1'}))\nprint(soup.find_all(attrs={'name': 'elements'}))\n```\n\n```python\n[<ul class=\"list\" id=\"list-1\" name=\"elements\">\n<li class=\"element\">Foo</li>\n<li class=\"element\">Bar</li>\n<li class=\"element\">Jay</li>\n</ul>]\n[<ul class=\"list\" id=\"list-1\" name=\"elements\">\n<li class=\"element\">Foo</li>\n<li class=\"element\">Bar</li>\n<li class=\"element\">Jay</li>\n</ul>]\n```\n\n\n这里查询的时候传入的是attrs参数，参数的类型是字典类型。比如，要查询id为list-1的节点，可以传入attrs={'id': 'list-1'}的查询条件，得到的结果是列表形式，包含的内容就是符合id为list-1的所有节点。在上面的例子中，符合条件的元素个数是1，所以结果是长度为1的列表。\n\n对于一些常用的属性，比如id和class等，我们可以不用attrs来传递。比如，要查询id为list-1的节点，可以直接传入id这个参数。还是上面的文本，我们换一种方式来查询：\n\n\n```python\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint(soup.find_all(id='list-1'))\nprint(soup.find_all(class_='element'))\n```\n\n```python\n[<ul class=\"list\" id=\"list-1\" name=\"elements\">\n<li class=\"element\">Foo</li>\n<li class=\"element\">Bar</li>\n<li class=\"element\">Jay</li>\n</ul>]\n[<li class=\"element\">Foo</li>, <li class=\"element\">Bar</li>, <li class=\"element\">Jay</li>, <li class=\"element\">Foo</li>, <li class=\"element\">Bar</li>]\n```\n\n\n这里直接传入id='list-1'，就可以查询id为list-1的节点元素了。而对于class来说，由于class在Python里是一个关键字，所以后面需要加一个下划线，即class_='element'，返回的结果依然还是Tag组成的列表。\n\n(3) text\ntext参数可用来匹配节点的文本，传入的形式可以是字符串，可以是正则表达式对象，示例如下：\n\n\n```python\nimport re\nhtml='''\n<div class=\"panel\">\n    <div class=\"panel-body\">\n        <a>Hello, this is a link</a>\n        <a>Hello, this is a link, too</a>\n    </div>\n</div>\n'''\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint(soup.find_all(text=re.compile('link')))\n```\n\n```python\n['Hello, this is a link', 'Hello, this is a link, too']\n```\n\n\n这里有两个a节点，其内部包含文本信息。这里在find_all()方法中传入text参数，该参数为正则表达式对象，结果返回所有匹配正则表达式的节点文本组成的列表。\n\nfind()\n\n除了find_all()方法，还有find()方法，只不过后者返回的是单个元素，也就是第一个匹配的元素，而前者返回的是所有匹配的元素组成的列表。示例如下：\n\n\n```python\nhtml='''\n<div class=\"panel\">\n    <div class=\"panel-heading\">\n        <h4>Hello</h4>\n    </div>\n    <div class=\"panel-body\">\n        <ul class=\"list\" id=\"list-1\">\n            <li class=\"element\">Foo</li>\n            <li class=\"element\">Bar</li>\n            <li class=\"element\">Jay</li>\n        </ul>\n        <ul class=\"list list-small\" id=\"list-2\">\n            <li class=\"element\">Foo</li>\n            <li class=\"element\">Bar</li>\n        </ul>\n    </div>\n</div>\n'''\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint(soup.find(name='ul'))\n\n```\n\n```python\n<ul class=\"list\" id=\"list-1\">\n<li class=\"element\">Foo</li>\n<li class=\"element\">Bar</li>\n<li class=\"element\">Jay</li>\n</ul>\n<class 'bs4.element.Tag'>\n<ul class=\"list\" id=\"list-1\">\n<li class=\"element\">Foo</li>\n<li class=\"element\">Bar</li>\n<li class=\"element\">Jay</li>\n</ul>\n```\n\n\n\n```python\nprint(type(soup.find(name='ul')))\n```\n\n```python\n<class 'bs4.element.Tag'>\n```\n\n\n\n```python\nprint(soup.find(class_='list'))\n```\n\n```python\n<ul class=\"list\" id=\"list-1\">\n<li class=\"element\">Foo</li>\n<li class=\"element\">Bar</li>\n<li class=\"element\">Jay</li>\n</ul>\n```\n\n\n这里的返回结果不再是列表形式，而是第一个匹配的节点元素，类型依然是Tag类型。\n\n另外，还有许多查询方法，其用法与前面介绍的find_all()、find()方法完全相同，只不过查询范围不同，这里简单说明一下。\n\n`find_parents()`和`find_parent()`：前者返回所有祖先节点，后者返回直接父节点。\n`find_next_siblings()`和`find_next_sibling()`：前者返回后面所有的兄弟节点，后者返回后面第一个兄弟节点。\n`find_previous_siblings()`和`find_previous_sibling()`：前者返回前面所有的兄弟节点，后者返回前面第一个兄弟节点。\n`find_all_next()`和`find_next()`：前者返回节点后所有符合条件的节点，后者返回第一个符合条件的节点。\n`find_all_previous()`和`find_previous()`：前者返回节点后所有符合条件的节点，后者返回第一个符合条件的节点。\n\n## CSS选择器\n\nBeautiful Soup还提供了另外一种选择器，那就是CSS选择器。如果对Web开发熟悉的话，那么对CSS选择器肯定也不陌生。如果不熟悉的话，可以参考http://www.w3school.com.cn/cssref/css_selectors.asp了解。\n\n使用CSS选择器时，只需要调用select()方法，传入相应的CSS选择器即可，示例如下：\n\n\n```python\nhtml='''\n<div class=\"panel\">\n    <div class=\"panel-heading\">\n        <h4>Hello</h4>\n    </div>\n    <div class=\"panel-body\">\n        <ul class=\"list\" id=\"list-1\">\n            <li class=\"element\">Foo</li>\n            <li class=\"element\">Bar</li>\n            <li class=\"element\">Jay</li>\n        </ul>\n        <ul class=\"list list-small\" id=\"list-2\">\n            <li class=\"element\">Foo</li>\n            <li class=\"element\">Bar</li>\n        </ul>\n    </div>\n</div>\n'''\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nprint(soup.select('.panel .panel-heading'))\nprint(soup.select('#list-2 .element'))\n```\n\n```python\n[<div class=\"panel-heading\">\n<h4>Hello</h4>\n</div>]\n[<li class=\"element\">Foo</li>, <li class=\"element\">Bar</li>]\n<class 'bs4.element.Tag'>\n```\n\n\nselect('ul li')则是选择所有ul节点下面的所有li节点，结果便是所有的li节点组成的列表。\n\n\n```python\nprint(soup.select('ul li'))\n```\n\n```python\n[<li class=\"element\">Foo</li>, <li class=\"element\">Bar</li>, <li class=\"element\">Jay</li>, <li class=\"element\">Foo</li>, <li class=\"element\">Bar</li>]\n```\n\n\n最后一句打印输出了列表中元素的类型。可以看到，类型依然是Tag类型\n\n\n```python\nprint(type(soup.select('ul')[0]))\n```\n\n```python\n<class 'bs4.element.Tag'>\n```\n\n\n### 获取属性\n\n我们知道节点类型是Tag类型，所以获取属性还可以用原来的方法。仍然是上面的HTML文本，这里尝试获取每个ul节点的id属性：\n\n\n```python\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nfor ul in soup.select('ul'):\n    print(ul['id'])\n    print(ul.attrs['id'])\n```\n\n```python\nlist-1\nlist-1\nlist-2\nlist-2\n```\n\n\n可以看到，直接传入中括号和属性名，以及通过attrs属性获取属性值，都可以成功。\n\n### 获取文本\n要获取文本，当然也可以用前面所讲的string属性。此外，还有一个方法，那就是get_text()，示例如下：\n\n\n```python\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nfor li in soup.select('li'):\n    print('Get Text:', li.get_text())\n    print('String:', li.string)\n```\n\n```python\nGet Text: Foo\nString: Foo\nGet Text: Bar\nString: Bar\nGet Text: Jay\nString: Jay\nGet Text: Foo\nString: Foo\nGet Text: Bar\nString: Bar\n```\n\n\n可以看到，二者的效果完全一致。\n\n到此，Beautiful Soup的用法基本就介绍完了，最后做一下简单的总结。\n\n推荐使用lxml解析库，必要时使用html.parser。\n节点选择筛选功能弱但是速度快。\n建议使用`find()`或者`find_all()`查询匹配单个结果或者多个结果。\n如果对CSS选择器熟悉的话，可以使用`select()`方法选择。 \n","tags":["爬虫"]},{"title":"xpath_note","url":"/2019/05/23/xpath-note/","content":"\n# Xpath 学习笔记\n\n\n## XPath概览\nXPath，全称XML Path Language，即XML路径语言，它是一门在XML文档中查找信息的语言。它最初是用来搜寻XML文档的，但是它同样适用于HTML文档的搜索。\n\n所以在做爬虫时，我们完全可以使用XPath来做相应的信息抽取。本节中，我们就来介绍XPath的基本用法。 \n\nXPath的选择功能十分强大，它提供了非常简洁明了的路径选择表达式。另外，它还提供了超过100个内建函数，用于字符串、数值、时间的匹配以及节点、序列的处理等。几乎所有我们想要定位的节点，都可以用XPath来选择。\n\n它被设计为供XSLT、XPointer以及其他XML解析软件使用，更多的文档可以访问其官方网站：https://www.w3.org/TR/xpath/。\n\n## XPath常用规则\n|表达式|描述|\n|:-----:|:-----:|\n|nodename|选取此节点的所有子节点|\n|/|从当前节点选取直接子节点|\n|//|从当前节点选取子孙节点|\n|.|选取当前节点|\n|..|选取当前节点的父节点|\n|@|选取属性|\n\n\n\n\n\n\nXPath的常用匹配规则，示例如下：\n```python\nstr_xpath=\"//title[@class='python']\"\nhtml.xpath()\n```\n\n这就是一个XPath规则，它代表选择所有名称为title，同时属性class的值为python的节点。\n后面会通过Python的lxml库，利用XPath进行HTML的解析。\n\n## 安装\n```\npip install  lxml\n```\n使用以上命令，安装即可。\n\n## 示例讲解\n\n\n```python\nfrom lxml import etree\n```\n\n对基本包导入，我们需要用到etree这个包来解析网页。\n\n\n```python\ntext = '''\n<div>\n    <ul>\n         <li class=\"item-0\"><a href=\"link1.html\">first item</a></li>\n         <li class=\"item-1\"><a href=\"link2.html\">second item</a></li>\n         <li class=\"item-inactive\"><a href=\"link3.html\">third item</a></li>\n         <li class=\"item-1\"><a href=\"link4.html\">fourth item</a></li>\n         <li class=\"item-0\"><a href=\"link5.html\">fifth item</a>\n     </ul>\n </div>\n'''\n```\n\n\n我们自己声明一个变量，该变量是字符创，html格式的文本，\n\n\n```python\nhtml = etree.HTML(text)\n```\n\n调用HTML类进行初始化，这样就成功构造了一个XPath解析对象。这里需要注意的是，HTML文本中的最后一个li节点是没有闭合的，但是etree模块可以自动修正HTML文本。\n\n\n```python\nprint(etree.tostring(html).decode('utf-8'))\n```\n\n```\n    <html><body><div>\n        <ul>\n             <li class=\"item-0\"><a href=\"link1.html\">first item</a></li>\n             <li class=\"item-1\"><a href=\"link2.html\">second item</a></li>\n             <li class=\"item-inactive\"><a href=\"link3.html\">third item</a></li>\n             <li class=\"item-1\"><a href=\"link4.html\">fourth item</a></li>\n             <li class=\"item-0\"><a href=\"link5.html\">fifth item</a>\n         </li></ul>\n     </div>\n    </body></html>\n    \n```\n\n调用tostring()方法即可输出修正后的HTML代码，但是结果是bytes类型。这里利用decode()方法将其转成str类型。\n我们可以看到，经过处理之后，li节点标签被补全，并且还自动添加了body、html节点。\n\n### 选取所有节点\n一般会用//开头的XPath规则来选取所有符合要求的节点。这里以前面的HTML文本为例，如果要选取所有节点，可以这样实现：\n\n\n```python\nhtml.xpath('//*')\n```\n```\n[<Element html at 0x199551eb408>,\n<Element body at 0x19955205108>,\n<Element div at 0x19955243288>,\n     <Element ul at 0x19955243508>,\n     <Element li at 0x19955243388>,\n     <Element a at 0x199552433c8>,\n     <Element li at 0x19955243408>,\n     <Element a at 0x199552434c8>,\n     <Element li at 0x19955243608>,\n     <Element a at 0x19955243488>,\n     <Element li at 0x19955243648>,\n     <Element a at 0x19955243688>,\n     <Element li at 0x199552436c8>,\n     <Element a at 0x19955243708>]\n```\n\n这里使用*代表匹配所有节点，也就是整个HTML文本中的所有节点都会被获取。\n可以看到，返回形式是一个列表，每个元素是Element类型，其后跟了节点的名称，如html、body、div、ul、li、a等，所有节点都包含在列表中了。\n\n\n### 选取某特定节点\n\n\n```python\nhtml.xpath('//li')\n```\n\n```\n\n    [<Element li at 0x19955243388>,\n     <Element li at 0x19955243408>,\n     <Element li at 0x19955243608>,\n     <Element li at 0x19955243648>,\n     <Element li at 0x199552436c8>]\n\n```\n\n如上所示，此处匹配也可以指定节点名称。获取所有li节点，可以用//，然后加上节点名称既可以，调用时依然需要使用xpath()方法。\n返回的结果是个列表，里面的元素是可以操作的`Element`对象。\n\n### 选取节点下的子节点\n\n\n\n```python\nhtml.xpath('//li/a')\n```\n\n\n\n```\n    [<Element a at 0x199552433c8>,\n     <Element a at 0x199552434c8>,\n     <Element a at 0x19955243488>,\n     <Element a at 0x19955243688>,\n     <Element a at 0x19955243708>]\n\n```\n\n我们通过`/`或`//`即可查找元素的子节点或子孙节点。如上我们选择li节点的所有直接a子节点。\n以上的`/`用于选取直接的子节点，如果想要获取所有的子孙节点，可以使用`//`.\n例如，我们获取'ul'节点下的所有子孙节点`a`,代码如下：\n\n\n```python\nhtml.xpath('//ul//a')\n```\n\n\n\n```\n    [<Element a at 0x199552433c8>,\n     <Element a at 0x199552434c8>,\n     <Element a at 0x19955243488>,\n     <Element a at 0x19955243688>,\n     <Element a at 0x19955243708>]\n```\n\n\n### 父节点\n我们知道通过连续的/或//可以查找子节点或子孙节点，那么假如我们知道了子节点，怎样来查找父节点呢？这可以用..来实现。\n\n比如，现在首先选中href属性为link4.html的a节点，然后再获取其父节点，然后再获取其class属性，相关代码如下：\n\n\n```python\nhtml.xpath('//a[@href=\"link4.html\"]/../@class')\n```\n\n```\n    ['item-1']\n```\n\n检查一下结果发现，这正是我们获取的目标li节点的class。  \n同时，我们也可以通过parent::来获取父节点，代码如下：\n\n\n```python\nhtml.xpath('//a[@href=\"link4.html\"]/parent::*/@class')\n```\n\n```\n    ['item-1']\n\n```\n\n### 属性匹配\n\n在选取的时候，我们还可以用@符号进行属性过滤。\n比如，这里如果要选取class为item-1的li节点，可以这样实现:\n\n\n```python\n html.xpath('//li[@class=\"item-0\"]')\n```\n\n\n\n```\n    [<Element li at 0x19955243388>, <Element li at 0x199552436c8>]\n```\n\n\n这里我们通过加入[@class=\"item-0\"]，限制了节点的class属性为item-0，而HTML文本中符合条件的li节点有两个，所以结果应该返回两个匹配到的元素。\n\n### 文本获取\n我们用XPath中的text()方法获取节点中的文本，接下来尝试获取前面li节点中的文本\n\n\n```python\nhtml.xpath('//li[@class=\"item-0\"]/text()')\n```\n\n\n```\n\n    ['\\n     ']\n```\n\n\n奇怪的是，我们并没有获取到任何文本，只获取到了一个换行符，这是为什么呢？因为XPath中`text()`前面是`/`， \n而此处/的含义是选取直接子节点，很明显li的直接子节点都是a节点，文本都是在a节点内部的，  \n所以这里匹配到的结果就是被修正的li节点内部的换行符，因为自动修正的li节点的尾标签换行了。\n即选中的是这两个节点：\n```\n<li class=\"item-0\"><a href=\"link1.html\">first item</a></li>\n<li class=\"item-0\"><a href=\"link5.html\">fifth item</a>\n</li>\n```\n其中一个节点因为自动修正，li节点的尾标签添加的时候换行了，所以提取文本得到的唯一结果就是li节点的尾标签和a节点的尾标签之间的换行符。\n\n因此，如果想获取`li`节点内部的文本，就有两种方式，一种是先选取a节点再获取文本，另一种就是使用`//`。接下来，我们来看下二者的区别。\n\n首先，选取到a节点再获取文本，代码如下：\n\n\n\n```python\n html.xpath('//li[@class=\"item-0\"]/a/text()')\n```\n\n\n\n````\n    ['first item', 'fifth item']\n````\n\n\n可以看到，这里的返回值是两个，内容都是属性为item-0的li节点的文本，这也印证了前面属性匹配的结果是正确的。\n\n这里我们是逐层选取的，先选取了li节点，又利用/选取了其直接子节点a，然后再选取其文本，得到的结果恰好是符合我们预期的两个结果。\n再来看下用另一种方式（即使用//）选取的结果，代码如下：\n\n\n```python\nhtml.xpath('//li[@class=\"item-0\"]//text()')\n```\n\n\n````\n\n    ['first item', 'fifth item', '\\n     ']\n````\n\n\n不出所料，这里的返回结果是3个。可想而知，这里是选取所有子孙节点的文本，其中前两个就是li的子节点a节点内部的文本，另外一个就是最后一个li节点内部的文本，即换行符。\n\n所以说，如果要想获取子孙节点内部的所有文本，可以直接用//加text()的方式，这样可以保证获取到最全面的文本信息，但是可能会夹杂一些换行符等特殊字符。如果想获取某些特定子孙节点下的所有文本，可以先选取到特定的子孙节点，然后再调用text()方法获取其内部文本，这样可以保证获取的结果是整洁的。\n\n### 属性获取\n我们知道用text()可以获取节点内部文本，那么节点属性该怎样获取呢？其实还是用@符号就可以。例如，我们想获取所有li节点下所有a节点的href属性，代码如下：\n\n\n```python\nhtml.xpath('//li/a/@href')\n```\n\n\n\n````\n    ['link1.html', 'link2.html', 'link3.html', 'link4.html', 'link5.html']\n````\n\n\n我们通过@href即可获取节点的href属性。注意，此处和属性匹配的方法不同，属性匹配是中括号加属性名和值来限定某个属性，如[@href=\"link1.html\"]，而此处的@href指的是获取节点的某个属性，二者需要做好区分。\n\n#### 属性多值匹配\n有时候，某些节点的某个属性可能有多个值，例如：\n\n\n```python\ntext = '''\n<li class=\"li li-first\"><a href=\"link.html\">first item</a></li>\n'''\nhtml = etree.HTML(text)\n\n```\n\n这里HTML文本中li节点的class属性有两个值li和li-first，此时如果还想用之前的属性匹配获取，就无法匹配了，此时的运行结果如下：\n\n\n\n```python\nhtml.xpath('//li[@class=\"li\"]/a/text()')\n```\n\n\n\n```\n    []\n```\n\n\n这时就需要用contains()函数了，代码可以改写如下：\n\n\n```python\nhtml.xpath('//li[contains(@class, \"li\")]/a/text()')\n```\n\n\n\n```\n    ['first item']\n```\n\n\n此种方式在某个节点的某个属性有多个值时经常用到，如某个节点的class属性通常有多个。\n\n#### 多属性匹配\n，我们可能还遇到一种情况，那就是根据多个属性确定一个节点，这时就需要同时匹配多个属性。此时可以使用运算符and来连接，示例如下：\n\n\n```python\n\ntext = '''\n<li class=\"li li-first\" name=\"item\"><a href=\"link.html\">first item</a></li>\n'''\nhtml = etree.HTML(text)\n```\n\n这里的li节点又增加了一个属性name。要确定这个节点，需要同时根据class和name属性来选择，一个条件是class属性里面包含li字符串，另一个条件是name属性为item字符串，二者需要同时满足，需要用and操作符相连，相连之后置于中括号内进行条件筛选。运行结果如下：\n\n\n```python\n html.xpath('//li[contains(@class, \"li\") and @name=\"item\"]/a/text()')\n```\n\n\n\n```\n    ['first item']\n\n```\n\n运算符及其介绍\n\n|运算符|描述|实例|返回值|\n|:-----:|:-----:|:----:|\n|or|或|age=19 or age=20|如果age是19，则返回true。如果age是21，则返回false|\n|and|与|age>19 and age<21|如果age是20，则返回true。如果age是18，则返回false|\n|mod|计算除法的余数|5 mod 2|1|\n|+|加法|6 + 4|10|\n|-|减法|6 - 4|2|\n|* |乘法|6 * 4|24|\n|div|除法|8 div 4|2|\n|=|等于|age=19|如果age是19，则返回true。如果age是20，则返回false|\n|!=|不等于|age!=19|如果age是18，则返回true。如果age是19，则返回false|\n|<|小于|age<19|如果age是18，则返回true。如果age是19，则返回false|\n|<=|小于或等于|age<=19|如果age是19，则返回true。如果age是20，则返回false|\n|>|大于|age>19|如果age是20，则返回true。如果age是19，则返回false|\n|>=|大于或等于|age>=19|如果age是19，则返回true。如果age是18，则返回false|\n\n### 按序选择\n\n有时候，我们在选择的时候某些属性可能同时匹配了多个节点，但是只想要其中的某个节点，如第二个节点或者最后一个节点，这时该怎么办呢？\n这时可以利用中括号传入索引的方法获取特定次序的节点，示例如下：\n\n\n```python\ntext = '''\n<div>\n    <ul>\n         <li class=\"item-0\"><a href=\"link1.html\">first item</a></li>\n         <li class=\"item-1\"><a href=\"link2.html\">second item</a></li>\n         <li class=\"item-inactive\"><a href=\"link3.html\">third item</a></li>\n         <li class=\"item-1\"><a href=\"link4.html\">fourth item</a></li>\n         <li class=\"item-0\"><a href=\"link5.html\">fifth item</a>\n     </ul>\n </div>\n'''\nhtml = etree.HTML(text)\n```\n\n我们选取了第一个li节点，中括号中传入数字1即可。注意，这里和代码中不同，序号是以1开头的，不是以0开头。\n\n\n```python\nhtml.xpath('//li[1]/a/text()')\n```\n\n\n\n```\n    ['first item']\n```\n\n\n我们选取了最后一个li节点，中括号中传入last()即可，返回的便是最后一个li节点。\n\n\n```python\n html.xpath('//li[last()]/a/text()')\n```\n\n\n\n```\n    ['fifth item']\n```\n\n\n我们选取了位置小于3的li节点，也就是位置序号为1和2的节点，得到的结果就是前两个li节点。\n\n\n```python\nhtml.xpath('//li[position()<3]/a/text()')\n```\n\n\n\n```\n    ['first item', 'second item']\n```\n\n\n我们选取了倒数第三个li节点，中括号中传入last()-2即可。因为last()是最后一个，所以last()-2就是倒数第三个。\n\n\n```python\nhtml.xpath('//li[last()-2]/a/text()')\n```\n\n\n\n```\n    ['third item']\n```\n\n\n如果想查询更多XPath的用法，可以查看：http://www.w3school.com.cn/xpath/index.asp。\n\n如果想查询更多Python lxml库的用法，可以查看http://lxml.de/。\n","tags":["xpath"]},{"title":"git 常用命令","url":"/2019/03/17/git常用命令/","content":"\n\n\n\n**git init** 把当前的目录变成可以管理的git仓库，生成隐藏.git文件。\n\n **git add XX** 把xx文件添加到暂存区去。\n\n **git commit –m “XX”** 提交文件 –m 后面的是注释。\n\n **git status** 查看仓库状态\n\n **git diff XX** 查看XX文件修改了那些内容\n\n **git log** 查看历史记录\n\n **git reset –hard HEAD^** 或者 **git reset –hard HEAD~** 回退到上一个版本(如果想回退到100个版本，使用**git reset –hard HEAD~100** )\n\n **cat XX** 查看XX文件内容\n\n **git reflog** 查看历史记录的版本号id\n\n **git checkout — XX** 把XX文件在工作区的修改全部撤销。\n\n **git rm XX** 删除XX文件\n\n **git remote add origin url** 关联一个远程库\n\n **git push –u(第一次要用-u 以后不需要) origin master** 把当前master分支推送到远程库\n\n **git clone url** 从远程库中克隆\n\n **git checkout –b dev** 创建dev分支 并切换到dev分支上\n\n **git branch** 查看当前所有的分支\n\n **git checkout master** 切换回master分支\n\n **git merge dev** 在当前的分支上合并dev分支\n\n **git branch –d dev** 删除dev分支\n\n **git branch name** 创建分支\n\n **git stash** 把当前的工作隐藏起来 等以后恢复现场后继续工作\n\n **git stash list** 查看所有被隐藏的文件列表\n\n **git stash apply** 恢复被隐藏的文件，但是内容不删除\n\n **git stash drop** 删除文件\n\n **git stash pop** 恢复文件的同时 也删除文件\n\n **git remote** 查看远程库的信息\n\n **git remote –v** 查看远程库的详细信息\n\n **git push origin master** Git会把master分支推送到远程库对应的远程分支上\n\n","tags":["git"],"categories":["常用工具"]},{"title":"T检验","url":"/2018/10/31/T检验/","content":"# 均值差异性的检验方法：T检验\n\n  数据分析中有一块很大的版图是属于均值对比的，应用广泛。例如，对比试验前后病人的症状，证明某种药是否有效；对比某个班级两次语文成绩，验证是否有提高；对比某个产品在投放广告前后的销量，看广告是否有效。这些都属于两均值对比的应用。\n\n  均值对比的假设检验方法主要有Z检验和T检验，它们的区别在于Z检验面向总体数据和大样本数据，而T检验适用于小规模抽样样本。下面分别介绍Z检验和T检验。\n\n\n\n  Z检验虽然能够进行均值差异性检验，但是，它要求总体标准差已知或者样本容量足够大，这是很难做到甚至无法达成的。这时候t检验就粉墨登场了，只需从正态总体中抽取小规模的样本数据，并计算均值与标准差，用来代替正态总体的均值和标准差即可，t值计算公式如下：\n\n![img](http://mmbiz.qpic.cn/mmbiz_jpg/YJotEuBMe44mx3ssxoRZOJm8gm3iabbb2Du5IibbRLHljoJhekG7RyW2SHd6lGicZRXMMTtE4iaPhUKPSBeZYO3wyg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)\n\n  样本数据计算得到的所有t值就组成了新的数据序列，这个新的数据形态就是t分布。t分布是曲线族，曲线与自由度密切相关，自由度为n-k-1（这里n是样本容量，k是样本中已知变量个数），自由度越小，曲线越低平，三自由度越大，曲线越接近正态分布。\n\n\n\n  有了t分布和t值计算公式，我们就能够进行T检验了，T检验在数据分析中的用途非常广，它是针对满足正态分布的数据所采取的均值差异显著性的检验方法。\n\n  T检验在使用前有三个应用的注意点：\n\n1、分析的数据对象需要满足正态分布，T检验前需判断样本是否正态分布；\n\n2、分析对比的统计量是均值；\n\n3、对比对象是两个，可以是两个样本；也可以是一个样本和一个常数；\n\n\n\nT检验有四种类别：\n\n1、配对样本的T检验；\n\n2、等方差的独立样本T检验；\n\n3、异方差的独立样本T检验；\n\n4、单样本的T检验。T检验与Z检验不同，需要考虑样本方差是否相同，这是因为自由度决定了T分布曲线，同时，自由度也影响样本方差。下面分别介绍四种T检验的检验公式。\n\n1、配对样本的T检验\n\n  所谓配对样本的T检验，是指参与对比的两列数据都是满足正态分布，而且两列数据之间存在一一对应关系。要想判断这种数据序列之间的差异是否显著，就可以使用配对样本T检验。处于待检验状态的两列配对样本，应该具有相同的数据个数，而且两列数据在语义上有一一对应关系。例如对同一个班级的两次考试成绩，这两次成绩都按照学号顺序存放，具有明确的对应关系。T检验公式如下：\n\n![img](http://mmbiz.qpic.cn/mmbiz_jpg/YJotEuBMe44mx3ssxoRZOJm8gm3iabbb2qUP5qLfervUjMkLNXIPicqWzByRduB463j2CEAzHoQ8gPXkTr0Ie4tw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)\n\n独立样本T检验\n\n   独立样本是两个没有对应关系的独立正态分布数据集合，可以有不同的数据个数，例如，对同一学校的某次考试，如果需要检验男生与女生的成绩之间有无显著性差异在总体成绩满足正态分布的情况下，则都可以使用独立样本的T检验，但是在进行T检验之前，需要明确两个样本的方差是否相同，然后根据方差齐性与否选择相应的计算方法。\n\n2、等方差独立样本T检验\n\n![img](http://mmbiz.qpic.cn/mmbiz_jpg/YJotEuBMe44mx3ssxoRZOJm8gm3iabbb2qARzh5ib2IhtcibRfNoVKKVJxqa1yCrgbLT8lGFtDficI4fpryBRmwgvg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)\n\n3、异方差独立样本T检验\n\n![img](http://mmbiz.qpic.cn/mmbiz_jpg/YJotEuBMe44mx3ssxoRZOJm8gm3iabbb2OJJT3qIUcmZHPhUQibjZINgfOgdyr3PiadUoeSUwfLQoykgQicBEstRCA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)\n\n4、单样本T检验\n\n除了针对两列正态分布数据的均值差异显著性检验，有时还经常需要判断单列正态分布数据是否与某一给定值有显著性差异，或单列正态分布数据是否来自满足某一均值的总体。例如，判断某班语文成绩的均值是否与80分有显著性差别。T检验公式为：\n\n![img](http://mmbiz.qpic.cn/mmbiz_jpg/YJotEuBMe44mx3ssxoRZOJm8gm3iabbb2wU5G5u6VJ31sRqb2et6GIfC5qRXxQZ3yLBlpBDzVGSsAicHxOJY2GPg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)\n","tags":["数据挖掘"]},{"title":"T-test","url":"/2018/10/31/T-test/","content":"\n# 单样本T检验-ttest_1samp \n\n\n```python\nfrom scipy import stats\nimport numpy as np\nnp.random.seed(7654567)\nrvs = stats.norm.rvs(loc=5,scale=10,size=(100,2))  # 均值为5，方差为10\nr\n```\n\n检验两列数的均值与1和2的差异是否显著\n\n\n```python\nstats.ttest_1samp(rvs, [1, 2])   \n# 分别显示两列数的t统计量和p值。\n# 由p值分别为0.042和0.018，\n#  当p值小于0.05时，认为差异显著，即第一列数的均值不等于1，第二列数的均值不等于2。\n```\n\n\n\n\n    Ttest_1sampResult(statistic=array([3.71134343, 2.85084277]), pvalue=array([0.00034072, 0.00530677]))\n\n\n\n不拒绝原假设——均值等于5\n\n\n```python\nstats.ttest_1samp(rvs, 5.0) \n```\n\n\n\n\n    Ttest_1sampResult(statistic=array([-0.01631806, -0.25538594]), pvalue=array([0.98701349, 0.79895486]))\n\n\n\n拒绝原假设——均值不等于5\n\n\n```python\nstats.ttest_1samp(rvs, 0.0) \n```\n\n\n\n\n    Ttest_1sampResult(statistic=array([4.6432588 , 4.92166191]), pvalue=array([1.05510059e-05, 3.42688414e-06]))\n\n\n\n第一行数均值等于5，第二行数均值不等于0\naxis=0按列运算，axis=1按行运算\n\n\n```python\nstats.ttest_1samp(rvs.T,[5.0,0.0],axis=1)\n```\n\n\n\n\n    Ttest_1sampResult(statistic=array([-0.01631806,  4.92166191]), pvalue=array([9.87013492e-01, 3.42688414e-06]))\n\n\n\n#  两独立样本t检验-ttest_ind\n\n生成数据\n\n\n```python\nnp.random.seed(12345678)\n#loc:平均值  scale：方差\nrvs1 = stats.norm.rvs(loc=5,scale=10,size=500)  \nrvs2 = stats.norm.rvs(loc=5,scale=10,size=500)\n```\n\n当两总体方差相等时，即具有“方差齐性”，可以直接检验 \n不拒绝原假设——两总体均值相等\n\n\n```python\nstats.ttest_ind(rvs1,rvs2)\n```\n\n\n\n\n    Ttest_indResult(statistic=0.26833823296238857, pvalue=0.788494433695651)\n\n\n\n当不确定两总体方差是否相等时，应先利用levene检验，检验两总体是否具有方差齐性。\n\n\n```python\nstats.levene(rvs1, rvs2)\n```\n\n\n\n\n    LeveneResult(statistic=0.9775501222315258, pvalue=0.323044034693146)\n\n\n\np值远大于0.05，认为两总体具有方差齐性。\n\n如果两总体不具有方差齐性，需要将equal_val参数设定为“False”。\n\n需注意的情况：\n\n如果两总体具有方差齐性，错将equal_var设为False，p值变大\n\n\n```python\nstats.ttest_ind(rvs1,rvs2, equal_var = False)\n```\n\n\n\n\n    Ttest_indResult(statistic=0.26833823296238857, pvalue=0.7884945274950106)\n\n\n\n两总体方差不等时，若没有将equal_var参数设定为False，则函数会默认equal_var为True，这样会低估p值\n\n\n```python\nrvs3 = stats.norm.rvs(loc=5, scale=20, size=500)\nstats.ttest_ind(rvs1, rvs3, equal_var = False)\n# 正确的p值 \n```\n\n\n\n\n    Ttest_indResult(statistic=-0.46580283298287956, pvalue=0.6414964624656874)\n\n\n\n\n```python\nstats.ttest_ind(rvs1, rvs3)\n#  被低估的p值 \n```\n\n\n\n\n    Ttest_indResult(statistic=-0.46580283298287956, pvalue=0.6414582741343561)\n\n\n\n当两样本数量不等时，equal_val的变化会导致t统计量变化 \nrvs1：来自总体——均值5，方差10，样本数500 \nrvs4：来自总体——均值5，方差20，样本数100 \n两总体不具有方差齐性，应设定equal_var=False\n\n\n\n```python\nrvs4 = stats.norm.rvs(loc=5, scale=20, size=100)\nstats.ttest_ind(rvs1, rvs4)\n# 错误的t统计量 \n```\n\n\n\n\n    Ttest_indResult(statistic=-0.9988253944278285, pvalue=0.3182832709103878)\n\n\n\n\n```python\nstats.ttest_ind(rvs1, rvs4, equal_var = False) # 错误的t统计量 \n```\n\n\n\n\n    Ttest_indResult(statistic=-0.6971257058465435, pvalue=0.4871692772540187)\n\n\n\n不同均值，不同方差，不同样本量的t检验 \n错误的检验：未将equal_var设定为False\n\n\n```python\nrvs5 = stats.norm.rvs(loc=8, scale=20, size=100)\nstats.ttest_ind(rvs1, rvs5)\n```\n\n\n\n\n    Ttest_indResult(statistic=-1.467966985449067, pvalue=0.14263895620529113)\n\n\n\n正确的检验：\n\n\n```python\nstats.ttest_ind(rvs1, rvs5, equal_var = False)\n```\n\n\n\n\n    Ttest_indResult(statistic=-0.9436597361713308, pvalue=0.3474417033479409)\n\n\n\n# 配对样本t检验\n\n\n```python\nnp.random.seed(12345678)\n```\n\n不拒绝原假设，认为rvs1 与 rvs2 所代表的总体均值相等\n\n\n```python\nrvs1 = stats.norm.rvs(loc=5,scale=10,size=500)\nrvs2 = (stats.norm.rvs(loc=5,scale=10,size=500) + stats.norm.rvs(scale=0.2,size=500))\nstats.ttest_rel(rvs1,rvs2)\n```\n\n\n\n\n    Ttest_relResult(statistic=0.24101764965300979, pvalue=0.8096404344581155)\n\n\n\n拒绝原假设，认为rvs1 与 rvs3所代表的总体均值不相等\n\n\n```python\nrvs3 = (stats.norm.rvs(loc=8,scale=10,size=500) + stats.norm.rvs(scale=0.2,size=500))\nstats.ttest_rel(rvs1,rvs3)\n```\n\n\n\n\n    Ttest_relResult(statistic=-3.9995108708727924, pvalue=7.308240219166128e-05)\n\n\n\n","tags":["数据挖掘"]},{"title":"递归","url":"/2018/10/24/递归/","content":"\n递归是一种解决问题的方法，将问题分解为更小的子问题，直到得到一个足够小的问题可以被很简单的解决。通常递归涉及函数调用自身。递归允许我们编写优雅的解决方案，解决可能很难编程的问题。\n\n## 计算整数列表和\n\n我们先不用递归来实现求一个整数列表的和\n\n```python\ndef sum(list):\n    sum = 0\n    for item in list:\n        sum += item\n    return sum\nprint(sum([1,2,3,4,5]))\n        \n```\n\n```\n15\n```\n\n递归实现\n\n```python\ndef sum(list):\n    if len(list)==1:\n        return list[0]\n    return list[0]+sum(list[1:])\nprint(sum([1,2,3,4,5]))\n```\n\n```\n15\n```\n\n所有递归算法必须服从三个重要的定律：\n递归算法必须具有基本情况。\n递归算法必须改变其状态并向基本情况靠近。\n递归算法必须以递归方式调用自身。\n\n递归实现任意进制转化\n\n```python\ndef toStr(n,base):\n   convertString = \"0123456789ABCDEF\"\n   if n < base:\n      return convertString[n]\n   else:\n      return toStr(n//base,base) + convertString[n%base]\n\nprint(toStr(1453,16)[-0:])\n```\n\n```\n5AD\n```\n\n栈帧：实现递归\n\n```python\nclass Stack:\n     def __init__(self):\n         self.items = []\n\n     def isEmpty(self):\n         return self.items == []\n\n     def push(self, item):\n         self.items.append(item)\n\n     def pop(self):\n         return self.items.pop()\n\n     def peek(self):\n         return self.items[len(self.items)-1]\n\n     def size(self):\n         return len(self.items)\n```\n\n```python\nrStack = Stack()\n\ndef toStr(n,base):\n    convertString = \"0123456789ABCDEF\"\n    while n > 0:\n        if n < base:\n            rStack.push(convertString[n])\n        else:\n            rStack.push(convertString[n % base])\n        n = n // base\n    res = \"\"\n    while not rStack.isEmpty():\n        res = res + str(rStack.pop())\n    return res\n\nprint(toStr(1453,16))\n```\n\n```\n5AD\n```\n\n可视化递归\n\n```python\nimport turtle\n\nmyTurtle = turtle.Turtle()\nmyWin = turtle.Screen()\n\ndef drawSpiral(myTurtle, lineLen):\n    if lineLen > 0:\n        myTurtle.forward(lineLen)\n        myTurtle.right(90)\n        drawSpiral(myTurtle,lineLen-5)\n\ndrawSpiral(myTurtle,300)\nmyWin.exitonclick()\n```\n\n谢尔宾斯基三角形\n\n```python\nimport turtle\n\ndef drawTriangle(points,color,myTurtle):\n    myTurtle.fillcolor(color)\n    myTurtle.up()\n    myTurtle.goto(points[0][0],points[0][1])\n    myTurtle.down()\n    myTurtle.begin_fill()\n    myTurtle.goto(points[1][0],points[1][1])\n    myTurtle.goto(points[2][0],points[2][1])\n    myTurtle.goto(points[0][0],points[0][1])\n    myTurtle.end_fill()\n\ndef getMid(p1,p2):\n    return ( (p1[0]+p2[0]) / 2, (p1[1] + p2[1]) / 2)\n\ndef sierpinski(points,degree,myTurtle):\n    colormap = ['blue','red','green','white','yellow',\n                'violet','orange']\n    drawTriangle(points,colormap[degree],myTurtle)\n    if degree > 0:\n        sierpinski([points[0],\n                        getMid(points[0], points[1]),\n                        getMid(points[0], points[2])],\n                   degree-1, myTurtle)\n        sierpinski([points[1],\n                        getMid(points[0], points[1]),\n                        getMid(points[1], points[2])],\n                   degree-1, myTurtle)\n        sierpinski([points[2],\n                        getMid(points[2], points[1]),\n                        getMid(points[0], points[2])],\n                   degree-1, myTurtle)\n\ndef main():\n   myTurtle = turtle.Turtle()\n   myWin = turtle.Screen()\n   myPoints = [[-300,-150],[0,300],[300,-150]]\n   sierpinski(myPoints,5,myTurtle)\n   myWin.exitonclick()\n\nmain()\n```\n","tags":["算法"]},{"title":"链表","url":"/2018/10/24/链表/","content":"# 链表\n\n链表实现的基本构造块是节点。每个节点对象必须至少保存两个信息。\n首先，节点必须包含列表项本身。我们将这个称为节点的数据字段。此外，每个节点必须保存对下一个节点的引用。\n\n## 无序链表\n\n```python\nclass Node():\n    def __init__(self,initdata):\n        self.data = initdata\n        self.next = None\n\n    def getData(self):\n        return self.data\n\n    def getNext(self):\n        return self.next\n\n    def setData(self,newdata):\n        self.data = newdata\n\n    def setNext(self,newnext):\n        self.next = newnext\n```\n\n```python\ntmp=Node(99)\ntmp.getData()\n```\n\n\n\n```\n99\n```\n\n\n\n```python\nclass UnorderedList():\n\n    def __init__(self):\n        self.head = None\n        \n    def isEmpty(self):\n        return self.head == None\n\n    def add(self,item):\n        temp = Node(item)\n        temp.setNext(self.head)\n        self.head = temp\n    \n    def size(self):\n        current = self.head\n        count = 0\n        while current != None:\n            count = count + 1\n            current = current.getNext()\n        return count\n    \n    def search(self,item):\n        current = self.head\n        found = False\n        while current != None and not found:\n            if current.getData() == item:\n                found = True\n            else:\n                current = current.getNext()\n\n        return found\n\n    def remove(self,item):\n        current=self.head\n        previous=None\n        found = False\n        while current != None and not found:\n            if current.getData() == item:\n                found = True\n            else:\n                previous=current\n                current= current.getNext()\n        if previous == None:\n            self.head = current.getNext()\n        else:\n            previous.setNext(current.getNext())\n\n```\n\n```python\nmylist = UnorderedList()\n\n```\n\n```python\nmylist.add(13)\nmylist.add(23)\n```\n\n```python\nmylist.size()\n```\n\n\n\n```\n2\n```\n\n\n\n```python\nmylist.search(13)\n```\n\n\n\n```\nTrue\n```\n\n\n\n```python\nmylist.remove(23)\n```\n\n```python\nmylist.search(23)\n```\n\n\n\n```\nFalse\n```\n\n\n\n## 有序列表\n\n```python\nclass OrderedList:\n    def __init__(self):\n        self.head = None\n               \n    def isEmpty(self):\n        return self.head == None\n\n    def add(self,item):\n        temp = Node(item)\n        temp.setNext(self.head)\n        self.head = temp\n    \n    def size(self):\n        current = self.head\n        count = 0\n        while current != None:\n            count = count + 1\n            current = current.getNext()\n        return count\n        \n        \n    def search(self,item):\n        current = self.head\n        found = False\n        stop = False\n        while current != None and not found and not stop:\n            if current.getData() == item:\n                found = True\n            else:\n                if current.getData() > item:\n                    stop = True\n                else:\n                    current = current.getNext()\n\n        return found\n    \n    def add(self,item):\n        current = self.head\n        previous = None\n        stop = False\n        while current != None and not stop:\n            if current.getData() > item:\n                stop = True\n            else:\n                previous = current\n                current = current.getNext()\n\n        temp = Node(item)\n        if previous == None:\n            temp.setNext(self.head)\n            self.head = temp\n        else:\n            temp.setNext(current)\n            previous.setNext(temp)\n```\n","tags":["算法"]},{"title":"deque","url":"/2018/10/22/deque/","content":"# Deque抽象数据类型\n\ndeque 抽象数据类型由以下结构和操作定义。如上所述，deque 被构造为项的有序集合，其中项从首部或尾部的任一端添加和移除。下面给出了 deque 操作。\n\n1. Deque() 创建一个空的新 deque。它不需要参数，并返回空的 deque。\n2. addFront(item) 将一个新项添加到 deque 的首部。它需要 item 参数 并不返回任何内容。\n3. addRear(item) 将一个新项添加到 deque 的尾部。它需要 item 参数并不返回任何内容。\n4. removeFront() 从 deque 中删除首项。它不需要参数并返回 item。deque 被修改。\n5. removeRear() 从 deque 中删除尾项。它不需要参数并返回 item。deque 被修改。\n6. isEmpty() 测试 deque 是否为空。它不需要参数，并返回布尔值。\n7. size() 返回 deque 中的项数。它不需要参数，并返回一个整数。\n\n## Python实现Deque\n\n```python\nclass Deque:\n    def __init__(self):\n        self.items = []\n\n    def isEmpty(self):\n        return self.items == []\n\n    def addFront(self, item):\n        self.items.append(item)\n\n    def addRear(self, item):\n        self.items.insert(0,item)\n\n    def removeFront(self):\n        return self.items.pop()\n\n    def removeRear(self):\n        return self.items.pop(0)\n\n    def size(self):\n        return len(self.items)\n```\n\n在 removeFront 中，我们使用 pop 方法从列表中删除最后一个元素。 但是，在removeRear中，pop(0)方法必须删除列表的第一个元素。同样，我们需要在 addRear 中使用insert方法（第12行），因为 append 方法在列表的末尾添加一个新元素。\n你可以看到许多与栈和队列中描述的 Python 代码相似之处。你也可能观察到，在这个实现中，从前面添加和删除项是 O(1)，而从后面添加和删除是 O(n)。 考虑到添加和删除项是出现的常见操作，这是可预期的。 同样，重要的是要确定我们知道在实现中前后都分配在哪里。\n\n## 回文检查\n\n使用 deque 数据结构可以容易地解决经典回文问题。回文是一个字符串，读取首尾相同的字符，例如，radar toot madam。 我们想构造一个算法输入一个字符串，并检查它是否是一个回文。\n该问题的解决方案将使用 deque 来存储字符串的字符。我们从左到右处理字符串，并将每个字符添加到 deque 的尾部。在这一点上，deque 像一个普通的队列。然而，我们现在可以利用 deque 的双重功能。 deque 的首部保存字符串的第一个字符，deque 的尾部保存最后一个字符\n![](https://xidianwlc.gitbooks.io/python-data-structrue-and-algrothms/content/3.%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/3.18.%E5%9B%9E%E6%96%87%E6%A3%80%E6%9F%A5/assets/3.18.%E5%9B%9E%E6%96%87%E6%A3%80%E6%9F%A5.figure2.png)\n\n```python\ndef palchecker(aString):\n    chardeque = Deque()\n\n    for ch in aString:\n        chardeque.addRear(ch)\n\n    stillEqual = True\n\n    while chardeque.size() > 1 and stillEqual:\n        first = chardeque.removeFront()\n        last = chardeque.removeRear()\n        if first != last:\n            stillEqual = False\n\n    return stillEqual\n\nprint(palchecker(\"lsdkjfskf\"))\nprint(palchecker(\"radar\"))\n```\n\n```\nFalse\nTrue\n```\n\n\n","tags":["算法"]},{"title":"队列","url":"/2018/10/22/队列/","content":"# 队列\n\n我们为了实现队列抽象数据类型创建一个新类。和前面一样，我们将使用列表集合来作为构建队列的内部表示。\n\n我们需要确定列表的哪一端作为队首，哪一端作为队尾。\n如下所示的实现假定队尾在列表中的位置为 0。\n这允许我们使用列表上的插入函数向队尾添加新元素。弹出操作可用于删除队首的元素（列表的最后一个元素）。回想一下，这也意味着入队为 O(n)，出队为 O(1)。\n\n```python\nclass Queue():\n    def __init__(self):\n        self.items=[]\n    def isEmpty(self):\n        return self.items == []\n    def enqueue(self,item):\n        self.items.insert(0,item)\n    def dequeue(self):\n        return self.items.pop()\n    def size(self):\n        return len(self.items)\n    \n\n```\n\n## 初始化队列以及基础应用\n\n```python\nq = Queue()\nq.isEmpty()\n\n```\n\n\n\n```\nTrue\n```\n\n\n\n```python\nq.enqueue(1)\nq.dequeue()\n```\n\n\n\n```\n1\n```\n\n\n\n```python\nfor i in range(10):\n    q.enqueue(i)\n```\n\n```python\nq.isEmpty()\n```\n\n\n\n```\nFalse\n```\n\n\n\n```python\nq.size()\n```\n\n\n\n```\n10\n```\n\n\n\n```python\nwhile not q.isEmpty():\n    print(q.dequeue())\n```\n\n```\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n```\n\n## 队列的应用\n\n### 烫手山芋\n\n队列的典型应用之一是模拟需要以 FIFO 方式管理数据的真实场景。首先，让我们看看孩子们的游戏烫手山芋，在这个游戏中，孩子们围成一个圈，并尽可能快的将一个山芋递给旁边的孩子。在某一个时间，动作结束，有山芋的孩子从圈中移除。游戏继续开始直到剩下最后一个孩子。\n![](https://xidianwlc.gitbooks.io/python-data-structrue-and-algrothms/content/3.%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/3.13.%E6%A8%A1%E6%8B%9F%EF%BC%9A%E7%83%AB%E6%89%8B%E5%B1%B1%E8%8A%8B/assets/3.13.%E6%A8%A1%E6%8B%9F%EF%BC%9A%E7%83%AB%E6%89%8B%E5%B1%B1%E8%8A%8B.figure2.png)\n\n为了模拟这个圈，我们使用队列。假设拿着山芋的孩子在队列的前面。当拿到山芋的时候，这个孩子将先出列再入队列，把他放在队列的最后。经过 num 次的出队入队后，前面的孩子将被永久移除队列。并且另一个周期开始，继续此过程，直到只剩下一个名字（队列的大小为 1）\n![](https://xidianwlc.gitbooks.io/python-data-structrue-and-algrothms/content/3.%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/3.13.%E6%A8%A1%E6%8B%9F%EF%BC%9A%E7%83%AB%E6%89%8B%E5%B1%B1%E8%8A%8B/assets/3.13.%E6%A8%A1%E6%8B%9F%EF%BC%9A%E7%83%AB%E6%89%8B%E5%B1%B1%E8%8A%8B.figure3.png)\n\n```python\ndef hotPotato(namelist, num):\n    simqueue = Queue()\n    for name in namelist:\n        simqueue.enqueue(name)\n\n    while simqueue.size() > 1:\n        for i in range(num):\n            simqueue.enqueue(simqueue.dequeue())\n\n        simqueue.dequeue()\n\n    return simqueue.dequeue()\n\nprint(hotPotato([\"Bill\",\"David\",\"Susan\",\"Jane\",\"Kent\",\"Brad\"],7))\n```\n\n请注意，在此示例中，计数常数的值大于列表中的名称数。这不是一个问题，因为队列像一个圈，计数会重新回到开始，直到达到计数值。另外，请注意，列表加载到队列中以使列表上的名字位于队列的前面。在这种情况下，Bill 是列表中的第一个项，因此他在队列的前面。\n\n### 打印机\n\n主要模拟步骤\n\n1. 创建打印任务的队列，每个任务都有个时间戳。队列启动的时候为空。\n2. 每秒（currentSecond）：\n   - 是否创建新的打印任务？如果是，将 currentSecond 作为时间戳添加到队列。\n   - 如果打印机不忙并且有任务在等待\n     - 从打印机队列中删除一个任务并将其分配给打印机\n     - 从 currentSecond 中减去时间戳，以计算该任务的等待时间。\n     - 将该任务的等待时间附件到列表中稍后处理。\n     - 根据打印任务的页数，确定需要多少时间。\n   - 打印机需要一秒打印，所以得从该任务的所需的等待时间减去一秒。\n   - 如果任务已经完成，换句话说，所需的时间已经达到零，打印机空闲。\n3. 模拟完成后，从生成的等待时间列表中计算平均等待时间。\n\n为了设计此模拟，我们将为上述三个真实世界对象创建类：Printer, Task, PrintQueue\n\nPrinter 类需要跟踪它当前是否有任务。\n\n如果有，则它处于忙碌状态（13-17 行），并且可以从任务的页数计算所需的时间。\n\n构造函数允许初始化每分钟页面的配置，tick 方法将内部定时器递减直到打印机设置为空闲(11 行)\n\n```python\nclass Printer:\n    def __init__(self, ppm):\n        self.pagerate = ppm\n        self.currentTask = None\n        self.timeRemaining = 0\n\n    def tick(self):\n        if self.currentTask != None:\n            self.timeRemaining = self.timeRemaining - 1\n            if self.timeRemaining <= 0:\n                self.currentTask = None\n\n    def busy(self):\n        if self.currentTask != None:\n            return True\n        else:\n            return False\n\n    def startNext(self,newtask):\n        self.currentTask = newtask\n        self.timeRemaining = newtask.getPages() * 60/self.pagerate\n```\n\nTask 类表示单个打印任务。创建任务时，随机数生成器将提供 1 到 20 页的长度。我们选择使用随机模块中的 randrange 函数\n每个任务还需要保存一个时间戳用于计算等待时间。此时间戳将表示任务被创建并放置到打印机队列中的时间。可以使用 waitTime 方法来检索在打印开始之前队列中花费的时间。\n\n```python\nimport random\n\nclass Task:\n    def __init__(self,time):\n        self.timestamp = time\n        self.pages = random.randrange(1,21)\n\n    def getStamp(self):\n        return self.timestamp\n\n    def getPages(self):\n        return self.pages\n\n    def waitTime(self, currenttime):\n        return currenttime - self.timestamp\n```\n\n以下代码实现了上述算法。PrintQueue 对象是我们现有队列 ADT 的一个实例。\nnewPrintTask 决定是否创建一个新的打印任务。我们再次选择使用随机模块的 randrange 函数返回 1 到 180 之间的随机整数。\n打印任务每 180 秒到达一次。通过从随机整数（32 行）的范围中任意选择，我们可以模拟这个随机事件。\n模拟功能允许我们设置打印机的总时间和每分钟的页数。\n\n```python\ndef simulation(numSeconds, pagesPerMinute):\n\n    labprinter = Printer(pagesPerMinute)\n    printQueue = Queue()\n    waitingtimes = []\n\n    for currentSecond in range(numSeconds):\n\n      if newPrintTask():\n         task = Task(currentSecond)\n         printQueue.enqueue(task)\n\n      if (not labprinter.busy()) and (not printQueue.isEmpty()):\n        nexttask = printQueue.dequeue()\n        waitingtimes.append(nexttask.waitTime(currentSecond))\n        labprinter.startNext(nexttask)\n\n      labprinter.tick()\n\n    averageWait=sum(waitingtimes)/len(waitingtimes)\n    print(\"Average Wait %6.2f secs %3d tasks remaining.\"%(averageWait,printQueue.size()))\n\ndef newPrintTask():\n    num = random.randrange(1,181)\n    if num == 180:\n        return True\n    else:\n        return False\n\n\n```\n当我们运行模拟时，我们不应该担心每次的结果不同。这是由于随机数的概率性质决定的。 因为模拟的参数可以被调整，我们对调整后可能发生的趋势感兴趣。 这里有一些结果。\n首先，我们将使用每分钟五页的页面速率运行模拟 60 分钟（3,600秒）。 此外，我们将进行 10 次独立试验。记住，因为模拟使用随机数，每次运行将返回不同的结果。\n\n```python\nfor i in range(10):\n    simulation(3600,5)\n```\n\n```\nAverage Wait 133.21 secs   0 tasks remaining.\nAverage Wait  69.95 secs   1 tasks remaining.\nAverage Wait  18.21 secs   0 tasks remaining.\nAverage Wait 156.15 secs   1 tasks remaining.\nAverage Wait 124.05 secs   1 tasks remaining.\nAverage Wait 194.70 secs   4 tasks remaining.\nAverage Wait  48.24 secs   1 tasks remaining.\nAverage Wait  80.65 secs   0 tasks remaining.\nAverage Wait  62.31 secs   0 tasks remaining.\nAverage Wait  85.43 secs   2 tasks remaining.\n```\n\n\n","tags":["算法"]},{"title":"数据挖掘基本知识","url":"/2018/10/22/数据挖掘基本知识/","content":"# 数据挖掘的基础知识点\n\n-  数据、信息和知识是广义数据表现的不同形式\n\n-  主要知识模式类型有：广义知识，关联知识，类知识，预测型知识，特异型知识\n\n\n\n-  web挖掘研究的主要流派有：Web结构挖掘、Web使用挖掘、Web内容挖掘\n\n- 一般地说，KDD是一个多步骤的处理过程，一般分为**问题定义**、**数据抽取**、**数据预处理**、.**数据挖掘**以及**模式评估**等基本阶段。\n\n- 数据库中的知识发现处理过程模型有：阶梯处理过程模型，螺旋处理过程模型，以用户为中心的处理结构模型，联机KDD模型，支持多数据源多知识模式的KDD处理模型\n-  粗略地说，知识发现软件或工具的发展经历了独立的知识发现软件、横向的知识发现工具集和纵向的知识发现解决方案三个主要阶段，其中后面两种反映了目前知识发现软件的两个主要发展方向。\n\n-  决策树分类模型的建立通常分为两个步骤：决策树生成，决策树修剪。\n\n\n\n-  从使用的主要技术上看，可以把分类方法归结为四种类型：\n\n  - ​\t基于距离的分类方法\n\n  - ​\t决策树分类方法\n\n  - ​\t贝叶斯分类方法\n\n  - ​\t规则归纳方法\n\n-  关联规则挖掘问题可以划分成两个子问题：\n  1. 发现频繁项目集:通过用户给定Minsupport ，寻找所有频繁项目集或者最大频繁项目集。\n  2. 生成关联规则:通过用户给定Minconfidence ，在频繁项目集中，寻找关联规则。\n\n- 数据挖掘是相关学科充分发展的基础上被提出和发展的，主要的相关技术：\n  - 数据库等信息技术的发展\n  - 统计学深入应用\n  - 人工智能技术的研究和应用\n\n \n\n- 衡量关联规则挖掘结果的有效性，应该从多种综合角度来考虑：\n  - 准确性：挖掘出的规则必须反映数据的实际情况。\n  - 实用性：挖掘出的规则必须是简洁可用的。\n  - 新颖性：挖掘出的关联规则可以为用户提供新的有价值信息。\n\n-  约束的常见类型有：\n\n  ​\t单调性约束;\n\n  ​\t反单调性约束;\n\n  ​\t可转变的约束;\n\n  ​\t简洁性约束.\n\n- 根据规则中涉及到的层次，多层次关联规则可以分为：\n\n  ​\t同层关联规则：如果一个关联规则对应的项目是同一个粒度层次，那么它是同层关联规则。\n\n  ​\t层间关联规则：如果在不同的粒度层次上考虑问题，那么可能得到的是层间关联规\n\n \n\n-  按照聚类分析算法的主要思路，聚类方法可以被归纳为如下几种。\n\n  - 划分法：基于一定标准构建数据的划分。\n\n  - 属于该类的聚类方法有：k-means、k-modes、k-prototypes、k-medoids、PAM、CLARA、CLARANS等。\n  - 层次法：对给定数据对象集合进行层次的分解\n  - 密度法：基于数据对象的相连密度评价。\n  - 网格法：将数据空间划分成为有限个单元(Cell)的网格结构，基于网格结构进行聚类。\n  - 模型法：给每一个簇假定一个模型，然后去寻找能够很好的满足这个模型的数据集。\n\n- 类间距离的度量主要有：\n  - 最短距离法：定义两个类中最靠近的两个元素间的距离为类间距离。\n  - 最长距离法：定义两个类中最远的两个元素间的距离为类间距离。\n  - 中心法：定义两类的两个中心间的距离为类间距离。\n  - 类平均法：它计算两个类中任意两个元素间的距离，并且综合他们为类间距离：离差平方和。\n\n \n\n- 层次聚类方法具体可分为：\n  - 凝聚的层次聚类：一种自底向上的策略，首先将每个对象作为一个簇，然后合并这些原子簇为越来越大的簇，直到某个终结条件被满足。\n  - 分裂的层次聚类：采用自顶向下的策略，它首先将所有对象置于一个簇中，然后逐渐细分为越来越小的簇，直到达到了某个终结条件。\n  - 层次凝聚的代表是AGNES算法。层次分裂的代表是DIANA算法。\n\n-   文本挖掘(TD)的方式和目标是多种多样的，基本层次有：\n  - 关键词检索：最简单的方式，它和传统的搜索技术类似。\n  - 挖掘项目关联：聚焦在页面的信息(包括关键词)之间的关联信息挖掘上。\n  - 信息分类和聚类：利用数据挖掘的分类和聚类技术实现页面的分类，将页面在一个更到层次上进行抽象和整理。\n  - 自然语言处理：揭示自然语言处理技术中的语义，实现Web内容的更精确处理。 \n\n-  在web访问挖掘中常用的技术：\n  - 路径分析\n    - 路径分析最常用的应用是用于判定在一个Web站点中最频繁访问的路径，这样的知识对于一个电子商务网站或者信息安全评估是非常重要的。\n  - 关联规则发现\n    - 使用关联规则发现方法可以从Web访问事务集中，找到一般性的关联知识。\n  - 序列模式发现\n    - 在时间戳有序的事务集中，序列模式的发现就是指找到那些如“一些项跟随另一个项”这样的内部事务模式。\n  - 分类\n    - 发现分类规则可以给出识别一个特殊群体的公共属性的描述。这种描述可以用于分类新的项。\n  - 聚类\n    - 可以从Web Usage数据中聚集出具有相似特性的那些客户。在Web事务日志中，聚类顾客信息或数据项，就能够便于开发和执行未来的市场战略。\n\n\n\n- 根据功能和侧重点不同，数据挖掘语言可以分为三种类型：\n  - 数据挖掘查询语言：希望以一种像SQL这样的数据库查询语言完成数据挖掘的任务。\n  - 数据挖掘建模语言：对数据挖掘模型进行描述和定义的语言，设计一种标准的数据挖掘建模语言，使得数据挖掘系统在模型定义和描述方面有标准可以遵循。\n  - 通用数据挖掘语言：通用数据挖掘语言合并了上述两种语言的特点，既具有定义模型的功能，又能作为查询语言与数据挖掘系统通信，进行交互式挖掘。通用数据挖掘语言标准化是目前解决数据挖掘行业出现问题的颇具吸引力的研究方向。\n\n \n\n- 规则归纳有四种策略：减法、加法，先加后减、先减后加策略。\n  - 减法策略：以具体例子为出发点，对例子进行推广或泛化，推广即减除条件(属性值)或减除合取项(为了方便，我们不考虑增加析取项的推广)，使推广后的例子或规则不覆盖任何反例。\n  - 加法策略：起始假设规则的条件部分为空(永真规则)，如果该规则覆盖了反例，则不停地向规则增加条件或合取项，直到该规则不再覆盖反例。\n  - 先加后减策略：由于属性间存在相关性，因此可能某个条件的加入会导致前面加入的条件没什么作用，因此需要减除前面的条件。\n  - 先减后加策略：道理同先加后减，也是为了处理属性间的相关性。\n\n \n\n-  数据挖掘定义有广义和狭义之分。\n  - 从广义的观点，数据挖掘是从大型数据集(可能是不完全的、有噪声的、不确定性的、各种存储形式的)中，挖掘隐含在其中的、人们事先不知道的、对决策有用的知识的过程。\n  - 从这种狭义的观点上，我们可以定义数据挖掘是从特定形式的数据集中提炼知识的过程。\n\n \n\n- web挖掘的含义： \n  - 针对包括Web页面内容、页面之间的结构、用户访问信息、电子商务信息等在内的各种Web数据，应用数据挖掘方法以帮助人们从因特网中提取知识，为访问者、站点经营者以及包括电子商务在内的基于因特网的商务活动提供决策支持。\n\n\n\n- K-近邻分类算法(K Nearest Neighbors，简称KNN)的定义：通过计算每个训练数据到待分类元组的距离，取和待分类元组距离最近的K个训练数据，K个数据中哪个类别的训练数据占多数，则待分类元组就属于哪个类别。\n\n- K-means算法的性能分析：\n\n  主要优点：\n\n  - 是解决聚类问题的一种经典算法，简单、快速。\n  - 对处理大数据集，该算法是相对可伸缩和高效率的。\n  - 当结果簇是密集的，它的效果较好。\n\n  主要缺点\n\n  - 在簇的平均值被定义的情况下才能使用，可能不适用于某些应用。\n  - 必须事先给出k(要生成的簇的数目)，而且对初值敏感，对于不同的初始值，可能会导致不同结果。\n  - 不适合于发现非凸面形状的簇或者大小差别很大的簇。而且，它对于“躁声”和孤立点数据是敏感的。\n\n- ID3算法的性能分析：\n  - ID3算法的假设空间包含所有的决策树，它是关于现有属性的有限离散值函数的一个完整空间。所以ID3算法避免了搜索不完整假设空间的一个主要风险：假设空间可能不包含目标函数。\n  - ID3算法在搜索的每一步都使用当前的所有训练样例，大大降低了对个别训练样例错误的敏感性。因此，通过修改终止准则，可以容易地扩展到处理含有噪声的训练数据。\n  - ID3算法在搜索过程中不进行回溯。所以，它易受无回溯的爬山搜索中的常见风险影响：收敛到局部最优而不是全局最优。\n\n \n\n- Apriori算法有两个致命的性能瓶颈:\n\n  - 多次扫描事务数据库，需要很大的I/O负载：\n\n    对每次k循环，侯选集Ck中的每个元素都必须通过扫描数据库一次来验证其是否加入Lk。假如有一个频繁大项目集包含10个项的话，那么就至少需要扫描事务数据库10遍。\n\n  - 可能产生庞大的侯选集：\n\n    由Lk-1产生k-侯选集Ck是指数增长的，例如104个1-频繁项目集就有可能产生接近107个元素的2-侯选集。如此大的侯选集对时间和主存空间都是一种挑战。a基于数据分割的方法：基本原理是“在一个划分中的支持度小于最小支持度的k-项集不可能是全局频繁的”。\n\n\n\n- 改善Apriori算法适应性和效率的主要的改进方法有：\n  - 基于数据分割(Partition)的方法：基本原理是“在一个划分中的支持度小于最小支持度的k-项集不可能是全局频繁的”。\n  - 基于散列的方法：基本原理是“在一个hash桶内支持度小于最小支持度的k-项集不可能是全局频繁的”。\n  - 基于采样的方法：基本原理是“通过采样技术，评估被采样的子集中，并依次来估计k-项集的全局频度”。\n  - 其他：如，动态删除没有用的事务：“不包含任何Lk的事务对未来的扫描结果不会产生影响，因而可以删除”。\n\n \n\n-  面向Web的数据挖掘比面向数据库和数据仓库的数据挖掘要复杂得多：\n  - 异构数据源环境：Web网站上的信息是异构: 每个站点的信息和组织都不一样;存在大量的无结构的文本信息、复杂的多媒体信息;站点使用和安全性、私密性要求各异等等。\n  - 数据的是复杂性：有些是无结构的(如Web页)，通常都是用长的句子或短语来表达文档类信息;有些可能是半结构的(如Email，HTML页)。当然有些具有很好的结构(如电子表格)。揭开这些复合对象蕴涵的一般性描述特征成为数据挖掘的不可推卸的责任。\n  - 动态变化的应用环境：\n  - Web的信息是频繁变化的，像新闻、股票等信息是实时更新的。\n  - 这种高变化也体现在页面的动态链接和随机存取上。\n  - Web上的用户是难以预测的。\n  - Web上的数据环境是高噪音的。\n\n- 简述知识发现项目的过程化管理I-MIN过程模型。\n  - MIN过程模型把KDD过程分成IM1、IM2、…、IM6等步骤处理，在每个步骤里，集中讨论几个问题，并按一定的质量标准来控制项目的实施。\n  - IM1任务与目的：它是KDD项目的计划阶段，确定企业的挖掘目标，选择知识发现模式，编译知识发现模式得到的元数据;其目的是将企业的挖掘目标嵌入到对应的知识模式中。\n  - IM2任务与目的：它是KDD的预处理阶段，可以用IM2a、IM2b、IM2c等分别对应于数据清洗、数据选择和数据转换等阶段。其目的是生成高质量的目标数据。\n  - IM3任务与目的：它是KDD的挖掘准备阶段，数据挖掘工程师进行挖掘实验，反复测试和验证模型的有效性。其目的是通过实验和训练得到浓缩知识(Knowledge Concentrate)，为最终用户提供可使用的模型。\n  - IM4任务与目的：它是KDD的数据挖掘阶段，用户通过指定数据挖掘算法得到对应的知识。\n  - IM5任务与目的：它是KDD的知识表示阶段，按指定要求形成规格化的知识。\n  - IM6任务与目的：它是KDD的知识解释与使用阶段，其目的是根据用户要求直观地输出知识或集成到企业的知识库中。\n\n-  改善Apriori算法适应性和效率的主要的改进方法有：\n  - 基于数据分割(Partition)的方法：基本原理是“在一个划分中的支持度小于最小支持度的k-项集不可能是全局频繁的”。\n  - 基于散列(Hash)的方法：基本原理是“在一个hash桶内支持度小于最小支持度的k-项集不可能是全局频繁的”。\n  - 基于采样(Sampling)的方法：基本原理是“通过采样技术，评估被采样的子集中，并依次来估计k-项集的全局频度”。\n  - 其他：如，动态删除没有用的事务：“不包含任何Lk的事务对未来的扫描结果不会产生影响，因而可以删除”。\n\n \n\n- 数据分类的两个步骤是什么?\n  - 建立一个模型，描述预定的数据类集或概念集\n  - 数据元组也称作样本、实例或对象。\n  - 为建立模型而被分析的数据元组形成训练数据集。\n  - 训练数据集中的单个元组称作训练样本，由于提供了每个训练样本的类标号，因此也称作有指导的学习。\n  - 通过分析训练数据集来构造分类模型，可用分类规则、决策树或数学公式等形式提供。\n  - 使用模型进行分类\n  - 首先评估模型(分类法)的预测准确率。\n  - 如果认为模型的准确率可以接受，就可以用它对类标号未知的数据元组或对象进行分类。\n\n- web访问信息挖掘的特点：\n  - Web访问数据容量大、分布广、内涵丰富和形态多样\n  - 一个中等大小的网站每天可以记载几兆的用户访问信息。\n  - 广泛分布于世界各处。\n  - 访问信息形态多样。\n  - 访问信息具有丰富的内涵。\n  - Web访问数据包含决策可用的信息\n  - 每个用户的访问特点可以被用来识别该用户和网站访问的特性。\n  - 同一类用户的访问，代表同一类用户的个性。\n  - 一段时期的访问数据代表了群体用户的行为和群体用户的共性。\n  - Web访问信息数据是网站的设计者和访问者进行沟通的桥梁。\n  - Web访问信息数据是开展数据挖掘研究的良好的对象。\n  - Web访问信息挖掘对象的特点\n  - 访问事务的元素是Web页面，事务元素之间存在着丰富的结构信息。\n  - 访问事务的元素代表的是每个访问者的顺序关系，事务元素之间存在着丰富的顺序信息。\n  - 每个页面的内容可以被抽象出不同的概念，访问顺序和访问量部分决定概念。\n  - 用户对页面存在不同的访问时长，访问长代表了用户的访问兴趣。\n\n \n\n- web页面内文本信息的挖掘：\n  - 挖掘的目标是对页面进行摘要和分类。\n    - 页面摘要：对每一个页面应用传统的文本摘要方法可以得到相应的摘要信息。\n    - 页面分类：分类器输入的是一个Web页面集(训练集)，再根据页面文本信息内容进行监督学习，然后就可以把学成的分类器用于分类每一个新输入的页面。\n\n{在文本学习中常用的方法是TFIDF向量表示法，它是一种文档的词集(Bag-of-Words)表示法，所有的词从文档中抽取出来，而不考虑词间的次序和文本的结构。这种构造二维表的方法是：\n\n- 每一列为一个词，列集(特征集)为辞典中的所有有区分价值的词，所以整个列集可能有几十万列之多。\n- 每一行存储一个页面内词的信息，这时，该页面中的所有词对应到列集(特征集)上。列集中的每一个列(词)，如果在该页面中不出现，则其值为0;如果出现k次，那么其值就为k;页面中的词如果不出现在列集上，可以被放弃。这种方法可以表征出页面中词的频度。\n\n对中文页面来说，还需先分词然后再进行以上两步处理。\n\n这样构造的二维表表示的是Web页面集合的词的统计信息，最终就可以采用Naive Bayesian方法或k-Nearest Neighbor等方法进行分类挖掘。\n\n在挖掘之前，一般要先进行特征子集的选取，以降低维数。\n\n**转自：数据在线；**\n\n\n","tags":["机器学习"]},{"title":"栈应用","url":"/2018/10/21/栈应用/","content":"# 栈应用\n\n## 栈数据结构的定义\n\n```python\nclass Stack:\n     def __init__(self):\n         self.items = []\n\n     def isEmpty(self):\n         return self.items == []\n\n     def push(self, item):\n         self.items.append(item)\n\n     def pop(self):\n         return self.items.pop()\n\n     def peek(self):\n         return self.items[len(self.items)-1]\n\n     def size(self):\n         return len(self.items)\n```\n\n```python\ns=Stack()\n\nprint(s.isEmpty())\ns.push(4)\ns.push('dog')\nprint(s.peek())\ns.push(True)\nprint(s.size())\nprint(s.isEmpty())\ns.push(8.4)\nprint(s.pop())\nprint(s.pop())\n```\n\n```\nTrue\ndog\n3\nFalse\n8.4\nTrue\n```\n\n## 简单的符号匹配\n\n```python\ndef parChecker(symbolString):\n    s = Stack()\n    balanced = True\n    index = 0\n    while index < len(symbolString) and balanced:\n        symbol = symbolString[index]\n        if symbol == \"(\":\n            s.push(symbol)\n        else:\n            if s.isEmpty():\n                balanced = False\n            else:\n                s.pop()\n\n        index = index + 1\n\n    if balanced and s.isEmpty():\n        return True\n    else:\n        return False\n\nprint(parChecker('((()))'))\nprint(parChecker('(()'))\n```\n\n```\nTrue\nFalse\n```\n\n## 符号匹配\n\n```python\ndef parChecker(symbolString):\n    s = Stack()\n    balanced = True\n    index = 0\n    while index < len(symbolString) and balanced:\n        symbol = symbolString[index]\n        if symbol in \"([{\":\n            s.push(symbol)\n        else:\n            if s.isEmpty():\n                balanced = False\n            else:\n                top = s.pop()\n                if not matches(top,symbol):\n                       balanced = False\n        index = index + 1\n    if balanced and s.isEmpty():\n        return True\n    else:\n        return False\n\ndef matches(open,close):\n    opens = \"([{\"\n    closers = \")]}\"\n    return opens.index(open) == closers.index(close)\n\n\nprint(parChecker('{{([][])}()}'))\nprint(parChecker('[{()]'))\n```\n\n```\nTrue\nFalse\n```\n\n## 十进制转二进制\n\n```python\ndef divideBy2(decNumber):\n    remstack = Stack()\n\n    while decNumber > 0:\n        rem = decNumber % 2\n        remstack.push(rem)\n        decNumber = decNumber // 2\n\n    binString = \"\"\n    while not remstack.isEmpty():\n        binString = binString + str(remstack.pop())\n\n    return binString\n\nprint(divideBy2(42))\n```\n\n```\n101010\n```\n\n## 十进制转化为多进制\n\n```python\ndef baseConverter(decNumber,base):\n    digits = \"0123456789ABCDEF\"\n\n    remstack = Stack()\n\n    while decNumber > 0:\n        rem = decNumber % base\n        remstack.push(rem)\n        decNumber = decNumber // base\n\n    newString = \"\"\n    while not remstack.isEmpty():\n        newString = newString + digits[remstack.pop()]\n\n    return newString\n\nprint(baseConverter(25,2))\nprint(baseConverter(25,16))\n```\n\n```\n11001\n19\n```\n\n## 中缀转后缀通用法\n\n```python\ndef infixToPostfix(infixexpr):\n    prec = {}\n    prec[\"*\"] = 3\n    prec[\"/\"] = 3\n    prec[\"+\"] = 2\n    prec[\"-\"] = 2\n    prec[\"(\"] = 1\n    opStack = Stack()\n    postfixList = []\n    tokenList = infixexpr.split()\n\n    for token in tokenList:\n        if token in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" or token in \"0123456789\":\n            postfixList.append(token)\n        elif token == '(':\n            opStack.push(token)\n        elif token == ')':\n            topToken = opStack.pop()\n            while topToken != '(':\n                postfixList.append(topToken)\n                topToken = opStack.pop()\n        else:\n            while (not opStack.isEmpty()) and \\\n               (prec[opStack.peek()] >= prec[token]):\n                  postfixList.append(opStack.pop())\n            opStack.push(token)\n\n    while not opStack.isEmpty():\n        postfixList.append(opStack.pop())\n    return \" \".join(postfixList)\n\nprint(infixToPostfix(\"A * B + C * D\"))\nprint(infixToPostfix(\"( A + B ) * C - ( D - E ) * ( F + G )\"))\n```\n\n```\nA B * C D * +\nA B + C * D E - F G + * -\n```\n\n## 后缀表达式求值\n\n```python\ndef postfixEval(postfixExpr):\n    operandStack = Stack()\n    tokenList = postfixExpr.split()\n\n    for token in tokenList:\n        if token in \"0123456789\":\n            operandStack.push(int(token))\n        else:\n            operand2 = operandStack.pop()\n            operand1 = operandStack.pop()\n            result = doMath(token,operand1,operand2)\n            operandStack.push(result)\n    return operandStack.pop()\n\ndef doMath(op, op1, op2):\n    if op == \"*\":\n        return op1 * op2\n    elif op == \"/\":\n        return op1 / op2\n    elif op == \"+\":\n        return op1 + op2\n    else:\n        return op1 - op2\n\nprint(postfixEval('7 8 + 3 2 + /'))\n```\n\n```\n3.0\n```\n\n\n","tags":["算法"]},{"title":"计算函数运行效率","url":"/2018/10/21/计算函数运行效率/","content":"计算函数时间\n\n```python\n\n```\n\n要捕获我们的每个函数执行所需的时间，我们将使用 Python 的 timeit 模块。timeit 模块旨在允许 Python 开发人员通过在一致的环境中运行函数并使用尽可能相似的操作系统的时序机制来进行跨平台时序测量。\n要使用 timeit，你需要创建一个 Timer 对象，其参数是两个 Python 语句。第一个参数是一个你想要执行时间的 Python 语句; 第二个参数是一个将运行一次以设置测试的语句。然后 timeit 模块将计算执行语句所需的时间。默认情况下，timeit 将尝试运行语句一百万次。 当它完成时，它返回时间作为表示总秒数的浮点值。由于它执行语句一百万次，可以读取结果作为执行测试一次的微秒数。你还可以传递 timeit 一个参数名字为 number，允许你指定执行测试语句的次数。以下显示了运行我们的每个测试功能 1000 次需要多长时间。\n\n```python\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndef test1():\n    l = []\n    for i in range(1000):\n        l = l + [i]\n\ndef test2():\n    l = []\n    for i in range(1000):\n        l.append(i)\n\ndef test3():\n    l = [i for i in range(1000)]\n\ndef test4():\n    l = list(range(1000))\n```\n\n```python\nimport timeit\nfrom timeit import Timer\nt1 = Timer(\"test1()\", \"from __main__ import test1\")\nprint(\"concat \",t1.timeit(number=1000), \"milliseconds\")\nt2 = Timer(\"test2()\", \"from __main__ import test2\")\nprint(\"append \",t2.timeit(number=1000), \"milliseconds\")\nt3 = Timer(\"test3()\", \"from __main__ import test3\")\nprint(\"comprehension \",t3.timeit(number=1000), \"milliseconds\")\nt4 = Timer(\"test4()\", \"from __main__ import test4\")\nprint(\"list range \",t4.timeit(number=1000), \"milliseconds\")\n\n\n```\n\n```\nconcat  1.0196415490549953 milliseconds\nappend  0.08268737967887319 milliseconds\ncomprehension  0.03390247851893946 milliseconds\nlist range  0.013240015113296977 milliseconds\n```\n\n作为一种演示性能差异的方法，我们用 timeit 来做一个实验。我们的目标是验证从列表从末尾 pop 元素和从开始 pop 元b素的性能。同样，我们也想测量不同列表大小对这个时间的影响。我们期望看到的是，从列表末尾处弹出所需时间将保持不变，即使列表不断增长。而从列表开始处弹出元素时间将随列表增长而增加。\n\n\n\n```python\npopzero = Timer(\"x.pop(0)\",\n                \"from __main__ import x\")\npopend = Timer(\"x.pop()\",\n               \"from __main__ import x\")\ndata=[]\nprint(\"pop(0)   pop()\")\nfor i in range(1000000,100000001,1000000):\n    x = list(range(i))\n    pt = popend.timeit(number=1000)\n    x = list(range(i))\n    pz = popzero.timeit(number=1000)\n    print(\"%15.5f, %15.5f\" %(pz,pt))\n    one_data={}\n    one_data['time']=i\n    one_data['pop(0)']=pz\n    one_data['pop()']=pt\n    data.append(one_data)\n    \n```\n\n```\npop(0)   pop()\n        0.56428,         0.00026\n        1.46132,         0.00009\n        2.27953,         0.00007\n        3.11111,         0.00025\n        4.03941,         0.00007\n        5.01496,         0.00011\n        5.72766,         0.00008\n        6.41760,         0.00007\n        6.98101,         0.00016\n        7.69488,         0.00009\n        8.59127,         0.00008\n        9.39142,         0.00008\n       10.03456,         0.00007\n       10.85724,         0.00011\n       11.69828,         0.00008\n       12.37023,         0.00008\n       13.12709,         0.00010\n       13.89727,         0.00008\n       14.65594,         0.00007\n       15.49640,         0.00008\n       16.35027,         0.00008\n       17.00838,         0.00010\n       17.72517,         0.00008\n       19.29465,         0.00010\n       21.02476,         0.00008\n       21.60967,         0.00008\n       22.32767,         0.00008\n       22.62550,         0.00008\n       25.15527,         0.00009\n       24.14118,         0.00008\n       25.48768,         0.00009\n       24.84720,         0.00007\n       26.40644,         0.00008\n       26.62662,         0.00008\n       28.86800,         0.00008\n       27.79692,         0.00008\n\n```\n\n将比较列表和字典之间的 contains 操作的性能。在此过程中，我们将确认列表的 contains 操作符是 O(n)，字典的 contains 操作符是 O(1)。我们将在实验中列出一系列数字。然后随机选择数字，并检查数字是否在列表中。如果我们的性能表是正确的，列表越大，确定列表中是否包含任意一个数字应该花费的时间越长。\n\n```python\nimport timeit\nimport random\ndata=[]\nfor i in range(10000,1000001,20000):\n    t = timeit.Timer(\"random.randrange(%d) in x\"%i,\n                     \"from __main__ import random,x\")\n    x = list(range(i))\n    lst_time = t.timeit(number=1000)\n    x = {j:None for j in range(i)}\n    d_time = t.timeit(number=1000)\n    print(\"%d,%10.3f,%10.3f\" % (i, lst_time, d_time))\n    one_data={}\n    one_data['time']=i\n    one_data['lis_contains']=lst_time\n    one_data['dict_contains']=d_time\n    data.append(one_data)\ndata\n```\n\n```\n10000,     0.075,     0.001\n30000,     0.149,     0.001\n50000,     0.235,     0.001\n70000,     0.353,     0.001\n90000,     0.449,     0.001\n110000,     0.526,     0.001\n130000,     0.614,     0.001\n\n\n```\n\n\n\n\n\n```\n\n```\n\n\n\n```python\nimport pandas as pd \nrow_data=pd.DataFrame.from_dict(data=data)\nrow_data.head()\n```\n\n\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dict_contains</th>\n      <th>lis_contains</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000865</td>\n      <td>0.074577</td>\n      <td>10000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000848</td>\n      <td>0.149025</td>\n      <td>30000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000817</td>\n      <td>0.235177</td>\n      <td>50000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.001056</td>\n      <td>0.352916</td>\n      <td>70000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000893</td>\n      <td>0.448741</td>\n      <td>90000</td>\n    </tr>\n  </tbody>\n</table>\n\n\n\n```python\npd.pivot_table(data=row_data,index='time').plot()\n```\n\n\n\n```\n<matplotlib.axes._subplots.AxesSubplot at 0x1a28403b4a8>\n```\n\n\n\n![png](/image/output_10_1.png)\n\n\n","tags":["算法"]},{"title":"skill_in_python","url":"/2018/10/20/skill-in-python/","content":"# python编程技巧\n\n\n\n## 可变类型和不可变类型\n\nPython提供两种内置或用户定义的类型。可变类型允许内容的内部修改。典型的动态类型 包括列表与字典：列表都有可变方法，如 `list.append()` 和 `list.pop()`， 并且能就地修改。字典也是一样。不可变类型没有修改自身内容的方法。比如，赋值为整数 6的变量 x 并没有 \"自增\" 方法，如果需要计算 x + 1，必须创建另一个整数变量并给其命名。\n\n这种差异导致的一个后果就是，可变类型是不 '稳定 '的，因而不能作为字典的键使用。合理地 使用可变类型与不可变类型有助于阐明代码的意图。例如与列表相似的不可变类型是元组， 创建方式为 `(1, 2)`。元组是不可修改的，并能作为字典的键使用。\n\nPython 中一个可能会让初学者惊讶的特性是：字符串是不可变类型。这意味着当需要组合一个 字符串时，将每一部分放到一个可变列表里，使用字符串时再组合 ('join') 起来的做法更高效。 值得注意的是，使用列表推导的构造方式比在循环中调用 `append()` 来构造列表更好也更快。\n\n差\n\n```\n# 创建将0到19连接起来的字符串 (例 \"012..1819\")\nnums = \"\"\nfor n in range(20):\n    nums += str(n)   # 慢且低效\nprint nums\n```\n\n好\n\n```\n# 创建将0到19连接起来的字符串 (例 \"012..1819\")\nnums = []\nfor n in range(20):\n    nums.append(str(n))\nprint \"\".join(nums)  # 更高效\n```\n\n## 避免对不同类型的对象使用同一个变量名\n\n差\n\n```\na = 1\na = 'a string'\ndef a():\n    pass  # 实现代码\n```\n\n好\n\n```\ncount = 1\nmsg = 'a string'\ndef func():\n    pass  # 实现代码\n```\n\n更好\n\n```\n# 创建将0到19连接起来的字符串 (例 \"012..1819\")\nnums = [str(n) for n in range(20)]\nprint \"\".join(nums)\n```\n\n极佳\n\n```\n# 创建将0到19连接起来的字符串 (例 \"012..1819\")\nnums = map(str, range(20))\nprint \"\".join(nums)\n```\n\n\n\n最后关于字符串的说明的一点是，使用 `join()` 并不总是最好的选择。比如当用预先 确定数量的字符串创建一个新的字符串时，使用加法操作符确实更快，但在上文提到的情况 下或添加到已存在字符串的情况下，使用 `join()` 是更好\n\n```\nfoo = 'foo'\nbar = 'bar'\n\nfoobar = foo + bar  # 好的做法\nfoo += 'ooo'  # 不好的做法, 应该这么做:\nfoo = ''.join([foo, 'ooo'])\n```\n\n除了 [`str.join()`](https://docs.python.org/3/library/stdtypes.html#str.join) 和 `+`，您也可以使用 [%](https://docs.python.org/3/library/string.html#string-formatting) 格式运算符来连接确定数量的字符串，但 [**PEP 3101**](https://www.python.org/dev/peps/pep-3101) 建议使用 [`str.format()`](https://docs.python.org/3/library/stdtypes.html#str.format) 替代 `%` 操作符。\n\n```\nfoo = 'foo'\nbar = 'bar'\n\nfoobar = '%s%s' % (foo, bar) # 可行\nfoobar = '{0}{1}'.format(foo, bar) # 更好\nfoobar = '{foo}{bar}'.format(foo=foo, bar=bar) # 最好\n```\n\n使用简短的函数或方法能降低对不相关对象使用同一个名称的风险。即使是相关的不同 类型的对象，也更建议\n\n使用不同命名：\n\n```\nitems = 'a b c d'  # 首先指向字符串...\nitems = items.split(' ')  # ...变为列表\nitems = set(items)  # ...再变为集合\n```\n\n重复使用命名对效率并没有提升：赋值时无论如何都要创建新的对象。然而随着复杂度的 提升，赋值语句被其他代码包括 'if' 分支和循环分开，使得更难查明指定变量的类型。 在某些代码的做法中，例如函数编程，推荐的是从不重复对同一个变量命名赋值。Java 内的实现方式是使用 'final' 关键字。Python并没有 'final' 关键字而且这与它的哲学 相悖。尽管如此，避免给同一个变量命名重复赋值仍是是个好的做法，并且有助于掌握 可变与不可变类型的概念。\n\n## 单行描述单行代码\n\n每一行一个语句，尤其在复杂的逻辑表达式的时候，这样会清晰很容易阅读。\n\n差\n\n```\nprint \"one\";print \"two\"\nif x == 1;print \"one\"\n```\n\n好\n\n```\nprint \"one\"\nprint \"two\"\nif x == 1:\n\tprint \"one\"\n\t\n```\n\n\n\n## 技巧\n\n使用enumerate() 将列表中的每个项提供两个元素的元组，一个是下标，一个是值。\n\n\n\n```\nfor index , iteme in enumerate(sorted_list):\n\tprint(index,iteme)\n```\n\n交换变量\n\n```\na,b=b,a\n```\n\n## **访问字典元素**\n\n不要使用该dict.has_key()方法。相反使用语法或传递默认参数 比如x in dict ，dict.get(k,default_value)\n\n差\n\n```\nd = {“hello”: \"world\"}\nif d.has_key(\"hello\"):\n\tprint(d['hello'])\nelse:\n\tprint(\"fault values\")\n```\n\n好\n\n\n\n```\nd = {“hello”: \"world\"}\nprint(d.get(\"hello\"),\"fault vluse\")\n\n# or\nif \"hello\" in d:\n\tprint(d[“hello”])\n```\n\n\n","tags":["python"]},{"title":"shadowsocks_config","url":"/2018/10/20/shadowsocks-config/","content":"# 搭建shadowsocks server 和 client端\n\n## sever 端\n\n```\nsudo apt update\nsudo apt install shadowsocks\n```\n\n下载完成后，配置shadowsocks 文件，\n\n```\nvim /etc/shadowsocks/config.json\n```\n\n```\n{\n    \"server\":\"ethan2lee.online\", #you server ip\n    \"server_port\":8888,\n    \"local_address\": \"127.0.0.1\",\n    \"local_port\":1080,\n    \"password\":\"your_password\",\n    \"timeout\":300,\n    \"method\":\"aes-256-cfb\"\n}\n```\n\n一般来说，我们只需要更改“server”和“password”字段的值即可，根据自己实际情况配置。更改完后，保存。\n\n接下来就是启动server端了，先设置开机自启，再启动。\n\n```\n➜  ~ sudo systemctl enable  shadowsocks.service\n\nshadowsocks.service is not a native service, redirecting to systemd-sysv-install\nExecuting /lib/systemd/systemd-sysv-install enable shadowsocks\n```\n\n\n\n```\n➜  ~ sudo systemctl start   shadowsocks.service\n```\n\n\n\n查看服务状态：\n\n```\n➜  ~ sudo systemctl status  shadowsocks.service\n● shadowsocks.service - LSB: Fast tunnel proxy that helps you bypass firewalls\n   Loaded: loaded (/etc/init.d/shadowsocks; bad; vendor preset: enabled)\n   Active: active (exited) since Sat 2018-10-20 04:59:16 UTC; 4s ago\n​     Docs: man:systemd-sysv-generator(8)\n  Process: 634 ExecStart=/etc/init.d/shadowsocks start (code=exited, status=0/SUCCESS)\n​    Tasks: 0\n   Memory: 0B\n​      CPU: 0\n\nOct 20 04:59:16 ethan systemd[1]: Starting LSB: Fast tunnel proxy that helps you bypass firewalls...\nOct 20 04:59:16 ethan systemd[1]: Started LSB: Fast tunnel proxy that helps you bypass firewalls.\n```\n\n这样我们shadowsocks的server端就配置完成了。\n\n## client端\n\n在需要用shadowsocks的server服务的机子也安装shaodowsocks。\n\n```\nsudo apt update\nsudo apt install shadowsocks\n```\n\n\n\n\n\n同样需要配置一下shadowsocks的配置文件，一般跟server端的配置差不多，但是需要自行更改“local_port”的字段值。\n\n```\nvim /etc/shadowsocks/config.json\n```\n\n\n\n```\n{\n    \"server\":\"ethan2lee.online\", #you server ip\n    \"server_port\":8888,\n    \"local_address\": \"127.0.0.1\",\n    \"local_port\":1080, ### 在clien端可以自主配置的\n    \"password\":\"your_password\",\n    \"timeout\":300,\n    \"method\":\"aes-256-cfb\"\n}\n```\n\n配置完成之手进行保存。\n\n开启client 端，这样我们只要使用sock5协议通过代理端口1080访问网络，就可以完全使用server的网络环境了。\n\n```\n➜  ~ sslocal -c /etc/shadowsocks/config.json\nINFO: loading config from /etc/shadowsocks/config.json\nshadowsocks 2.1.0\n2018-10-20 05:10:08 INFO     starting local at 127.0.0.1:1080\n```\n\n打开之后，shadowsocks的client 端是运行在前台的。我们先按Ctr +c 终止进行，再输入以下命令，让它在后台运行：\n\n```\n➜  ~ nohup sslocal -c /etc/shadowsocks/config.json &\n[1] 9479\nnohup: ignoring input and appending output to 'nohup.out'\n```\n\n这样它就作为守护进程在运行着了。\n\n## proxychains\n\n安装完shadowsocks之后，如果想让终端命令也经常代理，那么我们就需要proxychains来帮忙了。\n\n安装proxychains\n\n```\nsudo apt install proxychains\n```\n\n修改proxychains 配置文件，直接找到最后一行。\n\n```\n➜  ~ vim /etc/proxychains.conf\n```\n\n将`socks4 127.0.0.1 9095`改为\n\n```\nsocks5 127.0.0.1 1080 \n```\n\n1080 为你刚刚配置shadowsocks client端的“local_host”字段的值，配置完成后保存。\n\n\n\n\n\n经过以上所有步骤，我们可以通过一下命令来查看我们是否配置正确了：\n\n\n\n```\nproxychains wget www.google.com\n```\n\n如果有一下输出，那么就证明你配置成功了：\n\n```\n➜  ~ proxychains wget www.google.com\nProxyChains-3.1 (http://proxychains.sf.net)\n--2018-10-20 05:28:01--  http://www.google.com/\nResolving www.google.com (www.google.com)... |DNS-request| www.google.com\n|S-chain|-<>-127.0.0.1:1080-<><>-4.2.2.2:53-<><>-OK\n|DNS-response| www.google.com is 74.125.20.103\n74.125.20.103\nConnecting to www.google.com (www.google.com)|74.125.20.103|:80... |S-chain|-<>-127.0.0.1:1080-<><>-74.125.20.103:80-<><>-OK\nconnected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: ‘index.html’\n\nindex.html                        [ <=>                                           ]  11.05K  --.-KB/s    in 0s\n\n2018-10-20 05:28:02 (63.6 MB/s) - ‘index.html’ saved [11319]\n\n```\n\n\n\n如果没有，就只能继续百度了。\n","tags":["vpn"]},{"title":"更换ubuntu18源","url":"/2018/10/20/更换ubuntu18源/","content":"# Ubuntu 18.04换国内源 \n\n\n\n更换源前，先对本来的源文件做好备份。\n\n```\nmv /etc/apt/sources.list  /etc/apt/sourceslist-save\n```\n\n备份完成后，我们就可再建立source.list\n\n```\nvim /etc/apt/sources.list \n```\n\n添加以下内容\n\n```\n##中科大源\n\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multivers\n```\n\n然后执行以下命令：\n\n```\nsudo apt update\nsudo apt upgrade\n```\n\n\n\n\n\n更改其他源：\n\n阿里源\n\n```\ndeb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\n```\n\n360源\n\n```\ndeb http://mirrors.163.com/ubuntu/ bionic main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiverse\n```\n\n清华源\n\n```\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\n```\n\n\n","tags":["linux"]},{"title":"统计量分析示例","url":"/2018/10/19/统计量分析示例/","content":"\n# \n\n# 描述性统计分析基础\n\n- 数据集描述与属性说明\n\n- ID 客户编号\n\n- Suc_flag   成功入网标识\n\n- ARPU   入网后ARPU\n\n- PromCnt12  12个月内的营销次数\n\n- PromCnt36  36个月内的营销次数\n\n- PromCntMsg12   12个月内发短信的次数\n\n- PromCntMsg36   36个月内发短信的次数\n\n- Class  客户重要性等级(根据前运营商消费情况)\n\n- Age    年龄\n\n- Gender 性别\n\n- HomeOwner  是否拥有住房\n\n- AvgARPU    当地平均ARPU\n\n- AvgHomeValue   当地房屋均价\n\n- AvgIncome  当地人均收入\n\n```\nimport os\nimport pandas as pd\nimport numpy as np\n\npd.set_option('display.max_columns', None)\n#os.chdir('Q:/data')\n#os.getcwd()\n```\n\n读取数据\n\n```\ncamp= pd.read_csv('teleco_camp.csv')\ncamp.head(10)\n```\n\n数据预处理\n\n```\ncamp.dtypes\n```\n\n```\nID                 int64\nSuc_flag        category\nARPU             float64\nPromCnt12        float64\nPromCnt36        float64\nPromCntMsg12     float64\nPromCntMsg36     float64\nClass           category\nAge              float64\nGender            object\nHomeOwner         object\nAvgARPU          float64\nAvgHomeValue     float64\nAvgIncome        float64\ndtype: object\n```\n\ncamp.describe(include='all')\n\n|        | ID            | Suc_flag | ARPU        | PromCnt12   | PromCnt36   | PromCntMsg12 | PromCntMsg36 | Class  | Age         | Gender | HomeOwner | AvgARPU     | AvgHomeValue  | AvgIncome     |\n| ------ | ------------- | -------- | ----------- | ----------- | ----------- | ------------ | ------------ | ------ | ----------- | ------ | --------- | ----------- | ------------- | ------------- |\n| count  | 9686.000000   | 9686.0   | 4843.000000 | 9686.000000 | 9686.000000 | 9686.000000  | 9686.000000  | 9686.0 | 7279.000000 | 9686   | 9686      | 9686.000000 | 9583.000000   | 7329.000000   |\n| unique | NaN           | 2.0      | NaN         | NaN         | NaN         | NaN          | NaN          | 4.0    | NaN         | 3      | 2         | NaN         | NaN           | NaN           |\n| top    | NaN           | 1.0      | NaN         | NaN         | NaN         | NaN          | NaN          | 2.0    | NaN         | F      | H         | NaN         | NaN           | NaN           |\n| freq   | NaN           | 4843.0   | NaN         | NaN         | NaN         | NaN          | NaN          | 3303.0 | NaN         | 5223   | 5377      | NaN         | NaN           | NaN           |\n| mean   | 97975.474086  | NaN      | 78.121722   | 3.447212    | 7.337059    | 1.178402     | 2.390935     | NaN    | 59.150845   | NaN    | NaN       | 52.905156   | 112179.202755 | 53513.457361  |\n| std    | 56550.171120  | NaN      | 62.225686   | 1.231890    | 1.952436    | 0.287226     | 0.914314     | NaN    | 16.516400   | NaN    | NaN       | 4.993775    | 98522.888583  | 19805.168339  |\n| min    | 12.000000     | NaN      | 5.000000    | 0.750000    | 1.000000    | 0.200000     | 0.400000     | NaN    | 0.000000    | NaN    | NaN       | 46.138968   | 7500.000000   | 2499.000000   |\n| 25%    | 48835.500000  | NaN      | 50.000000   | 2.900000    | 6.250000    | 1.000000     | 1.400000     | NaN    | 47.000000   | NaN    | NaN       | 49.760116   | 53200.000000  | 40389.000000  |\n| 50%    | 99106.000000  | NaN      | 65.000000   | 3.250000    | 7.750000    | 1.200000     | 2.600000     | NaN    | 60.000000   | NaN    | NaN       | 50.876672   | 77700.000000  | 48699.000000  |\n| 75%    | 148538.750000 | NaN      | 100.000000  | 3.650000    | 8.250000    | 1.400000     | 3.200000     | NaN    | 73.000000   | NaN    | NaN       | 54.452822   | 129350.000000 | 62385.000000  |\n| max    | 191779.000000 | NaN      | 1000.000000 | 15.150000   | 19.500000   | 3.600000     | 5.600000     | NaN    | 87.000000   | NaN    | NaN       | 99.444787   | 600000.000000 | 200001.000000 |\n\n\n\n\n\n\n# 描述性统计与探索型数据分析\n\n## 分类变量分析\n\n可以查看列原因元素的种类\n\n```\ncamp['Suc_flag'].groupby(camp['Suc_flag']).count()\n```\n\n```\nSuc_flag\n0    4843\n1    4843\nName: Suc_flag, dtype: int64\n```\n\n\n\n\n\n## 连续变量分析\n### 数据的集中趋势\n#### ARPU的均值与中位数\n```\n\nfs = camp['ARPU'] # 可以使用camp.ARPU \nprint('mean = %6.4f' %fs.mean())                     # 求fs的均值\nprint('median = %6.4f' %fs.median() )                # 求fs的中位数\nprint('quantiles\\n', fs.quantile([0.25, 0.5, 0.75])) # 求a的上下四分位数与中位数\n```\n\n```\nmad = 38.1896\nrange = 995.0000\nvar = 3872.0359\nstd = 62.2257\n```\n\n\n\n```\nget_ipython().run_line_magic('matplotlib', 'inline'）\n\nfs.plot(kind='hist')\n```\n\n```\n(array([2.510e+03, 1.978e+03, 2.160e+02, 9.800e+01, 4.000e+00, 7.000e+00,\n        0.000e+00, 2.500e+01, 0.000e+00, 0.000e+00, 0.000e+00, 1.000e+00,\n        0.000e+00, 0.000e+00, 4.000e+00]),\n array([   5.        ,   71.33333333,  137.66666667,  204.        ,\n         270.33333333,  336.66666667,  403.        ,  469.33333333,\n         535.66666667,  602.        ,  668.33333333,  734.66666667,\n         801.        ,  867.33333333,  933.66666667, 1000.        ]),\n <a list of 15 Patch objects>)\n```\n\n![](image/output_11_1.png)\n\n\n### 数据的离散程度\n\n```\nprint ('mad = %6.4f' %fs.mad())      # 求平均绝对偏差 mad = np.abs(fs - fs.mean()).mean()\nprint ('range = %6.4f' %(fs.max(skipna=True) - fs.min(skipna=True))) # 求极差\nprint ('var = %6.4f' %fs.var())   # 求方差\nprint ('std = %6.4f' %fs.std())   # 求标准差\n```\n\n\n\n\n\n### 数据的偏度与峰度\n```\nimport matplotlib.pyplot as plt\n\nplt.hist(fs.dropna(), bins=15)\n```\n\n![](image/output_15_1.png)\n\n```\nprint ('skewness = %6.4f' %fs.skew(skipna=True))\nprint ('kurtosis = %6.4f' %fs.kurt(skipna=True))\n```\n\n```\nskewness = 5.1695\nkurtosis = 52.8509\n```\n\n\n### apply\\map\\groupby及其它相关\n\n```\ndata = pd.DataFrame(data={'a':range(1,11), 'b':np.random.randn(10)})\ndata.T\n```\n\n|      | 0         | 1        | 2        | 3        | 4        | 5         | 6        | 7       | 8        | 9         |\n| ---- | --------- | -------- | -------- | -------- | -------- | --------- | -------- | ------- | -------- | --------- |\n| a    | 1.000000  | 2.000000 | 3.000000 | 4.000000 | 5.000000 | 6.000000  | 7.000000 | 8.0000  | 9.000000 | 10.000000 |\n| b    | -0.087919 | 0.903531 | 0.603965 | 0.203005 | 0.282077 | -1.420298 | 0.283303 | -0.0565 | 1.047595 | -0.787566 |\n\n```\ndata.apply(np.mean) # 等价于data.mean()，是其完整形式\n```\n\n```\na    5.500000\nb    0.097119\ndtype: float64\n```\n\n```\ndata.apply(lambda x: x.astype('str')).dtypes # DataFrame没有astype方法，只有Series有\n```\n","tags":["数据分析"]},{"title":"data_clearning","url":"/2018/10/17/data-clearning/","content":"# 数据清洗\n\n> 数据清洗， 是整个数据分析过程中不可缺少的一个环节，其结果质量直接关系到模型效果和最终结论。在实际操作中，数据清洗通常会占据分析过程的50%—80%的时间。\n\n## 去除重复数据\n\n在获取到的数据中，我们发现会有重复数据。我们使用Pandas 提供的方法 duplicated 和 drop_duplicates来去重。\n\n\n\n```\nsample =  pd.DataFrame({'id':[1,1,1,3,4,5],\n                        'name':['Bob','Bob','Mark','Miki','Sully','Rose'],\n                        'score':[99,99,87,77,77,np.nan],\n                        'group':[1,1,1,2,1,2]})\nsample\nOut[85]: \n   id   name  score  group\n0   1    Bob   99.0      1\n1   1    Bob   99.0      1\n2   1   Mark   87.0      1\n3   3   Miki   77.0      2\n4   4  Sully   77.0      1\n5   5   Rose    NaN      2\n\n```\n\n查找重复数据：\n\n```\nsample[sample.duplicated()]\nOut[86]: \n   id name  score  group\n1   1  Bob   99.0      1\n```\n\n需要去重时：\n\n```\nsample.drop_duplicates()\nOut[87]: \n   id   name  score  group\n0   1    Bob   99.0      1\n2   1   Mark   87.0      1\n3   3   Miki   77.0      2\n4   4  Sully   77.0      1\n5   5   Rose    NaN      2\n```\n\n按列去重时，需要加入列索引：\n\n```\nsample.drop_duplicates('id')\nOut[88]: \n   id   name  score  group\n0   1    Bob   99.0      1\n3   3   Miki   77.0      2\n4   4  Sully   77.0      1\n5   5   Rose    NaN      2\n```\n\n## 缺失值处理\n\n在数据挖掘中，面对数据中存在缺失值时，我们一般采取以下几种办法：\n\n1. 缺失值较多的特征处理\n\n当缺失值处于20%~80%，每个缺失值可以生成一个指示哑变量，参与后续的建模。\n\n2.缺失较少时\n\n首先需要根据业务理解处理缺失值，弄清楚缺失值产生的原因，是故意缺失还是随机缺失。可以依靠业务经验进行填补。连续变量可以使用均值或中位数进行填补。\n\n- 把 NaN 直接作为一个特征，假设0表示\n\n```\ndf.fillna(0) \n```\n\n\n\n- 用均值填充\n\n```\n# 将所有行用各自的均值填充 \ndf.fillna(df.mean())  \n# 将所有行用各自的均值填充 \ndf.fillna(df.mean()['collum_name])\n```\n\n\n\n- 用上下数据填充\n\n```\n# 用前一个数据替代NaN\ndf.fillnan(method=\"pad\")\n\n# 与pad相反，bfill表示用后一个数据代替NaN\ndf.fillna(method=)\n```\n\n\n\n- 用插值填充\n\n```\n# 插值法就是通过两点（x0,y0）,(x1,y1)估计中间点的值\ndf.interpolate() \n```\n\n\n\n\n\n- 查看数据值缺失情况，我们可有构造一个lambda 函数来查看缺失值。\n\n```\nsample =  pd.DataFrame({'id':[1,1,1,3,4,np.nan],\n                        'name':['Bob','Bob','Mark','Miki','Sully',np.nan],\n                        'score':[99,99,87,77,77,np.nan],\n                        'group':[1,1,1,2,1,np.nan]})\nsample\nsample.apply(lambda col:sum(col.isnull())/col.size)\nOut[91]: \nid       0.166667\nname     0.166667\nscore    0.166667\ngroup    0.166667\ndtype: float64\n```\n\n### 已指定值填补\n\n均值\n\n```\nsample.score.fillna(sample.score.mean())\nOut[93]: \n0    99.0\n1    99.0\n2    87.0\n3    77.0\n4    77.0\n5    87.8\nName: score, dtype: float64\n```\n\n中位数\n\n```\nsample.score.fillna(sample.score.median())\nOut[94]: \n0    99.0\n1    99.0\n2    87.0\n3    77.0\n4    77.0\n5    87.0\nName: score, dtype: float64\n```\n\n### 缺失值指示变量\n\nPanda DataFrame 对象可以直接调用方法isnull 产生缺失值指示变量，例如产生score变量的缺失值变量：\n\n```\n# 指示变量\nsample.score.isnull()\nOut[95]: \n0    False\n1    False\n2    False\n3    False\n4    False\n5     True\nName: score, dtype: bool\n```\n\n若想转化为数据0，1型指示变量，可以使用apply方法，int表示将该列替换为 int 类型：\n\n```\nsample.score.isnull().apply(int)\nOut[97]: \n0    0\n1    0\n2    0\n3    0\n4    0\n5    1\n```\n\n\n\n\n\n## 噪声处理\n\n噪声值是指数据中有一个或多个数据与其他数值相比差异性比较大，又称异常值，离群值。\n\n对于单变量，我们可以采用盖帽法，分箱法；\n\n对于多变量，我们可以采用就聚类。\n\n### 盖帽法\n\n盖帽法将连续变量均值上下三倍标准差范围外的记录替换为均值上下三倍标准差值。\n\n```\n\ndef cap(df,quantile=[0.01,0.99]):\n    \"\"\"\n    盖帽法处理异常值\n    :param df: pd.Series 列，连续变量\n    :param quantile: 指定盖帽法的上下分位数范围\n    :return: \n    \"\"\"\n    Q01 ,Q99 = df.quantile(quantile).values.tolist() # 生成分位数\n    # 替代异常值\n    if Q01 > df.min():\n        x = df.copy()\n        x.loc[x<Q01] = Q01\n    if Q99 < df.max():\n        x = df.copy()\n        x.loc[x > Q99] = Q99\n    return(x)\n```\n\n生成一组服从正态分布的随机数，sample.hsit 为直方图。\n\n```\nimport matplotlib.pyplot as plt\nsample = pd.DataFrame({'normall':np.random.randn(1000)})\nsample.hist(bins =50)\nplt.show()\n```\n\n![处理前](\\image\\before_handle.png)\n\n\n\n对 sample 数据所有列进行盖帽法转换，，下图可以看出盖帽后极端值频数的变化。\n\n```\nnew =sample.apply(cap,quantile=[0.001,0.99])\nnew.hist(bins=50)\nplt.show()\n```\n\n![](\\image\\after_handle.png)\n\n\n\n\n\n### 分箱法\n\n分箱法通过考察数据的\"近邻\"来光滑有序数据的值。有序值分布到一些桶或箱中。\n\n深分箱，即每个分箱中的样本量一致；\n\n等宽分箱，即每个分箱中的取值范围一致。直方图就是首先对数据进行了等宽分箱，再计算频数画图。\n\n分箱法可以将异常数据包含再箱子中，在进行建模的时候，不直接进行到模型中，因而可以达到处理异常值的目的。\n\n```\n# 生成10个标准正态分布的随机数\nsample = pd.DataFrame({\"normal\":np.random.randn(10)})\nsample\nOut[118]: \n     normal\n0 -0.028929\n1  0.327508\n2 -0.596384\n3 -2.036334\n4  1.452605\n5 -0.403936\n6  0.315138\n7  0.252127\n8 -0.775113\n9  0.171641\n```\n\n#### 等宽分箱\n\n现将sample 按照宽度分位5份，下限中，cut 函数自动选择小于列最小值的一个数值未下限，最大值为上限，等分为5份。\n\n```\npd.cut(sample.normal,5)\nOut[119]: \n0     (-0.641, 0.057]\n1      (0.057, 0.755]\n2     (-0.641, 0.057]\n3     (-2.04, -1.339]\n4      (0.755, 1.453]\n5     (-0.641, 0.057]\n6      (0.057, 0.755]\n7      (0.057, 0.755]\n8    (-1.339, -0.641]\n9      (0.057, 0.755]\nName: normal, dtype: category\nCategories (5, interval[float64]): [(-2.04, -1.339] < (-1.339, -0.641] < (-0.641, 0.057] <\n                                    (0.057, 0.755] < (0.755, 1.453]]\n\n```\n\n使用labels参数指定分箱后的各个水平的标签，\n\n```\npd.cut(sample.normal,bins=5,labels=[1,2,3,4,5])\nOut[120]: \n0    3\n1    4\n2    3\n3    1\n4    5\n5    3\n6    4\n7    4\n8    2\n9    4\nName: normal, dtype: category\nCategories (5, int64): [1 < 2 < 3 < 4 < 5]\n```\n\n### 等深分箱\n\n等深分箱中，各个箱的宽度可能不一，但频数是几乎相等，所以可以采用数据的分位数来分箱。现对sample数据进行等深度分二箱，首先要找到2箱的分位数：\n\n```\nsample.normal.quantile([0,0.5,1])\nOut[121]: \n0.0   -2.036334\n0.5    0.071356\n1.0    1.452605\nName: normal, dtype: float64\n```\n\n在bins参数中设定分位数区间，为将下边界包含，include_lowest= True，完成等深分箱：\n\n```\npd.cut(sample.normal,bins=sample.normal.quantile([0,0.5,1]),include_lowest=True)\nOut[124]: \n0    (-2.037, 0.0714]\n1     (0.0714, 1.453]\n2    (-2.037, 0.0714]\n3    (-2.037, 0.0714]\n4     (0.0714, 1.453]\n5    (-2.037, 0.0714]\n6     (0.0714, 1.453]\n7     (0.0714, 1.453]\n8    (-2.037, 0.0714]\n9     (0.0714, 1.453]\nName: normal, dtype: category\nCategories (2, interval[float64]): [(-2.037, 0.0714] < (0.0714, 1.453]]\n```\n\n可以对分组进行标签化。\n\n```\npd.cut(sample.normal,bins=sample.normal.quantile([0,0.5,1]),include_lowest=True,labels=['bad','good'])\nOut[125]: \n0     bad\n1    good\n2     bad\n3     bad\n4    good\n5     bad\n6    good\n7    good\n8     bad\n9    good\nName: normal, dtype: category\nCategories (2, object): [bad < good]\n\n```\n\n\n\n\n\n### 多变量异常值处理-聚类法\n\n通过快速聚类法将数据对象分组成为多个簇，在同一个簇中的对象具有较高的相似度，而不同簇之间的对象差别较大。聚类分析可以挖掘孤立点以发现噪声数据，因为噪声本身就是孤立点。\n","tags":["数据清洗"],"categories":["数据挖掘"]},{"title":"RFM模型分析用户行为","url":"/2018/10/17/RFM模型分析用户行为/","content":"# RFM模型分析用户行为\n\n根据美国数据库营销研究所Arthur Hughes的研究，客户数据库中有三个神奇的要素，这三个要素构成了数据分析最好的指标：最近一次消费(Recency)、消费频率(Frequency)、消费金额(Monetary)。\n\nRFM模型：\n\n- R(Recency)表示客户最近一次购买的时间有多远，对消费时间越近的客户，提供即时的商品或服务也最有可能有所反应。\n\n- F(Frequency)表示客户在最近一段时间内购买的次数，经常买的客户也是满意度最高的客户。\n\n- M  (Monetary)表示客户在最近一段时间内购买的金额，消费金额是最近消费的平均金额，是体现客户短期价值的中重要变量。如果预算不多，那么我们酒的将服务信息提供给收入贡献较高的那些人。\n\n一般原始数据为3个字段：客户ID、购买时间（日期格式）、购买金额，用数据挖掘软件处理，加权（考虑权重）得到RFM得分，进而可以进行客户细分，客户等级分类，Customer Level Value得分排序等，实现数据库营销！\n\n![](\\image\\RFM.png)\n\n\n\n（编号次序RFM,1代表高，0代表低）\n\n重要价值客户（111）：最近消费时间近、消费频次和消费金额都很高，必须是VIP啊！\n\n重要保持客户（011）：最近消费时间较远，但消费频次和金额都很高，说明这是个一段时间没来的忠实客户，我们需要主动和他保持联系。\n\n重要发展客户（101）：最近消费时间较近、消费金额高，但频次不高，忠诚度不高，很有潜力的用户，必须重点发展。\n\n重要挽留客户（001）：最近消费时间较远、消费频次不高，但消费金额高的用户，可能是将要流失或者已经要流失的用户，应当基于挽留措施。\n\nRFM模型的应用在于建立一个用户行为报告，这个报告会成为维系顾客的一个重要指标。\n\n\n\n现在我们以某淘宝店家做客户激活为案例，[RFM_TRAD_FLOW.csv](/data/RFM_TRAD_FLOW.csv) 为某段时间内客户消费记录\n\n```\nimport pandas as pd\ntrad_flow = pd.read_csv('data/RFM_TRAD_FLOW.csv', encoding='gbk')\ntrad_flow.head(10)\n```\n\n数据部分展示：\n\n| transID | cumid | time | amount | type_label | type |\n| ------- | ----- | ---- | ------ | ---------- | ---- |\n|9407|\t10001|\t14JUN09:17:58:34|\t199\t|正常|\tNormal|\n|9625\t|10001\t|16JUN09:15:09:13\t|369\t|正常\t|Normal|\n|11837\t|10001|\t01JUL09:14:50:36\t|369\t|正常|\tNormal|\n|26629\t|10001\t|14DEC09:18:05:32\t|359\t|正常\t|Normal|\n|30850|\t10001\t|12APR10:13:02:20|\t399|\t正常|\tNormal|\n|32007\t|10001|\t04MAY10:16:45:58|\t269\t|正常|\tNormal|\n|36637\t|10001\t|04JUN10:20:03:06|\t0\t|赠送|\tPresented|\n|43108\t|10001|\t06JUL10:16:56:40|\t381\t|正常|\tNormal|\n\n1. 计算F 反应顾客对打折的偏好程度\n\n通过计算F反应客户对打折产品的偏好\n\n```\nF=trad_flow.groupby(['cumid','type'])[['transID']].count()\nF.head()\n```\n\n建立数据透视表\n\n```\nF_trans=pd.pivot_table(F,index='cumid',columns='type',values='transID')\nF_trans.head()\n```\n\n对缺失的数据填补为零\n\n```\nF_trans['Special_offer']= F_trans['Special_offer'].fillna(0)\nF_trans.head()\n```\n\n计算兴趣用户比例\n\n```\nF_trans[\"interest\"]=F_trans['Special_offer']/(F_trans['Special_offer']+F_trans['Normal'])\nF_trans.head()\n```\n\n2. 计算M反应客户的价值信息\n\n通过计算M反应客户的价值信息\n\n```\nM=trad_flow.groupby(['cumid','type'])[['amount']].sum()\nM.head()\n```\n\n数据透视，缺失值补零，计算价值用户\n\n```\nM_trans=pd.pivot_table(M,index='cumid',columns='type',values='amount')\nM_trans['Special_offer']= M_trans['Special_offer'].fillna(0)\nM_trans['returned_goods']= M_trans['returned_goods'].fillna(0)\nM_trans[\"value\"]=M_trans['Normal']+M_trans['Special_offer']+M_trans['returned_goods']\nM_trans.head()\n```\n\n3. 通过计算R反应客户是否为沉默客户\n\n定义一个从文本转化为时间的函数\n\n```\nfrom datetime import datetime\nimport time\ndef to_time(t):\n    out_t=time.mktime(time.strptime(t, '%d%b%y:%H:%M:%S'))  \n    return out_t\t\t\n```\n\n将时间进行转化\n\n```\nrad_flow[\"time_new\"]= trad_flow.time.apply(to_time)\n```\n\n获取高频消费客户\n\n```\nR=trad_flow.groupby(['cumid'])[['time_new']].max()\nR.head()\n```\n\n4. 构建模型，筛选目标客户\n\n```\n# In[12]\nfrom sklearn import preprocessing\nthreshold = pd.qcut(F_trans['interest'], 2, retbins=True)[1][1]\nbinarizer = preprocessing.Binarizer(threshold=threshold)\ninterest_q = pd.DataFrame(binarizer.transform(F_trans['interest'].values.reshape(-1, 1)))\ninterest_q.index=F_trans.index\ninterest_q.columns=[\"interest\"]\n# In[12]\nthreshold = pd.qcut(M_trans['value'], 2, retbins=True)[1][1]\nbinarizer = preprocessing.Binarizer(threshold=threshold)\nvalue_q = pd.DataFrame(binarizer.transform(M_trans['value'].values.reshape(-1, 1)))\nvalue_q.index=M_trans.index\nvalue_q.columns=[\"value\"]\n# In[12]\nthreshold = pd.qcut(R[\"time_new\"], 2, retbins=True)[1][1]\nbinarizer = preprocessing.Binarizer(threshold=threshold)\ntime_new_q = pd.DataFrame(binarizer.transform(R[\"time_new\"].values.reshape(-1, 1)))\ntime_new_q.index=R.index\ntime_new_q.columns=[\"time\"]\n# In[12]\nanalysis=pd.concat([interest_q, value_q,time_new_q], axis=1)\n# In[12]\n#analysis['rank']=analysis.interest_q+analysis.interest_q\nanalysis = analysis[['interest','value','time']]\nanalysis.head()\n\nlabel = {\n    (0,0,0):'无兴趣-低价值-沉默',\n    (1,0,0):'有兴趣-低价值-沉默',\n    (1,0,1):'有兴趣-低价值-活跃',\n    (0,0,1):'无兴趣-低价值-活跃',\n    (0,1,0):'无兴趣-高价值-沉默',\n    (1,1,0):'有兴趣-高价值-沉默',\n    (1,1,1):'有兴趣-高价值-活跃',\n    (0,1,1):'无兴趣-高价值-活跃'\n}\nanalysis['label'] = analysis[['interest','value','time']].apply(lambda x: label[(x[0],x[1],x[2])], axis = 1)\nanalysis.head()\n```\n\n\n"},{"title":"markdown_picture","url":"/2018/10/16/markdown-picture/","content":"# 序列图\n\n``` sequence\ntitle: 序列图sequence(示例)\nparticipant A\nparticipant B\nparticipant C\n\nnote left of A: A左侧说明\nnote over B: 覆盖B的说明\nnote right of C: C右侧说明\n\nA->A:自己到自己\nA->B:实线实箭头\nA-->C:虚线实箭头\nB->>C:实线虚箭头\nB-->>A:虚线虚箭头\t\t\t\t\t\n```\n\n关键词:\n\n1. title, 定义该序列图的标题\n2. participant, 定义时序图中的对象\n3. note, 定义对时序图中的部分说明\n4. {actor}, 表示时序图中的具体对象（名称自定义）\n\n针对note的方位控制主要包含以下几种关键词：\n\n1. left of, 表示当前对象的左侧\n2. right of, 表示当前对象的右侧\n3. over, 表示覆盖在当前对象（们）的上面\n\n针对{actor}的箭头分为以下几种：\n\n1. -> 表示实线实箭头\n2. –> 表示虚线实箭头\n3. ->> 表示实线虚箭头\n4. –>> 表示虚线虚箭头\n\n# 流程图\n\n```flow\nst=>start: 开始\ne=>end: 结束\nop=>operation: 操作\nsub=>subroutine: 子程序\ncond=>condition: 是或者不是?\nio=>inputoutput: 输出\n\nst(right)->op->cond\ncond(yes)->io(right)->e\ncond(no)->sub(right)->op\n```\n\n- start,end, 表示程序的开始与结束\n- operation, 表示程序的处理块\n- subroutine, 表示子程序块\n- condition, 表示程序的条件判断\n- inputoutput, 表示程序的出入输出\n- right,left, 表示箭头在当前模块上的起点(默认箭头从下端开始)\n- yes,no, 表示condition判断的分支(其可以和right,left同时使用)\n\n模块定义(模块标识与模块名称可以任意定义名称,关键词不可随意取名)如下:\n\n```\n模块标识=>模块关键词: 模块名称\n```\n\n连接定义如下:\n\n```\n模块标识1->模块标识2\n模块标识1->模块标识2->模块标识3\n... ...\n```\n\n```flow\nst=>start: Start\ne=>end: End\nop1=>operation: My Operation\nop2=>operation: Stuff\nsub1=>subroutine: My Subroutine\ncond=>condition: Yes\nor No?\nc2=>condition: Good idea\nio=>inputoutput: catch something...\n\nst->op1(right)->cond\ncond(yes, right)->c2\ncond(no)->sub1(left)->op1\nc2(yes)->io->e\nc2(no)->op2->e\n```\n\n![Alt text](https://g.gravizo.com/svg?\n  digraph G {\n​    aize =\"4,4\";\n​    main [shape=box];\n​    main -> parse [weight=8];\n​    parse -> execute;\n​    main -> init [style=dotted];\n​    main -> cleanup;\n​    execute -> { make_string; printf}\n​    init -> make_string;\n​    edge [color=red];\n​    main -> printf [style=bold,label=\"100 times\"];\n​    make_string [label=\"make a string\"];\n​    node [shape=box,style=filled,color=\".7 .3 1.0\"];\n​    execute -> compare;\n  }\n)\n","tags":["markdown"]},{"title":"data_integration","url":"/2018/10/16/data-integration/","content":"# 数据整合\n\n## 行列操作\n\n```python\nimport pandas as pd\nimport numpy as np\nsample = pd.DataFrame(np.random.randn(4,5),columns=['a','b','c','d','e'])\nsample\n```\n\n```\nOut[2]: \n          a         b         c         d         e\n0 -0.776807  2.355071 -0.940921  0.164487 -1.025772\n1  0.596704  0.962625  1.848441 -1.122676 -0.359290\n2 -0.092755 -0.124250 -0.259899 -0.111997 -1.816197\n3  0.372941  0.297850 -0.409256  0.485376 -2.790929\n```\n\n### 选择单列\n\n```\nsample['a']\n```\n\n\n\n```\nOut[4]: \n0   -0.776807\n1    0.596704\n2   -0.092755\n3    0.372941\nName: a, dtype: float64\n```\n\n数据框的ix，iloc，ioc方法都可以选择行，列，iloc方法只能使用数值作为索引来选择行列，loc方法在选择时能使用字符串索引，ix方法则可以使用两种索引\n\n```\nsample.ix[:,'a']\n```\n\n```\nOut[5]: \n0   -0.776807\n1    0.596704\n2   -0.092755\n3    0.372941\nName: a, dtype: float64\n```\n\n或者单选列\n\n```\nsample[['a']]\nOut[6]: \n          a\n0 -0.776807\n1  0.596704\n2 -0.092755\n3  0.372941\n```\n\n\n\n### 选择多行多列\n```\nsample.ix[0:2,0:2]\n```\n\n```\nOut[7]: \n          a         b\n0 -0.776807  2.355071\n1  0.596704  0.962625\n2 -0.092755 -0.124250\n\n```\n\n```\nsample.iloc[0:2,0:2]\n\n```\n\n### 创建，删除列\n\n第一种方式\n\n```\nsample['new_col1']=sample['a']-sample['b']\nsample\n```\n\n```\nOut[10]: \n          a         b         c         d         e  new_col1\n0 -0.776807  2.355071 -0.940921  0.164487 -1.025772 -3.131877\n1  0.596704  0.962625  1.848441 -1.122676 -0.359290 -0.365921\n2 -0.092755 -0.124250 -0.259899 -0.111997 -1.816197  0.031495\n3  0.372941  0.297850 -0.409256  0.485376 -2.790929  0.075091\n```\n\n第二种方式\n\n```\nsample.assign(new_col2=sample['a']-sample['b'],new_col3=sample['a']+sample['b'])\n```\n\n```\nOut[11]: \n          a         b         c    ...     new_col1  new_col2  new_col3\n0 -0.776807  2.355071 -0.940921    ...    -3.131877 -3.131877  1.578264\n1  0.596704  0.962625  1.848441    ...    -0.365921 -0.365921  1.559329\n2 -0.092755 -0.124250 -0.259899    ...     0.031495  0.031495 -0.217004\n3  0.372941  0.297850 -0.409256    ...     0.075091  0.075091  0.670792\n[4 rows x 8 columns]\n```\n\n删除列，第一种方式\n\n```\nsample.drop('a',axis=1)\n```\n\n```\nOut[12]: \n          b         c         d         e  new_col1\n0  2.355071 -0.940921  0.164487 -1.025772 -3.131877\n1  0.962625  1.848441 -1.122676 -0.359290 -0.365921\n2 -0.124250 -0.259899 -0.111997 -1.816197  0.031495\n3  0.297850 -0.409256  0.485376 -2.790929  0.075091\n```\n\n第二种方式，\n\n```\n# In\nsample.drop(['a','b'],axis=1)\n```\n\n```\n      c         d         e  new_col1\n\n0 -0.940921  0.164487 -1.025772 -3.131877\n1  1.848441 -1.122676 -0.359290 -0.365921\n```\n\n## 条件查询\n\n### 生成示例数据\n\n```\n# In[]\nsample = pd.DataFrame({'name':['Bob','Lindy','Mark',\"Miki\",'Sully','Rose'],\n                       'score':[98,78,88,77,69,69],\n                       'group':[1,1,1,2,1,2]})\nsample\n```\n\n```\nOut[14]: \n    name  score  group\n0    Bob     98      1\n1  Lindy     78      1\n2   Mark     88      1\n3   Miki     77      2\n4  Sully     69      1\n5   Rose     69      2\n```\n\n### 单条件查询\n\n涉及单条件查询时，一般会使用比较运算符，产生布尔类型的索引可用于条件查询。\n\n```\nsample.score >66\n```\n\n\n\n```\nOut[15]: \n0    True\n1    True\n2    True\n3    True\n4    True\n5    True\n```\n\n再通过指定的索引进行条件查询，返回bool值为True的数据：\n\n```\nsample[sample.score >66]\n\nOut[16]: \n    name  score  group\n0    Bob     98      1\n1  Lindy     78      1\n2   Mark     88      1\n3   Miki     77      2\n4  Sully     69      1\n5   Rose     69      2\n```\n\n### 多条件查询\n```\nsample[(sample.score >66) & (sample.group==1)]\nOut[17]: \n    name  score  group\n0    Bob     98      1\n1  Lindy     78      1\n2   Mark     88      1\n4  Sully     69      1\n```\n\n### 使用 qurey\n\n```\nsample.query('score > 90')\nOut[20]: \n  name  score  group\n0  Bob     98      1\n```\n\n其他查询\n\n查询sample中70到80之间的记录，并且将边界包含进来（inclusive=True）\n\n```\n# In[]\nsample[sample['score'].between(70,80,inclusive=True)]\nOut[21]: \n    name  score  group\n1  Lindy     78      1\n3   Miki     77      2\n```\n\n对于字符串列来说，可以使用isin 方法进行查询：\n\n```\nsample[sample['name'].isin(['Bob','Lindy'])]\nOut[24]: \n    name  score  group\n0    Bob     98      1\n1  Lindy     78      1\n```\n\n使用正则表达式匹配进行查询，例如查询姓名以M开头的人的所有记录：\n\n```\nsample[sample['name'].str.contains('[M]+')]\nOut[26]: \n   name  score  group\n2  Mark     88      1\n3  Miki     77      2\n```\n\n## 横向连接\n\nPandas Data Frame 提供 merge 方法以完成各种表格的横向连接操作，这种连接操作跟SQL语句的连接操作类似。\n\n```\ndf1 =pd.DataFrame({'id':[1,2,3],\n                   'col1':['a','b','c']})\ndf2 = pd.DataFrame({'id':[4,3],\n                    'col2':['d','e']})\ndf1\nOut[29]: \n   id col1\n0   1    a\n1   2    b\n2   3    c\ndf2\nOut[30]: \n   id col2\n0   4    d\n1   3    e\n```\n\n内连接使用merge函数示例，根据公共字段保留两表的共有信息，`how = 'innner'`参数表示使用内连接，`on`表示两表的公共字段，若公共字段再两表名称不一致时，可以通过 `left_on`和`right_on`指定：\n\n```\ndf1.merge(df2,how='inner',on='id')\nOut[32]: \n   id col1 col2\n0   3    c    e\ndf1.merge(df2,how='inner',left_on='id',right_on='id')\nOut[33]: \n   id col1 col2\n0   3    c    e\n```\n\n### 外连接\n\n外连接包括左连接，全连接，右连接\n\n#### 左连接\n\n左连接通过公共字段，保留坐标的全部信息，右表在左表缺失的信息会以NaN补全：\n\n```\ndf1.merge(df2,how='left',on='id')\nOut[34]: \n   id col1 col2\n0   1    a  NaN\n1   2    b  NaN\n2   3    c    e\n```\n\n#### 右连接\n\n右连接与左连接相对，右连接通过公共字段，保留右表的全部信息，左表在右表缺失的信息会以 NaN 补全。\n\n```\ndf1.merge(df2,how='right',on='id')\nOut[35]: \n   id col1 col2\n0   3    c    e\n1   4  NaN    d\n```\n\n#### 全连接\n\n全连接通过公共字段，保留右表的全部信息，两表相互缺失的信息会以 NaN 补全。\n\n\n\n```\ndf1.merge(df2,how='outer',on='id')\nOut[36]: \n   id col1 col2\n0   1    a  NaN\n1   2    b  NaN\n2   3    c    e\n3   4  NaN    d\n```\n\n## 行索引连接\n\n`pd.concat`可以完成横向和纵向的合并，这通过 ’axis=‘ 来控制，当参数axis= 1时表示进行横向合并。\n\n```\ndf1 = pd.DataFrame({'id':[1,2,3],\n                    'col1':['a','b','c']},\n                   index=[1,2,3])\ndf2 =pd.DataFrame({'id':[1,2,3],\n                   'col2':['aa','bb','cc']},\n                  index=[1,3,2])\npd.concat([df1,df2],axis=1)\nOut[37]: \n   id col1  id col2\n1   1    a   1   aa\n2   2    b   3   cc\n3   3    c   2   bb\n\n```\n\n## 纵向合并\n\n当参数 axis = 0 时，表示纵向合并。ignore_index= True 表示忽略df1 和 df2 的原先的行索引，合并后重新排列索引。\n\n```\npd.concat([df1,df2],ignore_index=True,axis=0)\nOut[43]: \n  col1 col2  id\n0    a  NaN   1\n1    b  NaN   2\n2    c  NaN   3\n3  NaN   aa   1\n4  NaN   bb   2\n5  NaN   cc   3\n```\n\n去除重复行\n\n```\npd.concat([df1,df2],ignore_index=True,axis=0).drop_duplicates()\n\nOut[44]: \n  col1 col2  id\n0    a  NaN   1\n1    b  NaN   2\n2    c  NaN   3\n3  NaN   aa   1\n4  NaN   bb   2\n5  NaN   cc   3\n```\n\n\n\n\n\n## 排序\n\n按照学生成绩降序排列数据，第一个参数表示排序的依据，ascending = False 代表降序排列，na_position='last'表示缺失值数据排列在数据的最后位置。\n\n```\nsample = pd.DataFrame({'name':['Bob','Lindy','Mark',\"Miki\",'Sully','Rose'],\n                       'score':[98,78,88,77,69,np.nan],\n                       'group':[1,1,1,2,1,2]})\nsample\n###\nsample.sort_values('score',ascending= False,na_position='last')\nOut[46]: \n    name  score  group\n0    Bob   98.0      1\n2   Mark   88.0      1\n1  Lindy   78.0      1\n3   Miki   77.0      2\n4  Sully   69.0      1\n5   Rose    NaN      2\n```\n\n## 分组汇总\n\n数据准备\n\n```\nsample = pd.read_csv('./sample.csv',encoding='utf-8')\nsample\nOut[58]: \n   chinese   class  grade  math   name\n0        88      1      1  98.0    Bob\n1        78      1      1  78.0  Lindy\n2        68      1      1  78.0   Miki\n3        56      2      2  77.0   Mark\n4        77      1      2  77.0  Sully\n5        56      2      2   NaN   Rose\n\n```\n\n分组汇总操作中，会涉及分组变量，度量变量和汇总统计量。pandas 提供了 groupby 方法进行分组汇总。\n\n在sample数据中，grade为分组变量，math 为度量变量，现需要查询grade 为1，2中数学成绩最高。\n\n### 分组变量\n\n在进行分组汇总时，分组变量可以有多个。\n\n```\nsample.groupby(['grade','class'])['math'].max()\nOut[65]: \ngrade  class\n1      1        98.0\n2      1        77.0\n       2        77.0\nName: math, dtype: float64\n```\n\n\n\n### 汇总变量\n\n在进行分组汇总时，汇总变量也可以多个。\n\n```\nsample.groupby('grade',)['math','chinese'].mean()\nOut[75]: \n            math  chinese\ngrade                    \n1      84.666667       78\n2      77.000000       63\n```\n\n### 汇总统计量\n\n| 方法   | 解释   | 方法     | 解释         |\n| ------ | ------ | -------- | ------------ |\n| mean   | 均值   | mad      | 平均绝对偏差 |\n| max    | 最大值 | count    | 计数         |\n| min    | 最小值 | skew     | 偏度         |\n| median | 中位数 | quantile | 指定分位数   |\n| std    | 标准差 |          |              |\n\n以上统计量方法可以直接接 groupby 对象使用，agg方法提供了一次汇总多个统计量的方法，例如，汇总各个班级的数学成绩的均值，最大值，最小值。\n\n```\nsample.groupby('class')['math'].agg(['mean','min','max'])\nOut[78]: \n        mean   min   max\nclass                   \n1      82.75  77.0  98.0\n2      77.00  77.0  77.0\n```\n\n\n\n\n\n### 多重索引\n\n\n\n以年级，班级对学生的数学，语文成绩进行分组汇总，汇总统计量为均值。此时df中有两个行索引和两个列索引。\n\n```\ndf=sample.groupby(['class','grade'])['math','chinese'].agg(['mean','min','max'])\nOut[80]: \n                  math             chinese        \n                  mean   min   max    mean min max\nclass grade                                       \n1     1      84.666667  78.0  98.0      78  68  88\n      2      77.000000  77.0  77.0      77  77  77\n2     2      77.000000  77.0  77.0      56  56  56\n```\n\n查询各个年级、班级的数学成绩的最小值。\n\n```\ndf['math']['min']\nOut[84]: \nclass  grade\n1      1        78.0\n       2        77.0\n2      2        77.0\nName: min, dtype: float64\n```\n\n\n","tags":["数据整合"],"categories":["数据挖掘"]},{"title":"data_mining","url":"/2018/10/16/data-mining/","content":"[TOC]\n\n# Python 常用数据分析框架\n\n| 名称       |             解释             |\n| ---------- | :--------------------------: |\n| Numpy      |  数组，矩阵的存储，运算框架  |\n| Scipy      | 提供统计，线性代数等计算框架 |\n| Pandas     |  结构化数据的整合，处理框架  |\n| Statsmodel |    常见的统计分析框架模型    |\n| Matplotlib |        数据可视化框架        |\n\n\n\n# python 基础数据类型\n\n| 名称    | 解释   | 示例        |\n| ------- | ------ | ----------- |\n| str     | 字符串 | 'a', '2'    |\n| float   | 浮点数 | 1.23， 3.45 |\n| int     | 整数   | 3，5        |\n| bool    | 布尔   | Ture, False |\n| complex | 复数   | 1+2j, 2 +0j |\n\n\n\n# Python 数据格式转换\n\n| 数据类型 | 转换函数  |\n| -------- | --------- |\n| Str      | str()     |\n| Float    | float()   |\n| Int      | Int()     |\n| Bool     | bool()    |\n| Complex  | complex() |\n\n# 读取数据\n\n```\nimport pandas as pd\ncsv =  pd.read_csv('filename')\n```\n\n# 制图步骤\n\n在进行描述性图展示时，制图分为以下四步：\n\n1. 整理原始数据：对初始数据进行预处理和清洗，已达到制图得要求。\n2. 明确表达的信息： 根据初始可用数据，明确分析要表达的信息。\n3. 确定比较的类型： 明确要表达的信息中对目标比较的类型。\n4. 选择图表类型： 选择合适的图表类型，进行绘制并展示。\n\n```flow\n\na=>operation: 数据\nb=>operation: 信息\nc=>operation: 相对关系\nd=>operation: 图形\n\na(right)->b(right)->c(right)->d\n```\n\n\n","tags":["数据分析"]},{"title":"UML","url":"/2018/10/16/UML/","content":"\n\n\n# 制图步骤\n\n在进行描述性图展示时，制图分为以下四步：\n\n1. 整理原始数据：对初始数据进行预处理和清洗，已达到制图得要求。\n2. 明确表达的信息： 根据初始可用数据，明确分析要表达的信息。\n3. 确定比较的类型： 明确要表达的信息中对目标比较的类型。\n4. 选择图表类型： 选择合适的图表类型，进行绘制并展示。\n\n``` flow\n\na=>operation: 数据\nb=>operation: 信息\nc=>operation: 相对关系\nd=>operation: 图形\n\na(right)->b(right)->c(right)->d\n```\n\n\n\n\n(refer link)[http://www.gravizo.com/#howto]\n","tags":["markdown"]},{"title":"es-python-client","url":"/2018/10/12/es-python-client/","content":"# python 操作 Elasticsearch\n\n## 安装 Elasticsearch 模块\n`pip install elasticsearch`\n\n## 添加数据\n``` \nfrom elasticsearch import Elasticsearch\n\n# 默认host为localhost,port为9200.但也可以指定host与port\nes = Elasticsearch([{'host': '192.168.10.21', 'port': 9200}])\ndoc = {\n     'author': 'kimchy',\n    'text': 'Elasticsearch: cool. bonsai cool.',\n     'timestamp': localtime(),\n      }\nres = es.index(index=\"test-index\", doc_type='tweet', id=1, body=doc)\nprint(res)\n```\n如果创建成功会返回以下结果\n``` \n{'_index': 'test-index', '_type': 'tweet', '_id': '1', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 2, 'failed': 0}, '_seq_no': 0, '_primary_term': 1}\n\n```\n## 创建索引\n```\nes.indices.create(index='irisaa') \n```\n## 删除索引\n``` \nes.indices.create(index='irisaa')\n```\n## 查看集群状态\n```\nes.cluster.health(wait_for_status='yellow', request_timeout=1)\n```\n## 查询数据\n### 按照 id 来查询数据\n``` \nres = es.get(index=\"test-index\", doc_type='tweet', id=1)\nprint(res['_source'])\n```\n操作成功后返回如下结果：\n```\n{'author': 'kimchy', 'text': 'Elasticsearch: cool. bonsai cool.', 'timestamp': [2018, 10, 12, 14, 9, 44, 4, 285, 0]}\n\n```\n### 按照DSL语句查询\n```   \nres = es.search(index=\"test-index\", body={\"query\": {\"match_all\": {}}})\nprint(res)\n```\n操作成功后结果，返回如下结果：\n```\n{'took': 2, 'timed_out': False, '_shards': {'total': 5, 'successful': 5, 'skipped': 0, 'failed': 0}, 'hits': {'total': 1, 'max_score': 1.0, 'hits': [{'_index': 'test-index', '_type': 'tweet', '_id': '1', '_score': 1.0, '_source': {'author': 'kimchy', 'text': 'Elasticsearch: cool. bonsai cool.', 'timestamp': [2018, 10, 12, 14, 9, 44, 4, 285, 0]}}]}}\n```\n\n## 更新数据\n```\nes = Elasticsearch()\ndoc = {\n     'author': 'kimchy',\n    'text': 'ok.',\n     'timestamp': localtime(),\n      }\nresult = es.update(index=\"test-index\", doc_type='tweet', id=1, body=doc)\nprint(result) \n```\n\n\n## 删除数据\n```angular2html\n\nes.delete_by_query(index='twtter',doc_type='_doc',body={\n   \"query\": {\n     \"match\": {\n       \"message\": \"some message\"\n     }\n   }\n })\n```\n\n## 批量化导入es\n```\nfrom elasticsearch import helpers\ndef gendata(index, type, jsons):\n    for json in jsons:\n        yield {\n            \"_index\": index,\n            \"_type\": type,\n            \"_source\": json,\n        }\n        \nhelpers.bulk(es, gendata(index='index-test', jsons))        \n```\n","tags":["ELK"]},{"title":"ELK_docker-compose.md","url":"/2018/10/12/ELK-docker-compose-md/","content":"# elasticsearch 集群部署\n我们使用dockers-compose实现单机多节点的部署\n\n## 安装docker\n在测试或开发环境中 Docker 官方为了简化安装流程，提供了一套便捷的安装脚本，Ubuntu 系统上可以使用这套脚本安装：\n``` \n$ curl -fsSL get.docker.com -o get-docker.sh\n$ sudo sh get-docker.sh --mirror Aliyun\n```\n执行这个命令后，脚本就会自动的将一切准备工作做好，并且把 Docker CE 的 Edge 版本安装在系统中。\n### 启动 Docker CE\n \n``` \n$ sudo systemctl enable docker\n$ sudo systemctl start docker\n```\n\n### 建立 docker 用户组\n默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组。\n\n`$ sudo groupadd docker\n`\n\n将当前用户 加入 ``docker`` 组：\n\n`$ sudo usermod -aG docker $USER`\n\n### 测试dockers 是否安装正确\n```\n$ docker run hello-world\n\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\nca4f61b1923c: Pull complete\nDigest: sha256:be0cd392e45be79ffeffa6b05338b98ebb16c87b255f48e297ec7f98e123905c\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://cloud.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/engine/userguide/\n```\n\n## 安装docker-compopse\nCompose 项目是 Docker 官方的开源项目，负责实现对 Docker 容器集群的快速编排。\n它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）。\n\nCompose 中有两个重要的概念：\n\n- 服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。\n\n- 项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。\n\nCompose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。\n\nCompose 项目由 Python 编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。因此，只要所操作的平台支持 Docker API，就可以在其上利用 Compose 来进行编排管理。\n\n### PIP 安装 dockerpose\n``$ sudo pip install -U docker-compose\n``\n\n### 编写 docker-compose.yml\n编写 docker-compose.yml 文件,输入以下内容：\n```\nversion: '2.2'\nservices:\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.0\n    container_name: elasticsearch\n    environment:\n      - cluster.name=docker-cluster\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n        - esdata1:/usr/share/elasticsearch/data\n    volumes:\n      - /home/ethan/EKL/config:/usr/share/elasticsearch/config\n    ports:\n      - 9200:9200\n      - 9300:9300\n    networks:\n      - esnet\n\n  elasticsearch2:\n    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.0\n    container_name: elasticsearch2\n    environment:\n      - cluster.name=docker-cluster\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n      - \"discovery.zen.ping.unicast.hosts=elasticsearch\"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n      - esdata2:/usr/share/elasticsearch/data\n    networks:\n      - esnet\n\n  elasticsearch3:\n    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.0\n    container_name: elasticsearch3\n    environment:\n      - cluster.name=docker-cluster\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n      - \"discovery.zen.ping.unicast.hosts=elasticsearch\"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n      - esdata3:/usr/share/elasticsearch/data\n    networks:\n      - esnet\n  kibana:\n    image: docker.elastic.co/kibana/kibana:6.4.0\n    container_name: kibana\n    environment:\n      SERVER_NAME: kibana\n      ELASTICSEARCH_URL: http://elasticsearch:9200\n    ports:\n      - 5601:5601\n    networks:\n      - esnet\n\nvolumes:\n  esdata1:\n    driver: local\n  esdata2:\n    driver: local\n  esdata3:\n    driver: local\n\nnetworks:\n  esnet:\n\n```\n## 运行 eslasticsearch-kibana\n```\ndocker-compose up\n\n```\n","tags":["elasticsearch"]},{"title":"Elk初探","url":"/2018/10/10/Elk_platfrom/","content":"> 启动elaticsearch需要java环境，请自己谷歌搭建哈\n\n# elaticsearch\n下载tar包\n```powershell\nwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.4.1.tar.gz\n\n```\n\n解压\n```\ntar -zxvf elasticsearch-6.4.1.tar.gz\n```\n进入elaticsearch 可执行文件目录\n```angular2html\ncd elasticsearch-6.4.0/bin\n```\n启动elaticsearch\n```angular2html\n./elaticseaerch\n```\n如果没有抱任何错误，正常来说应该是可以通过restful API来访问的。\n````\ncurl -X GET \"localhost:9200/_cat/health?v\"\n\n````\n对应的返回结果\n```\nepoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\n1538014388 10:13:08  elasticsearch green           1         1      0   0    0    0        0             0                  -                100.0%\n```\n由此我们可以认为elaticsearch已经启动成功了。\n\n# kibana\n下载tar包\n```angular2html\nhttps://artifacts.elastic.co/downloads/kibana/kibana-6.4.1-linux-x86_64.tar.gz\n```\n解压tar包\n```\ntar -zxvf kibana-6.4.1-linux-x86_64.tar.gz\n```\n\n更改kibana配置文件\n```\nvim  /kibana-6.4.1-linux-x86_64/config/kibana.yml\n```\n添加或解注释以下内容\n```\n server.port: 5601\n server.host: \"localhost\"\n elasticsearch.url: \"http://localhost:9200\"\n\n```\n进入kibana可执行文件目录\n```\ncd kibana-6.4.1-linux-x86_64/bin/\n```\n启动kibana\n\n`./kibana`\n\n配置成功后，用浏览器访问`http://ip:5601`可以看到以下页面\n![](/image/kibana.png)\n\n\n# elaticsearc 基本操作\n## 检查集群状态\n```\ncurl -X GET \"localhost:9200/_cat/health?v\"\n\n```\n获取结果如下显示\n```\nepoch      timestamp cluster     status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\n1538019385 11:36:25  data-mining green           1         1      1   1    0    0        0             0                  -                100.0%\n\n```\n## 列出所有索引\n``` \ncurl -X GET \"localhost:9200/_cat/indices?v\"\n```\n如下图所示，我们只有一个索引\n```\nhealth status index   uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   .kibana eFUOj6qJTQCt4xQdvbv8KA   1   0          1            0        4kb            4kb\n\n```\n\n## 创建一个自定义的索引\n```angular2html\n curl -X PUT \"localhost:9200/customer?pretty\"\n```\n对插入的索引返回结果\n```\n\n{\n  \"acknowledged\" : true,\n  \"shards_acknowledged\" : true,\n  \"index\" : \"customer\"\n}\n\n```\n\n再次查看当前索引\n````angular2html\n curl -X GET \"localhost:9200/_cat/indices?v\"\n````\n可以观察到多了一个名为‘customer’的新索引\n```\nhealth status index    uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   .kibana  eFUOj6qJTQCt4xQdvbv8KA   1   0          1            0        4kb            4kb\nyellow open   customer k50FrrMLScGvwzOvfiF0fg   5   1          0            0       401b           401b\n```\n## 插入一个文档\n```angular2html\ncurl -X PUT \"localhost:9200/customer/_doc/1?pretty\" -H 'Content-Type: application/json' -d'\n{\n  \"name\": \"John Doe\"\n}\n'\n```\n如果操作正常，那么插入成功后会返回以下结果\n``` \n{\n  \"_index\" : \"customer\",\n  \"_type\" : \"_doc\",\n  \"_id\" : \"1\",\n  \"_version\" : 1,\n  \"result\" : \"created\",\n  \"_shards\" : {\n    \"total\" : 2,\n    \"successful\" : 1,\n    \"failed\" : 0\n  },\n  \"_seq_no\" : 0,\n  \"_primary_term\" : 1\n}\n```\n## 按照_id查询索引内的文档\n```\ncurl -X GET \"localhost:9200/customer/_doc/1?pretty\"\n```\n查看返回结果，\"_index\"为当前查询的索引，“_type”为查询的文档类型，“_id”为文档所在的id，\n“_source”为我们要查询的内容\n\n```\n\n{\n  \"_index\" : \"customer\",\n  \"_type\" : \"_doc\",\n  \"_id\" : \"1\",\n  \"_version\" : 1,\n  \"found\" : true,\n  \"_source\" : {\n    \"name\" : \"John Doe\"\n  }\n}\n```\n# elastcisear-head\n为了更方便的地查询es中的数据，我推荐使用es插件elasticsearch-head来快速检索es中的数据。\n\n传送门：[elastcsearch-head](https://github.com/mobz/elasticsearch-head)\n\n如果你使用的版本是在elasticsearch 5之后，还需要对elasticsearch 进行配置\n```\ncat >> elasticsearch-6.4.0/config/elasticsearch.yml << EOF\nhttp.cors.enabled: true\nhttp.cors.allow-origin: \"*\"\nEOF\n```\n\n\n## 安装 \n由于elasticsearch-head 需要nodejs，所以我们需要先安装 nodejs 以及 npm\n## nodejs 安装\n```\n$ sudo apt install nodejs\n$ sudo apt instlal npm\n```\nnodejs 安装完后，我们就可以把elasticsearch-head 下载，进行配置了。\n```\ngit clone git://github.com/mobz/elasticsearch-head.git\ncd elasticsearch-head\nnpm install\nnpm run start\n```\n操作成功后的输出显示\n```\n $ npm run start\n\n> elasticsearch-head@0.0.0 start /home/ethan/ekl/elasticsearch-head\n> grunt server\n\n(node:22696) ExperimentalWarning: The http2 module is an experimental API.\nRunning \"connect:server\" (connect) task\nWaiting forever...\nStarted connect web server on http://localhost:9100\n```\n![](/image/eshead.png)\n\n这样我们最简单的数据搜素平台就搭建成功了。\n","tags":["Elk"]},{"title":"Ethan Lee","url":"/2018/08/18/mygirl/","content":"# say something to mygirl\n\n\n<blockquote class=\"blockquote-center\">我知道到遇见你不容易,错过了会很可惜,我不希望余生都是回忆,我希望余生都是你,我爱你</blockquote>\n\n\n![](http://img02.tooopen.com/images/20160509/tooopen_sy_161967094653.jpg)\n\n[link](https://ethan2lee.github.io)\n","tags":["paper"]}]