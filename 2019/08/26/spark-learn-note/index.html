<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="李逸炫的博客">
    <meta name="keyword"  content="盈盛">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        spark-learn-note - Ethan Lee | ethan | ethan2lee
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 代码承旧万世基积沙镇海，梦想永在凌云意气风发 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar">
            <img src="/img/avatar.jpg" />
        </div>
        <div class="name">
            <i>Ethan Lee</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>主页</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archive">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>关于</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#服务器分布"><span class="toc-text">服务器分布</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#安装"><span class="toc-text">安装</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#快速入门"><span class="toc-text">快速入门</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#编程指南"><span class="toc-text">编程指南</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#环境变量参考"><span class="toc-text">环境变量参考</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#自建worker"><span class="toc-text">自建worker</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#python-spark-基础学习"><span class="toc-text">python spark 基础学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#搭建集群"><span class="toc-text">搭建集群</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#spark-machine-learning-api"><span class="toc-text">spark machine learning api</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#脚本启动spark-hadoop集群"><span class="toc-text">脚本启动spark-hadoop集群</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#配置jupyter运行spark"><span class="toc-text">配置jupyter运行spark</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#在pycharm调用pyspark"><span class="toc-text">在pycharm调用pyspark</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformation操作"><span class="toc-text">Transformation操作</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#spark-基本函数操作"><span class="toc-text">spark 基本函数操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#集合操作"><span class="toc-text">集合操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#key-values-值操作"><span class="toc-text">key values 值操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#下载-mysql-connector-jar"><span class="toc-text">下载  mysql-connector jar</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark-SQL"><span class="toc-text">Spark SQL</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#创建SparkSession"><span class="toc-text">创建SparkSession</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#创建-DataFrames"><span class="toc-text">创建 DataFrames</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataSet-操作"><span class="toc-text">DataSet 操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SQL-查询编程"><span class="toc-text">SQL 查询编程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Global-Temporary-View"><span class="toc-text">Global Temporary View</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#JDBC-连接创建"><span class="toc-text">JDBC  连接创建</span></a></li></ol></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>
        <div class="index-about-mobile">
            <i> 代码承旧万世基积沙镇海，梦想永在凌云意气风发 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        spark-learn-note
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2019-08-26 06:12:31</span></span>
        
        <span class="attr">标签：/
        
        <a class="tag" href="/tags/#saprk" title="saprk">saprk</a>
        <span>/</span>
        
        
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content no-indent">
        <p>先读些系列的文章，对spark有个基本的了解</p>
<p><a href="https://juejin.im/entry/57316d021532bc0065140876" target="_blank" rel="noopener">1. spark 简介</a></p>
<p><a href="http://litaotao.github.io/spark-questions-concepts" target="_blank" rel="noopener">2. spark 基本概念解析</a></p>
<p><a href="http://litaotao.github.io/spark-programming-model?s=inner" target="_blank" rel="noopener">3. spark 编程模式</a></p>
<p><a href="http://litaotao.github.io/spark-what-is-rdd?s=inner" target="_blank" rel="noopener">4. spark 之 RDD</a></p>
<p><a href="http://litaotao.github.io/spark-resouces-blogs-paper?s=inner" target="_blank" rel="noopener">5. 这些年，你不能错过的 spark 学习资源</a></p>
<p><a href="http://litaotao.github.io/deep-into-spark-exection-model?s=inner" target="_blank" rel="noopener">6. 深入研究 spark 运行原理之 job, stage, task</a></p>
<p><a href="http://litaotao.github.io/spark-dataframe-introduction?s=inner" target="_blank" rel="noopener">7. 使用 Spark DataFrame 进行大数据分析</a></p>
<p><a href="http://litaotao.github.io/spark-in-finance-and-investing?s=inner" target="_blank" rel="noopener">8. 实战案例 ｜ Spark 在金融领域的应用 ｜ 日内走势预测</a></p>
<p><a href="http://litaotao.github.io/ipython-notebook-spark" target="_blank" rel="noopener">9. 搭建 IPython + Notebook + Spark 开发环境</a></p>
<p><a href="http://litaotao.github.io/boost-spark-application-performance?s=inner" target="_blank" rel="noopener">10. spark 应用程序性能优化｜12 个优化方法</a></p>
<p><a href="http://litaotao.github.io/spark-mlib-machine-learning?s=inner" target="_blank" rel="noopener">11. spark 机器学习</a></p>
<p><a href="http://shiyanjun.cn/archives/744.html" target="_blank" rel="noopener">RDD：基于内存的集群计算容错抽象</a></p>
<p>书籍</p>
<p>百度云盘：<br>【spark 快速大数据分析】链接: <a href="https://pan.baidu.com/s/1D7GMbGeKEVg7edJwujehGA" target="_blank" rel="noopener">https://pan.baidu.com/s/1D7GMbGeKEVg7edJwujehGA</a> 提取码: he2d </p>
<h1 id="服务器分布"><a href="#服务器分布" class="headerlink" title="服务器分布"></a>服务器分布</h1><p>master ip: 192.168.10.170<br>user: spark<br>passwd:s1<br>spark_root_dir:/home/spark/spark-2.4.0-bin-hadoop2.7  </p>
<p>slaver ip: 192.168.10.171,   192.168.10.172<br>user: spark<br>passwd:s1<br>spark_root_dir:/home/spark/spark-2.4.0-bin-hadoop2.7 </p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p><a href="https://spark.apache.org/downloads.html" target="_blank" rel="noopener">download</a></p>
<p>1 下载特定版本安装包，并解压<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">tar</span> <span class="selector-tag">-zxvf</span> <span class="selector-tag">spark-2</span><span class="selector-class">.4</span><span class="selector-class">.0-bin-hadoop2</span><span class="selector-class">.7</span><span class="selector-class">.tgz</span></span><br></pre></td></tr></table></figure></p>
<p>2 配置 JAVA_HOME</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">JAVA_HOME</span>=/home/spark/jdk1.8.0_191</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">PATH</span>=<span class="variable">$PATH</span>:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure>
<p>3 配置python api<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cd</span> spark_home_dir/<span class="keyword">python</span></span><br><span class="line">sudo <span class="keyword">python3</span> setup.<span class="keyword">py</span> install</span><br></pre></td></tr></table></figure></p>
<p>4 测试配置成功<br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark<span class="variable">@spark1</span><span class="symbol">:~/spark-</span><span class="number">2.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">7</span><span class="variable">$ </span>./bin/spark-submit examples/src/main/python/pi.py</span><br></pre></td></tr></table></figure></p>
<p>若配置成功会输出以下内容<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">31</span> WARN  <span class="string">NativeCodeLoader:</span><span class="number">62</span> - Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">31</span> INFO  <span class="string">SparkContext:</span><span class="number">54</span> - Running Spark version <span class="number">2.4</span><span class="number">.0</span></span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">31</span> INFO  <span class="string">SparkContext:</span><span class="number">54</span> - Submitted <span class="string">application:</span> PythonPi</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">31</span> INFO  <span class="string">SecurityManager:</span><span class="number">54</span> - Changing view acls <span class="string">to:</span> spark</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">31</span> INFO  <span class="string">SecurityManager:</span><span class="number">54</span> - Changing modify acls <span class="string">to:</span> spark</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">31</span> INFO  <span class="string">SecurityManager:</span><span class="number">54</span> - Changing view acls groups <span class="string">to:</span></span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">31</span> INFO  <span class="string">SecurityManager:</span><span class="number">54</span> - Changing modify acls groups <span class="string">to:</span></span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">31</span> INFO  <span class="string">SecurityManager:</span><span class="number">54</span> - <span class="string">SecurityManager:</span> authentication disabled; ui acls disabled; users  with view <span class="string">permissions:</span> Set(spark); groups with view <span class="string">permissions:</span> Set(); users  with modify <span class="string">permissions:</span> Set(spark); groups with modify <span class="string">permissions:</span> Set()</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">31</span> INFO  <span class="string">Utils:</span><span class="number">54</span> - Successfully started service <span class="string">'sparkDriver'</span> on port <span class="number">34371.</span></span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">31</span> INFO  <span class="string">SparkEnv:</span><span class="number">54</span> - Registering MapOutputTracker</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">31</span> INFO  <span class="string">SparkEnv:</span><span class="number">54</span> - Registering BlockManagerMaster</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">31</span> INFO  <span class="string">BlockManagerMasterEndpoint:</span><span class="number">54</span> - Using org.apache.spark.storage.DefaultTopologyMapper <span class="keyword">for</span> getting topology information</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">31</span> INFO  <span class="string">BlockManagerMasterEndpoint:</span><span class="number">54</span> - BlockManagerMasterEndpoint up</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">31</span> INFO  <span class="string">DiskBlockManager:</span><span class="number">54</span> - Created local directory at <span class="regexp">/tmp/</span>blockmgr<span class="number">-0</span>f28f494<span class="number">-80</span>f9<span class="number">-4</span>d57<span class="number">-880</span>f-ff3ab72fc569</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">31</span> INFO  <span class="string">MemoryStore:</span><span class="number">54</span> - MemoryStore started with capacity <span class="number">366.3</span> MB</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">31</span> INFO  <span class="string">SparkEnv:</span><span class="number">54</span> - Registering OutputCommitCoordinator</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">log:</span><span class="number">192</span> - Logging initialized @<span class="number">1724</span>ms</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">Server:</span><span class="number">351</span> - jetty<span class="number">-9.3</span>.z-SNAPSHOT, build <span class="string">timestamp:</span> unknown, git <span class="string">hash:</span> unknown</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">Server:</span><span class="number">419</span> - Started @<span class="number">1791</span>ms</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">AbstractConnector:</span><span class="number">278</span> - Started ServerConnector@<span class="number">8</span>aee1e3&#123;HTTP<span class="regexp">/1.1,[http/</span><span class="number">1.1</span>]&#125;&#123;<span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>:<span class="number">4040</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">Utils:</span><span class="number">54</span> - Successfully started service <span class="string">'SparkUI'</span> on port <span class="number">4040.</span></span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">4</span>f06548c&#123;/jobs,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">291e161</span>b&#123;<span class="regexp">/jobs/</span>json,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler<span class="meta">@fca</span>97e6&#123;<span class="regexp">/jobs/</span>job,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">4</span>b23942e&#123;<span class="regexp">/jobs/</span>job/json,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">1254</span>fd9f&#123;/stages,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">49</span>fa3d94&#123;<span class="regexp">/stages/</span>json,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">3</span>c3aa98a&#123;<span class="regexp">/stages/</span>stage,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">46</span>ecf8e5&#123;<span class="regexp">/stages/</span>stage/json,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">2</span>f736f4b&#123;<span class="regexp">/stages/</span>pool,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">3</span>acf3392&#123;<span class="regexp">/stages/</span>pool/json,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">7721</span>ef93&#123;/storage,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">56</span>ab1a42&#123;<span class="regexp">/storage/</span>json,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">62163</span>eeb&#123;<span class="regexp">/storage/</span>rdd,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">2106</span>c298&#123;<span class="regexp">/storage/</span>rdd/json,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">5923</span>ec5c&#123;/environment,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">57</span>d49430&#123;<span class="regexp">/environment/</span>json,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">4e60947</span>&#123;/executors,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">13e31941</span>&#123;<span class="regexp">/executors/</span>json,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">55e4566</span>d&#123;<span class="regexp">/executors/</span>threadDump,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">26e23</span>df5&#123;<span class="regexp">/executors/</span>threadDump/json,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">127</span>aa45f&#123;/<span class="keyword">static</span>,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">60</span>a5aa78&#123;/,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">2</span>b225b78&#123;/api,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">62</span>fead0&#123;<span class="regexp">/jobs/</span>job/kill,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">62</span>f13110&#123;<span class="regexp">/stages/</span>stage/kill,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">SparkUI:</span><span class="number">54</span> - Bound SparkUI to <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>, and started at <span class="string">http:</span><span class="comment">//spark1:4040</span></span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">Executor:</span><span class="number">54</span> - Starting executor ID driver on host localhost</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">Utils:</span><span class="number">54</span> - Successfully started service <span class="string">'org.apache.spark.network.netty.NettyBlockTransferService'</span> on port <span class="number">42589.</span></span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">NettyBlockTransferService:</span><span class="number">54</span> - Server created on <span class="string">spark1:</span><span class="number">42589</span></span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">BlockManager:</span><span class="number">54</span> - Using org.apache.spark.storage.RandomBlockReplicationPolicy <span class="keyword">for</span> block replication policy</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">BlockManagerMaster:</span><span class="number">54</span> - Registering BlockManager BlockManagerId(driver, spark1, <span class="number">42589</span>, None)</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">BlockManagerMasterEndpoint:</span><span class="number">54</span> - Registering block manager <span class="string">spark1:</span><span class="number">42589</span> with <span class="number">366.3</span> MB RAM, BlockManagerId(driver, spark1, <span class="number">42589</span>, None)</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">BlockManagerMaster:</span><span class="number">54</span> - Registered BlockManager BlockManagerId(driver, spark1, <span class="number">42589</span>, None)</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">BlockManager:</span><span class="number">54</span> - Initialized <span class="string">BlockManager:</span> BlockManagerId(driver, spark1, <span class="number">42589</span>, None)</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">526</span>f5cb1&#123;<span class="regexp">/metrics/</span>json,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">SharedState:</span><span class="number">54</span> - Setting hive.metastore.warehouse.dir (<span class="string">'null'</span>) to the value of spark.sql.warehouse.dir (<span class="string">'file:/home/spark/spark-2.4.0-bin-hadoop2.7/spark-warehouse'</span>).</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">SharedState:</span><span class="number">54</span> - Warehouse path is <span class="string">'file:/home/spark/spark-2.4.0-bin-hadoop2.7/spark-warehouse'</span>.</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">61e254</span>a2&#123;/SQL,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">4</span>c890f2&#123;<span class="regexp">/SQL/</span>json,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">460</span>c2a3f&#123;<span class="regexp">/SQL/</span>execution,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">7462</span>cbcf&#123;<span class="regexp">/SQL/</span>execution/json,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">ContextHandler:</span><span class="number">781</span> - Started o.s.j.s.ServletContextHandler@<span class="number">116110</span>ac&#123;<span class="regexp">/static/</span>sql,<span class="literal">null</span>,AVAILABLE,<span class="meta">@Spark</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">32</span> INFO  <span class="string">StateStoreCoordinatorRef:</span><span class="number">54</span> - Registered StateStoreCoordinator endpoint</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">SparkContext:</span><span class="number">54</span> - Starting <span class="string">job:</span> reduce at <span class="regexp">/home/</span>spark<span class="regexp">/spark-2.4.0-bin-hadoop2.7/</span>examples<span class="regexp">/src/</span>main<span class="regexp">/python/</span>pi.<span class="string">py:</span><span class="number">44</span></span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">DAGScheduler:</span><span class="number">54</span> - Got job <span class="number">0</span> (reduce at <span class="regexp">/home/</span>spark<span class="regexp">/spark-2.4.0-bin-hadoop2.7/</span>examples<span class="regexp">/src/</span>main<span class="regexp">/python/</span>pi.<span class="string">py:</span><span class="number">44</span>) with <span class="number">2</span> output partitions</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">DAGScheduler:</span><span class="number">54</span> - Final <span class="string">stage:</span> ResultStage <span class="number">0</span> (reduce at <span class="regexp">/home/</span>spark<span class="regexp">/spark-2.4.0-bin-hadoop2.7/</span>examples<span class="regexp">/src/</span>main<span class="regexp">/python/</span>pi.<span class="string">py:</span><span class="number">44</span>)</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">DAGScheduler:</span><span class="number">54</span> - Parents of <span class="keyword">final</span> <span class="string">stage:</span> List()</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">DAGScheduler:</span><span class="number">54</span> - Missing <span class="string">parents:</span> List()</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">DAGScheduler:</span><span class="number">54</span> - Submitting ResultStage <span class="number">0</span> (PythonRDD[<span class="number">1</span>] at reduce at <span class="regexp">/home/</span>spark<span class="regexp">/spark-2.4.0-bin-hadoop2.7/</span>examples<span class="regexp">/src/</span>main<span class="regexp">/python/</span>pi.<span class="string">py:</span><span class="number">44</span>), which has no missing parents</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">MemoryStore:</span><span class="number">54</span> - Block broadcast_0 stored <span class="keyword">as</span> values <span class="keyword">in</span> memory (estimated size <span class="number">6.2</span> KB, free <span class="number">366.3</span> MB)</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">MemoryStore:</span><span class="number">54</span> - Block broadcast_0_piece0 stored <span class="keyword">as</span> bytes <span class="keyword">in</span> memory (estimated size <span class="number">4.2</span> KB, free <span class="number">366.3</span> MB)</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">BlockManagerInfo:</span><span class="number">54</span> - Added broadcast_0_piece0 <span class="keyword">in</span> memory on <span class="string">spark1:</span><span class="number">42589</span> (<span class="string">size:</span> <span class="number">4.2</span> KB, <span class="string">free:</span> <span class="number">366.3</span> MB)</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">SparkContext:</span><span class="number">54</span> - Created broadcast <span class="number">0</span> from broadcast at DAGScheduler.<span class="string">scala:</span><span class="number">1161</span></span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">DAGScheduler:</span><span class="number">54</span> - Submitting <span class="number">2</span> missing tasks from ResultStage <span class="number">0</span> (PythonRDD[<span class="number">1</span>] at reduce at <span class="regexp">/home/</span>spark<span class="regexp">/spark-2.4.0-bin-hadoop2.7/</span>examples<span class="regexp">/src/</span>main<span class="regexp">/python/</span>pi.<span class="string">py:</span><span class="number">44</span>) (first <span class="number">15</span> tasks are <span class="keyword">for</span> partitions Vector(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">TaskSchedulerImpl:</span><span class="number">54</span> - Adding task set <span class="number">0.0</span> with <span class="number">2</span> tasks</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">TaskSetManager:</span><span class="number">54</span> - Starting task <span class="number">0.0</span> <span class="keyword">in</span> stage <span class="number">0.0</span> (TID <span class="number">0</span>, localhost, executor driver, partition <span class="number">0</span>, PROCESS_LOCAL, <span class="number">7852</span> bytes)</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">TaskSetManager:</span><span class="number">54</span> - Starting task <span class="number">1.0</span> <span class="keyword">in</span> stage <span class="number">0.0</span> (TID <span class="number">1</span>, localhost, executor driver, partition <span class="number">1</span>, PROCESS_LOCAL, <span class="number">7852</span> bytes)</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">Executor:</span><span class="number">54</span> - Running task <span class="number">1.0</span> <span class="keyword">in</span> stage <span class="number">0.0</span> (TID <span class="number">1</span>)</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">Executor:</span><span class="number">54</span> - Running task <span class="number">0.0</span> <span class="keyword">in</span> stage <span class="number">0.0</span> (TID <span class="number">0</span>)</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">PythonRunner:</span><span class="number">54</span> - <span class="string">Times:</span> total = <span class="number">378</span>, boot = <span class="number">246</span>, init = <span class="number">48</span>, finish = <span class="number">84</span></span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">PythonRunner:</span><span class="number">54</span> - <span class="string">Times:</span> total = <span class="number">382</span>, boot = <span class="number">242</span>, init = <span class="number">52</span>, finish = <span class="number">88</span></span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">Executor:</span><span class="number">54</span> - Finished task <span class="number">1.0</span> <span class="keyword">in</span> stage <span class="number">0.0</span> (TID <span class="number">1</span>). <span class="number">1421</span> bytes result sent to driver</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">Executor:</span><span class="number">54</span> - Finished task <span class="number">0.0</span> <span class="keyword">in</span> stage <span class="number">0.0</span> (TID <span class="number">0</span>). <span class="number">1421</span> bytes result sent to driver</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">TaskSetManager:</span><span class="number">54</span> - Finished task <span class="number">1.0</span> <span class="keyword">in</span> stage <span class="number">0.0</span> (TID <span class="number">1</span>) <span class="keyword">in</span> <span class="number">426</span> ms on localhost (executor driver) (<span class="number">1</span>/<span class="number">2</span>)</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">TaskSetManager:</span><span class="number">54</span> - Finished task <span class="number">0.0</span> <span class="keyword">in</span> stage <span class="number">0.0</span> (TID <span class="number">0</span>) <span class="keyword">in</span> <span class="number">446</span> ms on localhost (executor driver) (<span class="number">2</span>/<span class="number">2</span>)</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">TaskSchedulerImpl:</span><span class="number">54</span> - Removed TaskSet <span class="number">0.0</span>, whose tasks have all completed, from pool</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">PythonAccumulatorV2:</span><span class="number">54</span> - Connected to AccumulatorServer at <span class="string">host:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span> <span class="string">port:</span> <span class="number">41033</span></span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">DAGScheduler:</span><span class="number">54</span> - ResultStage <span class="number">0</span> (reduce at <span class="regexp">/home/</span>spark<span class="regexp">/spark-2.4.0-bin-hadoop2.7/</span>examples<span class="regexp">/src/</span>main<span class="regexp">/python/</span>pi.<span class="string">py:</span><span class="number">44</span>) finished <span class="keyword">in</span> <span class="number">0.536</span> s</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">DAGScheduler:</span><span class="number">54</span> - Job <span class="number">0</span> <span class="string">finished:</span> reduce at <span class="regexp">/home/</span>spark<span class="regexp">/spark-2.4.0-bin-hadoop2.7/</span>examples<span class="regexp">/src/</span>main<span class="regexp">/python/</span>pi.<span class="string">py:</span><span class="number">44</span>, took <span class="number">0.576000</span> s</span><br><span class="line">Pi is roughly <span class="number">3.150560</span></span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">AbstractConnector:</span><span class="number">318</span> - Stopped Spark@<span class="number">8</span>aee1e3&#123;HTTP<span class="regexp">/1.1,[http/</span><span class="number">1.1</span>]&#125;&#123;<span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>:<span class="number">4040</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">SparkUI:</span><span class="number">54</span> - Stopped Spark web UI at <span class="string">http:</span><span class="comment">//spark1:4040</span></span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">MapOutputTrackerMasterEndpoint:</span><span class="number">54</span> - MapOutputTrackerMasterEndpoint stopped!</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">MemoryStore:</span><span class="number">54</span> - MemoryStore cleared</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">BlockManager:</span><span class="number">54</span> - BlockManager stopped</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">BlockManagerMaster:</span><span class="number">54</span> - BlockManagerMaster stopped</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:</span><span class="number">54</span> - OutputCommitCoordinator stopped!</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">33</span> INFO  <span class="string">SparkContext:</span><span class="number">54</span> - Successfully stopped SparkContext</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">34</span> INFO  <span class="string">ShutdownHookManager:</span><span class="number">54</span> - Shutdown hook called</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">34</span> INFO  <span class="string">ShutdownHookManager:</span><span class="number">54</span> - Deleting directory <span class="regexp">/tmp/</span>spark-c09679ff-e0e0<span class="number">-4e24</span><span class="number">-839</span>b<span class="number">-74</span>de1930969a</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">34</span> INFO  <span class="string">ShutdownHookManager:</span><span class="number">54</span> - Deleting directory <span class="regexp">/tmp/</span>spark-c42919fe<span class="number">-751</span>c<span class="number">-4223</span><span class="number">-9</span>f83<span class="number">-530</span>cbd978945</span><br><span class="line"><span class="number">2018</span><span class="number">-12</span><span class="number">-12</span> <span class="number">02</span>:<span class="number">53</span>:<span class="number">34</span> INFO  <span class="string">ShutdownHookManager:</span><span class="number">54</span> - Deleting directory <span class="regexp">/tmp/</span>spark-c09679ff-e0e0<span class="number">-4e24</span><span class="number">-839</span>b<span class="number">-74</span>de1930969a/pyspark<span class="number">-876</span>b59fd<span class="number">-18</span>f9<span class="number">-4284</span><span class="number">-8131</span>-b09ac5642632</span><br></pre></td></tr></table></figure></p>
<h1 id="快速入门"><a href="#快速入门" class="headerlink" title="快速入门"></a>快速入门</h1><p><a href="https://spark-reference-doc-cn.readthedocs.io/zh_CN/latest/programming-guide/quick-start.html" target="_blank" rel="noopener">spark 2.2.x中文文档</a></p>
<h1 id="编程指南"><a href="#编程指南" class="headerlink" title="编程指南"></a>编程指南</h1><p><a href="https://aiyanbo.gitbooks.io/spark-programming-guide-zh-cn/content/quick-start/using-spark-shell.html" target="_blank" rel="noopener">link</a></p>
<h1 id="环境变量参考"><a href="#环境变量参考" class="headerlink" title="环境变量参考"></a>环境变量参考</h1><p>export JAVA_HOME=/home/spark/jdk1.8.0_191</p>
<p>export PATH=$PATH:$JAVA_HOME/bin</p>
<p>export PYSPARK_PYTHON=python3</p>
<p>export SPARK_HOME=/home/spark/spark-2.4.0-bin-hadoop2.7</p>
<p>export PYSPARK_SUBMIT_ARGS=–master spark://192.168.10.170:7077 –deploy-mode client</p>
<p>或者</p>
<p>export JAVA_HOME=/home/spark/jdk1.8.0_191<br>export PATH=$PATH:$JAVA_HOME/bin</p>
<p>export SPARK_HOME=/home/spark/spark-2.4.0-bin-hadoop2.7<br>export PATH=$PATH:$SPARK_HOME/bin</p>
<p>export PYSPARK_PYTHON=python3<br>export PYSPARK_DRIVER_PYTHON=jupyter<br>export PYSPARK_DRIVER_PYTHON_OPTS=”notebook  –ip=192.168.10.170”<br>export MASTER=”spark://192.168.10.170:7077”</p>
<p>export PYSPARK_SUBMIT_ARGS=”–master spark://192.168.10.170:7077 –deploy-mode client”<br>export HADOOP_HOME=/home/spark/hadoop-2.7.7<br>export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</p>
<p>export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native/</p>
<p>export SPARK__VERSION=2.4.0<br>export SCALA_VERSION=2.11.8</p>
<h1 id="自建worker"><a href="#自建worker" class="headerlink" title="自建worker"></a>自建worker</h1><p>spark-class org.apache.spark.deploy.worker.Worker spark://spark1:7077</p>
<h1 id="python-spark-基础学习"><a href="#python-spark-基础学习" class="headerlink" title="python spark 基础学习"></a>python spark 基础学习</h1><p>在 Spark 目录下运行以下命令可以启动 Spark Shell：</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.<span class="regexp">/bin/</span>pyspark</span><br></pre></td></tr></table></figure>
<p>或者如果在你当前环境已经使用 pip 安装了 PySpark，你也可以直接使用以下命令:<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">pyspark</span></span><br></pre></td></tr></table></figure></p>
<p>运行输出</p>
<figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">spark@spark1:~/spark-<span class="number">2.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">7</span>$ pyspark</span><br><span class="line">Python <span class="number">3.6</span>.<span class="number">7</span> (<span class="keyword">default</span>, Oct <span class="number">22</span> <span class="number">2018</span>, <span class="number">11</span>:<span class="number">32</span>:<span class="number">17</span>)</span><br><span class="line">[GCC <span class="number">8.2</span>.<span class="number">0</span>] on linux</span><br><span class="line"><span class="built_in">Type</span> <span class="string">"help"</span>, <span class="string">"copyright"</span>, <span class="string">"credits"</span> <span class="built_in">or</span> <span class="string">"license"</span> <span class="keyword">for</span> more information.</span><br><span class="line"><span class="number">2018</span>-<span class="number">12</span>-<span class="number">12</span> <span class="number">03</span>:<span class="number">32</span>:<span class="number">28</span> WARN  NativeCodeLoader:<span class="number">62</span> - Unable <span class="keyword">to</span> <span class="built_in">load</span> native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</span><br><span class="line">Setting <span class="keyword">default</span> <span class="built_in">log</span> level <span class="keyword">to</span> <span class="string">"WARN"</span>.</span><br><span class="line"><span class="keyword">To</span> adjust logging level use sc.setLogLevel(newLevel). <span class="keyword">For</span> SparkR, use setLogLevel(newLevel).</span><br><span class="line">Welcome <span class="keyword">to</span></span><br><span class="line">      <span class="variable">____</span>              <span class="variable">__</span></span><br><span class="line">     / <span class="variable">__</span>/<span class="variable">__</span>  <span class="variable">___</span> <span class="variable">_____</span>/ /<span class="variable">__</span></span><br><span class="line">    _\ \/ _ \/ _ `/ <span class="variable">__</span>/  <span class="string">'_/</span></span><br><span class="line"><span class="string">   /__ / .__/\_,_/_/ /_/\_\   version 2.4.0</span></span><br><span class="line"><span class="string">      /_/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Using Python version 3.6.7 (default, Oct 22 2018 11:32:17)</span></span><br><span class="line"><span class="string">SparkSession available as '</span>spark<span class="string">'.</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;</span></span><br></pre></td></tr></table></figure>
<p>Spark 最主要的抽象概念就是一个叫做 Dataset 的分布式数据集。<br>Dataset 可以从 Hadoop InputFormats(例如 HDFS 文件)创建或者由其他 Dataset 转换而来。由于 Python 语言的动态性, 我们不需要 Dataset 是强类型的。<br>因此 Python 中所有的 Dataset 都是 Dataset[Row], 并且为了和 Pandas 以及 R 中的 data frame 概念保持一致, 我们称其为 DataFrame。<br>下面我们利用 Spark 源码目录下 README 文件中的文本来新建一个 DataFrame:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; textFile = spark<span class="selector-class">.read</span><span class="selector-class">.text</span>(<span class="string">"README.md"</span>)</span><br></pre></td></tr></table></figure>
<p>计算文本的行数,以及获取第一行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>textFile.count()   <span class="comment"># Number of rows in this DataFrame</span></span><br><span class="line"><span class="number">105</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>textFile.first()  <span class="comment"># First row in this DataFrame</span></span><br><span class="line">Row(value=<span class="string">u'# Apache Spark'</span>)</span><br></pre></td></tr></table></figure>
<p>现在我们将该 DataFrame 转换成一个新的 DataFrame。</p>
<p>我们调用 filter 这个 transformation 算子返回一个只包含原始文件数据项子集的新 DataFrame。</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; linesWithSpark = textFile.<span class="built_in">filter</span>(textFile.<span class="built_in">value</span>.<span class="keyword">contains</span>(<span class="string">"Spark"</span>))</span><br></pre></td></tr></table></figure>
<p>我们可以将 transformation 算子和 action 算子连在一起:<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; textFile.<span class="built_in">filter</span>(textFile.<span class="built_in">value</span>.<span class="keyword">contains</span>(<span class="string">"Spark"</span>)).count()  <span class="comment"># How many lines contain "Spark"?</span></span><br><span class="line"><span class="number">20</span></span><br><span class="line">`</span><br></pre></td></tr></table></figure></p>
<p>Spark 可以很容易地实现 MapReduce 流程：</p>
<figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions import *</span><br><span class="line">textFile.<span class="built_in">select</span>(<span class="built_in">size</span>(split(textFile.value, <span class="string">"\s+"</span>)).<span class="built_in">name</span>(<span class="string">"numWords"</span>)).agg(<span class="built_in">max</span>(col(<span class="string">"numWords"</span>))).collect()</span><br></pre></td></tr></table></figure>
<p>首先，使用 map 算子将每一行映射为一个整数值并给其取别名 “numWords”, 创建了一个新的 DataFrame。</p>
<p>然后在该 DataFrame 上调用 agg 算子找出最大的单词计数。select 和 agg 的参数都是 Column , </p>
<p>我们可以使用 df.colName 从 DataFrame 上获取一列，也可以引入 pyspark.sql.functions, 它提供了很多方便的函数用来从旧的 Column 构建新的 Column。</p>
<p>因 Hadoop 而广为流行的 MapReduce 是一种通用的数据流模式。Spark 可以很容易地实现 MapReduce 流程：</p>
<figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; wordCounts = textFile.<span class="keyword">select</span>(explode(split(textFile.<span class="keyword">value</span>, <span class="string">"\s+"</span>)).<span class="keyword">alias</span>(<span class="string">"word"</span>)).groupBy(<span class="string">"word"</span>).count()</span><br></pre></td></tr></table></figure>
<p>这里我们在 select 函数中使用 explode 函数将一个行的 Dataset 转换成了一个单词的 Dataset,<br>然后组合 groupBy 和 count 算子来计算文件中每个单词出现的次数，生成一个包含 “word” 和 “count” 这 2 列的 DataFrame。<br>为了在 shell 中收集到单词计数, 我们可以调用 collect 算子:</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; wordCounts.collect()</span><br><span class="line">[Row(<span class="attribute">word</span>=<span class="string">'online'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'graphs'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'["Parallel'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'["Building'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'thread'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'documentation'</span>, <span class="attribute">count</span>=3), Row(<span class="attribute">word</span>=<span class="string">'command,'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'abbreviated'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'overview'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'rich'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'set'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'-DskipTests'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'name'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'page](http://spark.apache.org/documentation.html).'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'["Specifying'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'stream'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'run:'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'not'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'programs'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'tests'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'./dev/run-tests'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'will'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'[run'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'particular'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'option'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Alternatively,'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'by'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'must'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'using'</span>, <span class="attribute">count</span>=5), Row(<span class="attribute">word</span>=<span class="string">'you'</span>, <span class="attribute">count</span>=4), Row(<span class="attribute">word</span>=<span class="string">'MLlib'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'DataFrames,'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'variable'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Note'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'core'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'more'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'protocols'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'guidance'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'shell:'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'can'</span>, <span class="attribute">count</span>=7), Row(<span class="attribute">word</span>=<span class="string">'site,'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'systems.'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Maven'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'[building'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'configure'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'for'</span>, <span class="attribute">count</span>=12), Row(<span class="attribute">word</span>=<span class="string">'README'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Interactive'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'how'</span>, <span class="attribute">count</span>=3), Row(<span class="attribute">word</span>=<span class="string">'[Configuration'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Hive'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'system'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'provides'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Hadoop-supported'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'pre-built'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'["Useful'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'directory.'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Example'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'example'</span>, <span class="attribute">count</span>=3), Row(<span class="attribute">word</span>=<span class="string">'Kubernetes'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'one'</span>, <span class="attribute">count</span>=3), Row(<span class="attribute">word</span>=<span class="string">'MASTER'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'in'</span>, <span class="attribute">count</span>=6), Row(<span class="attribute">word</span>=<span class="string">'library'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Spark.'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'contains'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Configuration'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'programming'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'with'</span>, <span class="attribute">count</span>=4), Row(<span class="attribute">word</span>=<span class="string">'contributing'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'downloaded'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'1000).count()'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'comes'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'machine'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Tools"](http://spark.apache.org/developer-tools.html).'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'building'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'params'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Guide](http://spark.apache.org/docs/latest/configuration.html)'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'given.'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'be'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'same'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'integration'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'than'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Programs'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'locally'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'using:'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'fast'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'[Apache'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'your'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'optimized'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Developer'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'R,'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'should'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'graph'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'package'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'-T'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'[project'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'project'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'`examples`'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'resource-managers/kubernetes/integration-tests/README.md'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'versions'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Spark](#building-spark).'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'general'</span>, <span class="attribute">count</span>=3), Row(<span class="attribute">word</span>=<span class="string">'other'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'learning,'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'when'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'submit'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Apache'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'1000:'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'detailed'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'About'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'is'</span>, <span class="attribute">count</span>=7), Row(<span class="attribute">word</span>=<span class="string">'on'</span>, <span class="attribute">count</span>=7), Row(<span class="attribute">word</span>=<span class="string">'scala&gt;'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'print'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'use'</span>, <span class="attribute">count</span>=3), Row(<span class="attribute">word</span>=<span class="string">'different'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'following'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'SparkPi'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'refer'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'./bin/run-example'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'data'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Tests'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Versions'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Data.'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'processing.'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'its'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'basic'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'latest'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'only'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'&lt;class&gt;'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'have'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'runs.'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'You'</span>, <span class="attribute">count</span>=4), Row(<span class="attribute">word</span>=<span class="string">'tips,'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'project.'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'developing'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'YARN,'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'It'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'"local"'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'processing,'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'built'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Pi'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'thread,'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'A'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'APIs'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Scala,'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'file'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'computation'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Once'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'find'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'the'</span>, <span class="attribute">count</span>=24), Row(<span class="attribute">word</span>=<span class="string">'To'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'sc.parallelize(1'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'uses'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Version'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'N'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'programs,'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'"yarn"'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'see'</span>, <span class="attribute">count</span>=4), Row(<span class="attribute">word</span>=<span class="string">'./bin/pyspark'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'return'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'computing'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Java,'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'from'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Because'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'cluster'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'Streaming'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'More'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'analysis.'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Maven](http://maven.apache.org/).'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'cluster.'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Running'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Please'</span>, <span class="attribute">count</span>=4), Row(<span class="attribute">word</span>=<span class="string">'talk'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'distributions.'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'guide,'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'tests](http://spark.apache.org/developer-tools.html#individual-tests).'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'There'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'"local[N]"'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Try'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'and'</span>, <span class="attribute">count</span>=10), Row(<span class="attribute">word</span>=<span class="string">'do'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'Scala'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'class'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'build'</span>, <span class="attribute">count</span>=4), Row(<span class="attribute">word</span>=<span class="string">'3"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'setup'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'need'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'spark://'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Hadoop,'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'Thriftserver'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'are'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'requires'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'package.'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Enabling'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'clean'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'sc.parallelize(range(1000)).count()'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'high-level'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'SQL'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'against'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'of'</span>, <span class="attribute">count</span>=5), Row(<span class="attribute">word</span>=<span class="string">'through'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'review'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'package.)'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Python,'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'easiest'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'no'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Testing'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'several'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'help'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'The'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'sample'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'MASTER=spark://host:7077'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Big'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'examples'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'an'</span>, <span class="attribute">count</span>=4), Row(<span class="attribute">word</span>=<span class="string">'#'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Online'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'test,'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'including'</span>, <span class="attribute">count</span>=4), Row(<span class="attribute">word</span>=<span class="string">'usage'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Python'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'at'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'development'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Spark"](http://spark.apache.org/docs/latest/building-spark.html).'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'IDE,'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'way'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Contributing'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'get'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'that'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'##'</span>, <span class="attribute">count</span>=9), Row(<span class="attribute">word</span>=<span class="string">'For'</span>, <span class="attribute">count</span>=3), Row(<span class="attribute">word</span>=<span class="string">'prefer'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'This'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'build/mvn'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'builds'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'running'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'web'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'run'</span>, <span class="attribute">count</span>=7), Row(<span class="attribute">word</span>=<span class="string">'locally.'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Spark'</span>, <span class="attribute">count</span>=16), Row(<span class="attribute">word</span>=<span class="string">'URL,'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'a'</span>, <span class="attribute">count</span>=9), Row(<span class="attribute">word</span>=<span class="string">'higher-level'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'tools'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'if'</span>, <span class="attribute">count</span>=4), Row(<span class="attribute">word</span>=<span class="string">'available'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">''</span>, <span class="attribute">count</span>=48), Row(<span class="attribute">word</span>=<span class="string">'Documentation'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'this'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'(You'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'&gt;&gt;&gt;'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'information'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'info'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'&lt;http://spark.apache.org/&gt;'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Shell'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'environment'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'built,'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'module,'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'them,'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'`./bin/run-example'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'instance:'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'first'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'[Contribution'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'guide](http://spark.apache.org/contributing.html)'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'documentation,'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'[params]`.'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'mesos://'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'engine'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'GraphX'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Maven,'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'example:'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'HDFS'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'YARN"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'or'</span>, <span class="attribute">count</span>=3), Row(<span class="attribute">word</span>=<span class="string">'to'</span>, <span class="attribute">count</span>=17), Row(<span class="attribute">word</span>=<span class="string">'Hadoop'</span>, <span class="attribute">count</span>=3), Row(<span class="attribute">word</span>=<span class="string">'individual'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'also'</span>, <span class="attribute">count</span>=5), Row(<span class="attribute">word</span>=<span class="string">'changed'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'started'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'./bin/spark-shell'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'threads.'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'supports'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'storage'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'version'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'instructions.'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Building'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'start'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'Many'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'which'</span>, <span class="attribute">count</span>=2), Row(<span class="attribute">word</span>=<span class="string">'And'</span>, <span class="attribute">count</span>=1), Row(<span class="attribute">word</span>=<span class="string">'distribution'</span>, <span class="attribute">count</span>=1)]</span><br></pre></td></tr></table></figure>
<h1 id="搭建集群"><a href="#搭建集群" class="headerlink" title="搭建集群"></a>搭建集群</h1><p><a href="https://spark.apache.org/docs/latest/spark-standalone.html" target="_blank" rel="noopener">link</a></p>
<p>在spark1,spark2,spark3的spark_home_dir下，<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /home/spark/spark<span class="number">-2.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.7</span>/conf/spark-env.sh</span><br></pre></td></tr></table></figure></p>
<p>添加以下内容<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">SPARK_MASTER_IP</span>=master</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">SPARK_WORKER_CORES</span>=4</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">SPARK_WORKER_MEMORY</span>=512m</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">SPARK_WORKER_INSTANCES</span>=4</span><br></pre></td></tr></table></figure></p>
<p>SPARK_MASTER_IP ：主机地址，现为192.168.10.170<br>SPARK_WORKER_CORES: 每个worker 分配的cpu内核数目，<br>SPARK_WORKER_MEMORY ：每个worker 分配的最大内存<br>SPARK_WORKER_INSTANCES： 每个spark启动worker的数目</p>
<p>在spark1的的spark_home_dir下，启动主机<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/<span class="literal">start</span>-<span class="literal">master</span>.sh</span><br></pre></td></tr></table></figure></p>
<p>在spark2的的spark_home_dir下，启动从机的worker<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.<span class="regexp">/sbin/</span>start-slave.sh <span class="string">spark:</span><span class="comment">//spark1:7077</span></span><br></pre></td></tr></table></figure></p>
<p>在spark3的的spark_home_dir下，启动从机的worker<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.<span class="regexp">/sbin/</span>start-slave.sh <span class="string">spark:</span><span class="comment">//spark1:7077</span></span><br></pre></td></tr></table></figure></p>
<p>启动成功后，用浏览器访问 <a href="http://spark1:8080" target="_blank" rel="noopener">http://spark1:8080</a> ，查看spark-standalone集群。</p>
<p>备注：<master-spark-url> 的文字格式应为  “spark://master_ip:7077”</master-spark-url></p>
<h1 id="spark-machine-learning-api"><a href="#spark-machine-learning-api" class="headerlink" title="spark machine learning api"></a>spark machine learning api</h1><p><a href="https://spark.apache.org/docs/latest/mllib-guide.html" target="_blank" rel="noopener">link</a></p>
<h1 id="脚本启动spark-hadoop集群"><a href="#脚本启动spark-hadoop集群" class="headerlink" title="脚本启动spark-hadoop集群"></a>脚本启动spark-hadoop集群</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### cluster_setup.sh</span></span><br><span class="line"><span class="comment">###  location:   spakr1:/home/spark/spark-2.4.0-bin-hadoop2.7/cluster_setup.sh</span></span><br><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"set up spark cluster and hadoop cluster"</span></span><br><span class="line"></span><br><span class="line">/home/spark/spark-2.4.0-bin-hadoop2.7/sbin/start-master.sh</span><br><span class="line">ssh spark@slave1 <span class="string">"/home/spark/spark-2.4.0-bin-hadoop2.7/sbin/start-slave.sh spark://spark1:7077"</span></span><br><span class="line">ssh spark@slave2 <span class="string">"/home/spark/spark-2.4.0-bin-hadoop2.7/sbin/start-slave.sh spark://spark1:7077"</span></span><br><span class="line">/home/spark/spark-2.4.0-bin-hadoop2.7/sbin/start-master.sh spark://spark1:7077</span><br><span class="line"></span><br><span class="line">/home/spark/hadoop-2.7.7/sbin/start-all.sh</span><br><span class="line">ssh spark@slave1 <span class="string">"/home/spark/hadoop-2.7.7/sbin/start-all.sh"</span></span><br><span class="line">ssh spark@slave2 <span class="string">"/home/spark/hadoop-2.7.7/sbin/start-all.sh"</span></span><br></pre></td></tr></table></figure>
<h1 id="配置jupyter运行spark"><a href="#配置jupyter运行spark" class="headerlink" title="配置jupyter运行spark"></a>配置jupyter运行spark</h1><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">PYSPARK_DRIVER_PYTHON</span>=jupyter <span class="attribute">PYSPARK_DRIVER_PYTHON_OPTS</span>=<span class="string">"notebook  --ip=192.168.10.170"</span>  pyspark</span><br></pre></td></tr></table></figure>
<p>不过我们可以把这两个加入环境变量中去,在~/.bashrc加入以下内容即可</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">PYSPARK_DRIVER_PYTHON</span>=jupyter </span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">PYSPARK_DRIVER_PYTHON_OPTS</span>=<span class="string">"notebook  --ip=192.168.10.170"</span></span><br></pre></td></tr></table></figure>
<p>更新环境变量<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span>  ~/.bashrc</span><br></pre></td></tr></table></figure></p>
<p>以后直接在shell中直接输入pyspark即可。</p>
<h1 id="在pycharm调用pyspark"><a href="#在pycharm调用pyspark" class="headerlink" title="在pycharm调用pyspark"></a>在pycharm调用pyspark</h1><p>需要在Run/Debug Configurations设置环境变量<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">SPARK_HOME</span>=/home/spark/spark-<span class="number">2.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">7</span></span><br><span class="line"><span class="attr">PYSPARK_DRIVER_PYTHON</span>=</span><br><span class="line"><span class="attr">PYSPARK_PYTHON</span>=/usr/bin/python3</span><br><span class="line"><span class="attr">MASTER</span>=spark://<span class="number">192.168</span>.<span class="number">10.170</span>:<span class="number">7077</span></span><br></pre></td></tr></table></figure></p>
<h1 id="Transformation操作"><a href="#Transformation操作" class="headerlink" title="Transformation操作"></a>Transformation操作</h1><table>
<thead>
<tr>
<th>Transformation</th>
<th>Transformation    Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>map(func)</td>
<td>利用函数func处理原DStream的每个元素，返回一个新的DStream</td>
</tr>
<tr>
<td>flatMap(func)</td>
<td>与map相似，但是每个输入项可用被映射为0个或者多个输出项</td>
</tr>
<tr>
<td>filter(func)</td>
<td>返回一个新的DStream，它仅仅包含源DStream中满足函数func的项</td>
</tr>
<tr>
<td>repartition(numPartitions)</td>
<td>通过创建更多或者更少的partition改变这个DStream的并行级别(level of parallelism)</td>
</tr>
<tr>
<td>union(otherStream)</td>
<td>返回一个新的DStream,它包含源DStream和otherStream的联合元素</td>
</tr>
<tr>
<td>count()</td>
<td>通过计算源DStream中每个RDD的元素数量，返回一个包含单元素(single-element)RDDs的新DStream</td>
</tr>
<tr>
<td>reduce(func)</td>
<td>利用函数func聚集源DStream中每个RDD的元素，返回一个包含单元素(single-element)RDDs的新DStream。函数应该是相关联的，以使计算可以并行化</td>
</tr>
<tr>
<td>countByValue()</td>
<td>这个算子应用于元素类型为K的DStream上，返回一个（K,long）对的新DStream，每个键的值是在原DStream的每个RDD中的频率。</td>
</tr>
<tr>
<td>reduceByKey(func, [numTasks])</td>
<td>当在一个由(K,V)对组成的DStream上调用这个算子，返回一个新的由(K,V)对组成的DStream，每一个key的值均由给定的reduce函数聚集起来。注意：在默认情况下，这个算子利用了Spark默认的并发任务数去分组。你可以用numTasks参数设置不同的任务数</td>
</tr>
<tr>
<td>join(otherStream, [numTasks])</td>
<td>当应用于两个DStream（一个包含（K,V）对,一个包含(K,W)对），返回一个包含(K, (V, W))对的新DStream</td>
</tr>
<tr>
<td>cogroup(otherStream, [numTasks])</td>
<td>当应用于两个DStream（一个包含（K,V）对,一个包含(K,W)对），返回一个包含(K, Seq[V], Seq[W])的元组</td>
</tr>
<tr>
<td>transform(func)</td>
<td>通过对源DStream的每个RDD应用RDD-to-RDD函数，创建一个新的DStream。这个可以在DStream中的任何RDD操作中使用</td>
</tr>
<tr>
<td>updateStateByKey(func)</td>
<td>利用给定的函数更新DStream的状态，返回一个新”state”的DStream。Meaning</td>
</tr>
</tbody>
</table>
<h1 id="spark-基本函数操作"><a href="#spark-基本函数操作" class="headerlink" title="spark 基本函数操作"></a>spark 基本函数操作</h1><p>使用parallelize对array对象并行化</p>
<figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">spark@spark1:~/spark<span class="number">-2.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.7</span>/test_app_build$ spark-shell</span><br><span class="line"><span class="number">18</span>/<span class="number">12</span>/<span class="number">13</span> <span class="number">01</span>:<span class="number">41</span>:<span class="number">20</span> WARN NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... <span class="built_in">using</span> builtin-java classes <span class="keyword">where</span> applicable</span><br><span class="line">Spark context Web UI available <span class="built_in">at</span> http://spark1:<span class="number">4040</span></span><br><span class="line">Spark context available <span class="built_in">as</span> 'sc' (master = local[*], app id = local<span class="number">-1544665284869</span>).</span><br><span class="line">Spark session available <span class="built_in">as</span> 'spark'.</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    <span class="keyword">_</span>\ \/ <span class="keyword">_</span> \/ <span class="keyword">_</span> `/ __/  '<span class="keyword">_</span>/</span><br><span class="line">   /___/ .__/\<span class="keyword">_</span>,<span class="keyword">_</span>/<span class="keyword">_</span>/ /<span class="keyword">_</span>/\<span class="keyword">_</span>\   version <span class="number">2.4</span><span class="number">.0</span></span><br><span class="line">      /<span class="keyword">_</span>/</span><br><span class="line"></span><br><span class="line">Using Scala version <span class="number">2.11</span><span class="number">.12</span> (Java HotSpot(TM) <span class="number">64</span>-Bit Server VM, Java <span class="number">1.8</span><span class="number">.0</span>_191)</span><br><span class="line"><span class="keyword">Type</span> <span class="built_in">in</span> expressions to have them evaluated.</span><br><span class="line"><span class="keyword">Type</span> :help <span class="keyword">for</span> more information.</span><br><span class="line">scala&gt; val lines = sc.parallelize(Array(<span class="string">"hello"</span>,<span class="string">"spark"</span>,<span class="string">"hello"</span>,<span class="string">"world"</span>,<span class="string">"!"</span>))</span><br><span class="line">lines: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[<span class="number">0</span>] <span class="built_in">at</span> parallelize <span class="built_in">at</span> &lt;console&gt;:<span class="number">24</span></span><br></pre></td></tr></table></figure>
<p>使用foreache和println内置函数对lines的元素进行打印输出</p>
<figure class="highlight golo"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; lines.<span class="keyword">foreach</span>(<span class="keyword">println</span>)</span><br><span class="line">hello</span><br><span class="line">world</span><br><span class="line">spark</span><br><span class="line">!</span><br><span class="line">hello</span><br></pre></td></tr></table></figure>
<p>是用map内置函数对array对象元素进行更改化为key-value类型<br><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">scala&gt; </span>val lines2 = lines.map(world =&gt; (world,<span class="number">1</span>))</span><br><span class="line"><span class="symbol">lines2:</span> <span class="keyword">org.apache.spark.rdd.RDD[(String, </span>Int)] = MapPartitionsRDD[<span class="number">1</span>] <span class="built_in">at</span> map <span class="built_in">at</span> &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span>lines2.foreach(println)</span><br><span class="line">(hello,<span class="number">1</span>)</span><br><span class="line">(spark,<span class="number">1</span>)</span><br><span class="line">(hello,<span class="number">1</span>)</span><br><span class="line">(world,<span class="number">1</span>)</span><br><span class="line">(!,<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<p>使用filter内置函数对array对象进行条件筛选。<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val lines3 = <span class="keyword">lines</span>.<span class="built_in">filter</span>(<span class="built_in">word</span> =&gt; <span class="built_in">word</span>.<span class="keyword">contains</span>(<span class="string">"hello"</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; lines3.foreach(println)</span><br><span class="line">hello</span><br><span class="line">hello</span><br></pre></td></tr></table></figure></p>
<h2 id="集合操作"><a href="#集合操作" class="headerlink" title="集合操作"></a>集合操作</h2><p>进入spark-shell并读取文件<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">spark@spark1:~/spark-2.4.0-bin-hadoop2.7$ spark-shell</span><br><span class="line">18/12/13 02:55:59 WARN NativeCodeLoader: Unable to <span class="keyword">load</span> <span class="keyword">native</span>-hadoop <span class="keyword">library</span> <span class="keyword">for</span> your platform... <span class="keyword">using</span> builtin-<span class="keyword">java</span> classes <span class="keyword">where</span> applicable</span><br><span class="line"><span class="number">18</span>/<span class="number">12</span>/<span class="number">13</span> <span class="number">02</span>:<span class="number">56</span>:<span class="number">03</span> WARN Utils: Service <span class="string">'SparkUI'</span> could <span class="keyword">not</span> bind <span class="keyword">on</span> port <span class="number">4040.</span> Attempting port <span class="number">4041.</span></span><br><span class="line">Spark <span class="keyword">context</span> Web UI available <span class="keyword">at</span> <span class="keyword">http</span>://spark1:<span class="number">4041</span></span><br><span class="line">Spark <span class="keyword">context</span> available <span class="keyword">as</span> <span class="string">'sc'</span> (<span class="keyword">master</span> = <span class="keyword">local</span>[*], app <span class="keyword">id</span> = <span class="keyword">local</span><span class="number">-1544669763113</span>).</span><br><span class="line">Spark <span class="keyword">session</span> available <span class="keyword">as</span> <span class="string">'spark'</span>.</span><br><span class="line">Welcome <span class="keyword">to</span></span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ <span class="string">`/ __/  '_/</span></span><br><span class="line"><span class="string">   /___/ .__/\_,_/_/ /_/\_\   version 2.4.0</span></span><br><span class="line"><span class="string">      /_/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_191)</span></span><br><span class="line"><span class="string">Type in expressions to have them evaluated.</span></span><br><span class="line"><span class="string">Type :help for more information.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">scala&gt; val inputs= sc.textFile("data.txt")</span></span><br><span class="line"><span class="string">inputs: org.apache.spark.rdd.RDD[String] = data.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:24</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">scala&gt; inputs.foreach(println)</span></span><br><span class="line"><span class="string">hello I am hear ,</span></span><br><span class="line"><span class="string">hello word !</span></span><br><span class="line"><span class="string">hello spark ,</span></span><br><span class="line"><span class="string">hello ethan .</span></span><br></pre></td></tr></table></figure></p>
<p>flatMap内置函数对每一行进行操作<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val <span class="keyword">lines</span> = input</span><br><span class="line">input_file_name   inputs</span><br><span class="line"></span><br><span class="line">scala&gt; val <span class="keyword">lines</span> = inputs.flatMap(<span class="built_in">line</span> =&gt; <span class="built_in">line</span>.<span class="built_in">split</span>(<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">lines</span>: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[<span class="number">2</span>] <span class="keyword">at</span> flatMap <span class="keyword">at</span> &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">lines</span>.foreach(println)</span><br><span class="line">hello</span><br><span class="line"><span class="built_in">word</span></span><br><span class="line">!</span><br><span class="line">hello</span><br><span class="line">spark</span><br><span class="line">,</span><br><span class="line">hello</span><br><span class="line">ethan</span><br><span class="line">.</span><br><span class="line">hello</span><br><span class="line">I</span><br><span class="line">am</span><br><span class="line">hear</span><br><span class="line">,</span><br></pre></td></tr></table></figure></p>
<p>初始化两个rdd集合，以便后续做集合运算<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="selector-tag">var</span> rdd1 = sc.parallelize(Array(<span class="string">"coffe"</span>,<span class="string">"coffe"</span>,<span class="string">"panda"</span>,<span class="string">"monkey"</span>,<span class="string">"tea"</span>))</span><br><span class="line">rdd1: org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.RDD</span>[String] = ParallelCollectionRDD[<span class="number">3</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.foreach(println)</span><br><span class="line">panda</span><br><span class="line">coffe</span><br><span class="line">coffe</span><br><span class="line">monkey</span><br><span class="line">tea</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd2  = sc.parallelize(Array(<span class="string">"coffe"</span>,<span class="string">"monkey"</span>,<span class="string">"kitty"</span>))</span><br><span class="line">rdd2: org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.RDD</span>[String] = ParallelCollectionRDD[<span class="number">4</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.foreach(println)</span><br><span class="line">coffe</span><br><span class="line">monkey</span><br><span class="line">kitty</span><br></pre></td></tr></table></figure></p>
<p>对rdd1 元素去重操作<br><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">scala&gt; </span>var rdd_distinct = rdd1.<span class="keyword">distinct()</span></span><br><span class="line"><span class="keyword">rdd_distinct: </span><span class="keyword">org.apache.spark.rdd.RDD[String] </span>= MapPartitionsRDD[<span class="number">7</span>] <span class="built_in">at</span> <span class="keyword">distinct </span><span class="built_in">at</span> &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span>rdd_distinct.foreach(println)</span><br><span class="line"></span><br><span class="line">tea</span><br><span class="line">monkey</span><br><span class="line">panda</span><br><span class="line">coffe</span><br></pre></td></tr></table></figure></p>
<p>求两集合的交集</p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd_union = rdd1.<span class="keyword">union</span>(rdd2)</span><br><span class="line"><span class="symbol">rdd_union:</span> org.apache.spark.rdd.RDD[String] = UnionRDD[<span class="number">8</span>] at <span class="class"><span class="keyword">union</span> <span class="title">at</span> &lt;<span class="title">console</span>&gt;:27</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd_union.foreach(println)</span><br><span class="line">coffe</span><br><span class="line">panda</span><br><span class="line">coffe</span><br><span class="line">monkey</span><br><span class="line">tea</span><br><span class="line">monkey</span><br><span class="line">coffe</span><br><span class="line">kitty</span><br></pre></td></tr></table></figure>
<p>求两集合的交集<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd_inter= rdd1.intersection(rdd2)</span><br><span class="line">rdd_inter: org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.RDD</span>[String] = MapPartitionsRDD[<span class="number">14</span>] at intersection at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd_inter</span><br><span class="line">res9: org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.RDD</span>[String] = MapPartitionsRDD[<span class="number">14</span>] at intersection at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd_inter.foreach(println)</span><br><span class="line">monkey</span><br><span class="line">coffe</span><br></pre></td></tr></table></figure></p>
<p>求集合的子集</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">scala&gt; </span>val rdd_sub=rdd1.<span class="keyword">subtract(rdd2)</span></span><br><span class="line"><span class="keyword">rdd_sub: </span><span class="keyword">org.apache.spark.rdd.RDD[String] </span>= MapPartitionsRDD[<span class="number">18</span>] <span class="built_in">at</span> <span class="keyword">subtract </span><span class="built_in">at</span> &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span>rdd_sub.foreach(println)</span><br><span class="line">panda</span><br><span class="line">tea</span><br></pre></td></tr></table></figure>
<h2 id="key-values-值操作"><a href="#key-values-值操作" class="headerlink" title="key values 值操作"></a>key values 值操作</h2><p>读取文件，形成key-values数据<br><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd=sc.textFile(<span class="string">"data.txt"</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[<span class="keyword">String</span>] = data.txt MapPartitionsRDD[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.foreach(<span class="built_in">println</span>)</span><br><span class="line">hello I am hear ,</span><br><span class="line">hello <span class="keyword">word</span> !</span><br><span class="line">hello spark ,</span><br><span class="line">hello ethan .</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd2 = rdd.<span class="built_in">map</span>(<span class="built_in">line</span> =&gt; (<span class="built_in">line</span>.split(<span class="string">" "</span>)(<span class="number">0</span>),<span class="built_in">line</span>))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[(<span class="keyword">String</span>, <span class="keyword">String</span>)] = MapPartitionsRDD[<span class="number">2</span>] at <span class="built_in">map</span> at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.foreach(<span class="built_in">println</span>)</span><br><span class="line">(hello,hello I am hear ,)</span><br><span class="line">(hello,hello <span class="keyword">word</span> !)</span><br><span class="line">(hello,hello spark ,)</span><br><span class="line">(hello,hello ethan .)</span><br></pre></td></tr></table></figure></p>
<p>reduceByKey</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">scala&gt; </span>val rdd3 = <span class="keyword">sc.parallelize(Array((1,2),(3,4),(3,6)))</span></span><br><span class="line"><span class="keyword">rdd3: </span><span class="keyword">org.apache.spark.rdd.RDD[(Int, </span>Int)] = ParallelCollectionRDD[<span class="number">3</span>] <span class="built_in">at</span> parallelize <span class="built_in">at</span> &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span>rdd3.foreach(println)</span><br><span class="line">(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">(<span class="number">3</span>,<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span>val rdd4 =rdd3.reduceByKey((x,y)=&gt;x+y)</span><br><span class="line"><span class="symbol">rdd4:</span> <span class="keyword">org.apache.spark.rdd.RDD[(Int, </span>Int)] = <span class="keyword">ShuffledRDD[4] </span><span class="built_in">at</span> reduceByKey <span class="built_in">at</span> &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span>rdd4.foreach(println)</span><br><span class="line">(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">(<span class="number">3</span>,<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>groupByKey<br><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span>val rdd5=rdd3.groupByKey()</span><br><span class="line"><span class="symbol">rdd5:</span> <span class="keyword">org.apache.spark.rdd.RDD[(Int, </span>Iterable[Int])] = <span class="keyword">ShuffledRDD[6] </span><span class="built_in">at</span> groupByKey <span class="built_in">at</span> &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span>rdd5.foreach(println)</span><br><span class="line">(<span class="number">3</span>,CompactBuffer(<span class="number">4</span>, <span class="number">6</span>))</span><br><span class="line">(<span class="number">1</span>,CompactBuffer(<span class="number">2</span>))</span><br></pre></td></tr></table></figure></p>
<p>查看rdds key值<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd6=rdd3.keys</span><br><span class="line">rdd6: org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.RDD</span>[Int] = MapPartitionsRDD[<span class="number">7</span>] at keys at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd6.foreach(println)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure></p>
<p>sortByKey</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">scala&gt; </span>val rdd7=rdd3.sortByKey()</span><br><span class="line"><span class="symbol">rdd7:</span> <span class="keyword">org.apache.spark.rdd.RDD[(Int, </span>Int)] = <span class="keyword">ShuffledRDD[10] </span><span class="built_in">at</span> sortByKey <span class="built_in">at</span> &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span>rdd7.foreach</span><br><span class="line">foreach   foreachAsync   foreachPartition   foreachPartitionAsync</span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span>rdd7.foreach(println)</span><br><span class="line">(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">(<span class="number">3</span>,<span class="number">6</span>)</span><br></pre></td></tr></table></figure>
<p>combineByKey()：(creatCombiner,mergeValue,mergeCombiners,partitioner)<br>遍历partiton中的元素，元素的key,要么之前见过的，要么不是。<br>如果是新元素，适应我们提供的creatCombiner()函数，<br>如果是这个partition中已经存在的Key,就会使用mergeValue()函数，<br>举例，求分数平均值</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">scala&gt; </span>val <span class="keyword">scores </span>= <span class="keyword">sc.parallelize(Array(("jake",80.0),("jake",90.0),("jake",85.0),("mike",85.0),("mike",92.0),("mike",90.0)))</span></span><br><span class="line"><span class="keyword">scores: </span><span class="keyword">org.apache.spark.rdd.RDD[(String, </span>Double)] = ParallelCollectionRDD[<span class="number">11</span>] <span class="built_in">at</span> parallelize <span class="built_in">at</span> &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span><span class="keyword">scores.for</span></span><br><span class="line"><span class="keyword">foreach </span>  foreachAsync   foreachPartition   foreachPartitionAsync   formatted</span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span><span class="keyword">scores.foreach</span></span><br><span class="line"><span class="keyword">foreach </span>  foreachAsync   foreachPartition   foreachPartitionAsync</span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span><span class="keyword">scores.foreach(println)</span></span><br><span class="line"><span class="keyword">(jake,90.0)</span></span><br><span class="line"><span class="keyword">(jake,80.0)</span></span><br><span class="line"><span class="keyword">(jake,85.0)</span></span><br><span class="line"><span class="keyword">(mike,85.0)</span></span><br><span class="line"><span class="keyword">(mike,90.0)</span></span><br><span class="line"><span class="keyword">(mike,92.0)</span></span><br><span class="line"><span class="keyword"></span></span><br><span class="line"><span class="keyword"></span></span><br><span class="line"><span class="keyword">scala&gt; </span>val <span class="keyword">score2=scores.combineByKey(score </span>=&gt; (<span class="number">1</span>,<span class="keyword">score),(c1:(Int,Double),newScore)=&gt;(c1._1+1,c1._2+newScore),(c1:(Int,Double),c2:(Int,Double))=&gt;(c1._1+c2._1,c1._2+c2._2))</span></span><br><span class="line"><span class="keyword">score2: </span><span class="keyword">org.apache.spark.rdd.RDD[(String, </span>(Int, Double))] = <span class="keyword">ShuffledRDD[12] </span><span class="built_in">at</span> combineByKey <span class="built_in">at</span> &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span><span class="keyword">score2.foreach(println)</span></span><br><span class="line"><span class="keyword">(jake,(3,255.0))</span></span><br></pre></td></tr></table></figure>
<p>配置spark JDBC</p>
<h2 id="下载-mysql-connector-jar"><a href="#下载-mysql-connector-jar" class="headerlink" title="下载  mysql-connector jar"></a>下载  mysql-connector jar</h2><p><a href="https://dev.mysql.com/downloads/file/?id=480090" target="_blank" rel="noopener">https://dev.mysql.com/downloads/file/?id=480090</a></p>
<p>为了让 Spark 能用上 MySQL 服务器，我们需要驱动程序 Connector/J for MySQL. 下载这个压缩文件解压后拷贝 mysql-connector-java-5.1.39-bin.jar 到 spark 目录，然后在 conf/spark-defaults.conf 中添加类路径，如下：</p>
<p>spark.driver.extraClassPath = /usr/local/spark/mysql-connector-java-5.1.47-bin.jar<br>spark.executor.extraClassPath = /usr/local/spark/mysql-connector-java-5.1.47-bin.jar</p>
<h1 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h1><h2 id="创建SparkSession"><a href="#创建SparkSession" class="headerlink" title="创建SparkSession"></a>创建SparkSession</h2><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession <span class="string">\</span></span><br><span class="line">    .builder <span class="string">\</span></span><br><span class="line">    .appName(<span class="string">"Python Spark SQL basic example"</span>) <span class="string">\</span></span><br><span class="line">    .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>) <span class="string">\</span></span><br><span class="line">    .getOrCreate()</span><br></pre></td></tr></table></figure>
<h2 id="创建-DataFrames"><a href="#创建-DataFrames" class="headerlink" title="创建 DataFrames"></a>创建 DataFrames</h2><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># spark is an existing SparkSession</span></span><br><span class="line">df = spark.read.json(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line"><span class="meta"># Displays the content of the DataFrame to stdout</span></span><br><span class="line">df.show()</span><br><span class="line"><span class="meta"># +----+-------+</span></span><br><span class="line"><span class="meta"># | age|   name|</span></span><br><span class="line"><span class="meta"># +----+-------+</span></span><br><span class="line"><span class="meta"># |null|Michael|</span></span><br><span class="line"><span class="meta"># |  30|   Andy|</span></span><br><span class="line"><span class="meta"># |  19| Justin|</span></span><br><span class="line"><span class="meta"># +----+-------+</span></span><br></pre></td></tr></table></figure>
<h2 id="DataSet-操作"><a href="#DataSet-操作" class="headerlink" title="DataSet 操作"></a>DataSet 操作</h2><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># spark, df are from the previous example</span></span><br><span class="line"><span class="meta"># Print the schema in a tree format</span></span><br><span class="line">df.printSchema()</span><br><span class="line"><span class="meta"># root</span></span><br><span class="line"><span class="meta"># |-- age: long (nullable = true)</span></span><br><span class="line"><span class="meta"># |-- name: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Select only the "name" column</span></span><br><span class="line">df.select(<span class="string">"name"</span>).show()</span><br><span class="line"><span class="meta"># +-------+</span></span><br><span class="line"><span class="meta"># |   name|</span></span><br><span class="line"><span class="meta"># +-------+</span></span><br><span class="line"><span class="meta"># |Michael|</span></span><br><span class="line"><span class="meta"># |   Andy|</span></span><br><span class="line"><span class="meta"># | Justin|</span></span><br><span class="line"><span class="meta"># +-------+</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Select everybody, but increment the age by 1</span></span><br><span class="line">df.select(df[<span class="string">'name'</span>], df[<span class="string">'age'</span>] + <span class="number">1</span>).show()</span><br><span class="line"><span class="meta"># +-------+---------+</span></span><br><span class="line"><span class="meta"># |   name|(age + 1)|</span></span><br><span class="line"><span class="meta"># +-------+---------+</span></span><br><span class="line"><span class="meta"># |Michael|     null|</span></span><br><span class="line"><span class="meta"># |   Andy|       31|</span></span><br><span class="line"><span class="meta"># | Justin|       20|</span></span><br><span class="line"><span class="meta"># +-------+---------+</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Select people older than 21</span></span><br><span class="line">df.filter(df[<span class="string">'age'</span>] &gt; <span class="number">21</span>).show()</span><br><span class="line"><span class="meta"># +---+----+</span></span><br><span class="line"><span class="meta"># |age|name|</span></span><br><span class="line"><span class="meta"># +---+----+</span></span><br><span class="line"><span class="meta"># | 30|Andy|</span></span><br><span class="line"><span class="meta"># +---+----+</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Count people by age</span></span><br><span class="line">df.groupBy(<span class="string">"age"</span>).count().show()</span><br><span class="line"><span class="meta"># +----+-----+</span></span><br><span class="line"><span class="meta"># | age|count|</span></span><br><span class="line"><span class="meta"># +----+-----+</span></span><br><span class="line"><span class="meta"># |  19|    1|</span></span><br><span class="line"><span class="meta"># |null|    1|</span></span><br><span class="line"><span class="meta"># |  30|    1|</span></span><br><span class="line"><span class="meta"># +----+-----+</span></span><br></pre></td></tr></table></figure>
<h2 id="SQL-查询编程"><a href="#SQL-查询编程" class="headerlink" title="SQL 查询编程"></a>SQL 查询编程</h2><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// Register the DataFrame as a SQL temporary view</span><br><span class="line">df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line">val sqlDF = spark.sql(<span class="string">"SELECT * FROM people"</span>)</span><br><span class="line">sqlDF.show()</span><br><span class="line">// +----+-------+</span><br><span class="line">// |<span class="string"> age</span>|<span class="string">   name</span>|</span><br><span class="line">// +----+-------+</span><br><span class="line">// |<span class="string">null</span>|<span class="string">Michael</span>|</span><br><span class="line">// |<span class="string">  30</span>|<span class="string">   Andy</span>|</span><br><span class="line">// |<span class="string">  19</span>|<span class="string"> Justin</span>|</span><br><span class="line">// +----+-------+</span><br></pre></td></tr></table></figure>
<h2 id="Global-Temporary-View"><a href="#Global-Temporary-View" class="headerlink" title="Global Temporary View"></a>Global Temporary View</h2><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># Register the DataFrame as a global temporary view</span></span><br><span class="line">df.createGlobalTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta"># Global temporary view is tied to a system preserved database `global_temp`</span></span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show()</span><br><span class="line"><span class="meta"># +----+-------+</span></span><br><span class="line"><span class="meta"># | age|   name|</span></span><br><span class="line"><span class="meta"># +----+-------+</span></span><br><span class="line"><span class="meta"># |null|Michael|</span></span><br><span class="line"><span class="meta"># |  30|   Andy|</span></span><br><span class="line"><span class="meta"># |  19| Justin|</span></span><br><span class="line"><span class="meta"># +----+-------+</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Global temporary view is cross-session</span></span><br><span class="line">spark.newSession().sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show()</span><br><span class="line"><span class="meta"># +----+-------+</span></span><br><span class="line"><span class="meta"># | age|   name|</span></span><br><span class="line"><span class="meta"># +----+-------+</span></span><br><span class="line"><span class="meta"># |null|Michael|</span></span><br><span class="line"><span class="meta"># |  30|   Andy|</span></span><br><span class="line"><span class="meta"># |  19| Justin|</span></span><br><span class="line"><span class="meta"># +----+-------+</span></span><br></pre></td></tr></table></figure>
<h2 id="JDBC-连接创建"><a href="#JDBC-连接创建" class="headerlink" title="JDBC  连接创建"></a>JDBC  连接创建</h2><figure class="highlight rsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">jdbcDF = spark.read.<span class="built_in">format</span>(<span class="string">"jdbc"</span>).<span class="built_in">option</span>(<span class="string">"url"</span>,<span class="string">"jdbc:mysql://192.168.10.170:3306"</span>).<span class="built_in">option</span>(<span class="string">"dbtable"</span>,<span class="string">"HKIA_v1.flight_schedule"</span>).<span class="built_in">option</span>(<span class="string">"user"</span>, <span class="string">"hkia_v1"</span>).<span class="built_in">option</span>(<span class="string">"password"</span>, <span class="string">"Asdf168!!"</span>).load()</span><br><span class="line"></span><br><span class="line">jdbcDF.printSchema()</span><br><span class="line">jdbcDF.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jdbcDF = spark.read.format(<span class="string">"jdbc"</span>).option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://192.168.10.170:3306"</span>).option(<span class="string">"dbtable"</span>, <span class="string">"HKIA_v1.flight_schedule"</span>).option(<span class="string">"user"</span>, <span class="string">"hkia_v1"</span>).option(<span class="string">"password"</span>, <span class="string">"Asdf168!!"</span>).load()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jdbcDF.show()</span><br></pre></td></tr></table></figure>
<pre><code>---------------------------------------------------------------------------

Py4JJavaError                             Traceback (most recent call last)

&lt;ipython-input-2-d4912bf3e00e&gt; in &lt;module&gt;()
----&gt; 1 jdbcDF.show()


/home/spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/dataframe.py in show(self, n, truncate, vertical)
    376         &quot;&quot;&quot;
    377         if isinstance(truncate, bool) and truncate:
--&gt; 378             print(self._jdf.showString(n, 20, vertical))
    379         else:
    380             print(self._jdf.showString(n, int(truncate), vertical))


/home/spark/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1255         answer = self.gateway_client.send_command(command)
   1256         return_value = get_return_value(
-&gt; 1257             answer, self.gateway_client, self.target_id, self.name)
   1258 
   1259         for temp_arg in temp_args:


/home/spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py in deco(*a, **kw)
     61     def deco(*a, **kw):
     62         try:
---&gt; 63             return f(*a, **kw)
     64         except py4j.protocol.Py4JJavaError as e:
     65             s = e.java_exception.toString()


/home/spark/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    326                 raise Py4JJavaError(
    327                     &quot;An error occurred while calling {0}{1}{2}.\n&quot;.
--&gt; 328                     format(target_id, &quot;.&quot;, name), value)
    329             else:
    330                 raise Py4JError(


Py4JJavaError: An error occurred while calling o38.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: KILLED
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
    at scala.Option.foreach(Option.scala:257)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
    at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)
    at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
    at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)
    at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
    at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
    at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)
    at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
    at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)
    at org.apache.spark.sql.Dataset.head(Dataset.scala:2545)
    at org.apache.spark.sql.Dataset.take(Dataset.scala:2759)
    at org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)
    at org.apache.spark.sql.Dataset.showString(Dataset.scala:292)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Thread.java:748)
</code></pre>
        
            <div class="donate-container">
    <div class="donate-button">
        <button id="donate-button">赞赏</button>
    </div>
    <div class="donate-img-container hide" id="donate-img-container">
        <img id="donate-img" src="" data-src="/img/donate.jpg">
        <p> 你的支持是我写作的莫大鼓励 </p>
    </div>
</div>
        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>
    </div>
</div>
    </div>
</div>


<footer class="footer">
	



    <ul class="list-inline text-center">


	




        
        

        

        

        
        <li>
            <a target="_blank"  href="https://github.com/ethan2lee">
                            <span class="fa-stack fa-lg">
                                <i class="iconfont icon-github"></i>
                            </span>
            </a>
        </li>
        

        

    </ul>
    
    <p>
        <span>/</span>
        
        <span><a href="http://anne2bin.top">anne2bin</a></span>
        <span>/</span>
        
        <span><a href="https://blog.ywandy.top/">yewei_andy</a></span>
        <span>/</span>
        
        <span><a href="https://wwb.colonp.top/">wwb</a></span>
        <span>/</span>
        
    </p>
    
</footer>




</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


    <script>
        /**
         *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
         *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
        */
        if( '' || '')
        var disqus_config = function () {
            this.page.url = '';  // Replace PAGE_URL with your page's canonical URL variable
            this.page.identifier = ''; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };

        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = 'https://ethan2lee.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>

</html>
